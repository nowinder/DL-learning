{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## out produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "import cmath\n",
    "import sawcom7 as sc\n",
    "import random\n",
    "\n",
    "pI = 3600E-9 #The period of IDT, normally is one wavelength\n",
    "h = 0.08*pI #The thickness of IDT, Al or Al-Cu1%\n",
    "W1 = 20*pI # Width of IDT (acoustic aperture), in m\n",
    "m_ratio = 0.6 #The metallization ratio\n",
    "epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "\n",
    "npiezo_1 = 50.3562796837374\n",
    "eta = (0.182+0.349*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "e = (0.0388+0.618*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "alpha = 0.05\n",
    "c = 1 + (0.0678+1.057*(2*h/pI))**2\n",
    "k2 = 0.0655 + 0.206*(2*h/pI)\n",
    "vb = 4226.54\n",
    "\n",
    "npiezo_1_num = [i for i in np.linspace(npiezo_1*0.95,npiezo_1*1.05,10000)]\n",
    "eta_num = [i for i in np.linspace(eta*0.95,eta*1.05,10000)]\n",
    "e_num = [i for i in np.linspace(e*0.95,e*1.05,10000)]\n",
    "alpha_num = [i for i in np.linspace(alpha*0.95,alpha*1.05,10000)]\n",
    "c_num = [i for i in np.linspace(c*0.75,c*1.25,10000)]\n",
    "k2_num = [i for i in np.linspace(k2*0.01,k2*2.0,10000)]\n",
    "vb_num = [i for i in np.linspace(vb*0.95,vb*1.05,10000)]\n",
    "\n",
    "x = np.zeros((20000,7))\n",
    "for i in range(0,20000):\n",
    "    random.seed()\n",
    "    npi = random.sample(npiezo_1_num,1)[0]\n",
    "    et = random.sample(eta_num,1)[0]\n",
    "    e = random.sample(e_num,1)[0]\n",
    "    al = random.sample(alpha_num,1)[0]\n",
    "    c = random.sample(c_num,1)[0]\n",
    "    k = random.sample(k2_num,1)[0]\n",
    "    v = random.sample(vb_num,1)[0]\n",
    "    x[i,:]=[npi,et,e,al,c,k,v]\n",
    "    \n",
    "x_max = x.max(axis=0).reshape((1,7))\n",
    "x_min = x.min(axis=0).reshape((1,7))\n",
    "file = 'D:/data/7p/out/' +'MP60_0.5' + '.csv'\n",
    "with open(file,'w',newline='') as f:\n",
    "    np.savetxt(f,x,delimiter=',',newline='\\n')\n",
    "file = file + 'maxmin.csv'\n",
    "with open(file, 'w', newline='') as f:\n",
    "    np.savetxt(f, x_max, delimiter=',',newline='\\n')\n",
    "    np.savetxt(f, x_min, delimiter=',',newline='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "import cmath\n",
    "import sawcom7 as sc\n",
    "\n",
    "pI = 3600E-9 #The period of IDT, normally is one wavelength\n",
    "h = 0.08*pI #The thickness of IDT, Al or Al-Cu1%\n",
    "W1 = 20*pI # Width of IDT (acoustic aperture), in m\n",
    "m_ratio = 0.6 #The metallization ratio\n",
    "epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "\n",
    "npiezo_1 = 50.3562796837374\n",
    "eta = (0.182+0.349*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "e = (0.0388+0.618*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "alpha = 0.05\n",
    "c = 1 + (0.0678+1.27*(2*h/pI))**2\n",
    "k2 = 0.0655 + 0.206*(2*h/pI)\n",
    "vb = 4226.54\n",
    "\n",
    "eta_b = (eta+2*abs(e))/2\n",
    "epsilon = npiezo_1*epsilon_0\n",
    "\n",
    "x1 = np.cos(np.pi*m_ratio )\n",
    "m1 = math.sqrt((1-x1)/2) \n",
    "km1 = special.ellipk(m1, out=None)\n",
    "p1 = 2*km1/np.pi\n",
    "x2 = -np.cos(np.pi*m_ratio )\n",
    "m2 = math.sqrt((1-x2)/2) \n",
    "km2 = special.ellipk(m2, out=None)\n",
    "p2 = 2*km2/np.pi\n",
    "p_factor = p1/p2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "freq = np.linspace(0.5E9, 1.5E9, 501)\n",
    "# freq_mhz = freq/1e6\n",
    "\n",
    "# eta = (0.182+0.349*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "# e = (0.0388+0.618*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "# eta_b = (eta+2*abs(e))/2\n",
    "# alpha = 10**(-3.39+7.39*(2*h/pI))\n",
    "# c = 1 + (0.0678+1.27*(2*h/pI))**2\n",
    "# k2 = 0.0655 + 0.206*(2*h/pI)\n",
    "# vb = 4226.54\n",
    "\n",
    "delta_v = - (eta**2)/2\n",
    "k = abs(e)*(eta+abs(e)/2)\n",
    "kb = -(abs(e)**2)*eta/(eta+2*abs(e))\n",
    "delta_b = -((eta**2)-2*((abs(e)**2)))/4\n",
    "omega = freq*2*np.pi\n",
    "delta = omega/vb - 2*np.pi/pI - 1j*alpha\n",
    "\n",
    "# epsilon = npiezo_1*epsilon_0\n",
    "v_delta = []\n",
    "for i in range(0,len(delta)):\n",
    "    v_delta_0 = eta_b/((cmath.sqrt(delta_b-delta[i]))+ eta_b)# wave velocity in m/s\n",
    "    v_delta.append(v_delta_0)\n",
    "v_delta = np.array(v_delta)\n",
    "omega = freq*2*np.pi\n",
    "C = (W1*epsilon*p_factor)/pI ##To check\n",
    "xi = []\n",
    "for i in range(0,len(omega)):\n",
    "    xi_0 = c*cmath.sqrt((omega[i]*C*k2)/(pI*np.pi))\n",
    "    xi_0 = -1j*xi_0\n",
    "    xi.append(xi_0)\n",
    "xi = np.array(xi)\n",
    "\n",
    "lam1 = pI # Wavelength in m of SAW filters \n",
    "#v1 = 3925 # wave velocity in m/s\n",
    "# epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "# epsilon = npiezo_1*epsilon_0 # The effective permittivity of piezoelectric layer \n",
    "c12 =  -1j*c*(k+kb*v_delta) # Reflectivity per unit length (~1.7% reflected per IDT spaced at lam/2)\n",
    "a1 = -xi # The transduction coefficient\n",
    "n1 = 100 # The number of IDT pairs\n",
    "L1 = n1*lam1 # Length of total IDT the grating, in m\n",
    "#W1 = 22*lam1 # Width of IDT (acoustic aperture), in m\n",
    "#d = sc.delta(freq,v1,lam1) - 500j\n",
    "Ct=n1*W1*epsilon # Static capacitance of total IDT\n",
    "#d1 = sc.delta(freq,v1,lam1)\n",
    "d1 = sc.thetau(c,delta,delta_v,kb,v_delta)\n",
    "C1 = sc.C0(freq,Ct)\n",
    "idt_ref_1 = sc.pmatrix(lam1,c12,a1,L1,d1,C1) #The P-Matrix of SAW resonator with refelection \n",
    "y11 = 20 * np.log10(abs(idt_ref_1.p33)/5)\n",
    "# y = np.stack((freq,y11), axis=-1)\n",
    "# return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "import cmath\n",
    "import sawcom7 as sc\n",
    "\n",
    "pI = 3600E-9 #The period of IDT, normally is one wavelength\n",
    "h = 0.08*pI #The thickness of IDT, Al or Al-Cu1%\n",
    "W1 = 20*pI # Width of IDT (acoustic aperture), in m\n",
    "m_ratio = 0.6 #The metallization ratio\n",
    "epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "\n",
    "npiezo_1 = 50.3562796837374*1.20\n",
    "eta = (0.182+0.349*(2*h/pI))*math.sqrt(2*np.pi/pI)*1.20\n",
    "e = (0.0388+0.618*(2*h/pI))*math.sqrt(2*np.pi/pI)*1.20\n",
    "alpha = 0.05*1.20\n",
    "c = (1 + (0.0678+1.27*(2*h/pI))**2)*1.20\n",
    "k2 = (0.0655 + 0.206*(2*h/pI))*1.20\n",
    "vb = 4226.54*1.20\n",
    "\n",
    "eta_b = (eta+2*abs(e))/2\n",
    "epsilon = npiezo_1*epsilon_0\n",
    "\n",
    "x1 = np.cos(np.pi*m_ratio )\n",
    "m1 = math.sqrt((1-x1)/2) \n",
    "km1 = special.ellipk(m1, out=None)\n",
    "p1 = 2*km1/np.pi\n",
    "x2 = -np.cos(np.pi*m_ratio )\n",
    "m2 = math.sqrt((1-x2)/2) \n",
    "km2 = special.ellipk(m2, out=None)\n",
    "p2 = 2*km2/np.pi\n",
    "p_factor = p1/p2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "freq = np.linspace(0.5E9, 1.5E9, 501)\n",
    "# freq_mhz = freq/1e6\n",
    "\n",
    "# eta = (0.182+0.349*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "# e = (0.0388+0.618*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "# eta_b = (eta+2*abs(e))/2\n",
    "# alpha = 10**(-3.39+7.39*(2*h/pI))\n",
    "# c = 1 + (0.0678+1.207*(2*h/pI))**2\n",
    "# k2 = 0.0655 + 0.206*(2*h/pI)\n",
    "# vb = 4226.54\n",
    "\n",
    "delta_v = - (eta**2)/2\n",
    "k = abs(e)*(eta+abs(e)/2)\n",
    "kb = -(abs(e)**2)*eta/(eta+2*abs(e))\n",
    "delta_b = -((eta**2)-2*((abs(e)**2)))/4\n",
    "omega = freq*2*np.pi\n",
    "delta = omega/vb - 2*np.pi/pI - 1j*alpha\n",
    "\n",
    "# epsilon = npiezo_1*epsilon_0\n",
    "v_delta = []\n",
    "for i in range(0,len(delta)):\n",
    "    v_delta_0 = eta_b/((cmath.sqrt(delta_b-delta[i]))+ eta_b)# wave velocity in m/s\n",
    "    v_delta.append(v_delta_0)\n",
    "v_delta = np.array(v_delta)\n",
    "omega = freq*2*np.pi\n",
    "C = (W1*epsilon*p_factor)/pI ##To check\n",
    "xi = []\n",
    "for i in range(0,len(omega)):\n",
    "    xi_0 = c*cmath.sqrt((omega[i]*C*k2)/(pI*np.pi))\n",
    "    xi_0 = -1j*xi_0\n",
    "    xi.append(xi_0)\n",
    "xi = np.array(xi)\n",
    "\n",
    "lam1 = pI # Wavelength in m of SAW filters \n",
    "#v1 = 3925 # wave velocity in m/s\n",
    "# epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "# epsilon = npiezo_1*epsilon_0 # The effective permittivity of piezoelectric layer \n",
    "c12 =  -1j*c*(k+kb*v_delta) # Reflectivity per unit length (~1.7% reflected per IDT spaced at lam/2)\n",
    "a1 = -xi # The transduction coefficient\n",
    "n1 = 100 # The number of IDT pairs\n",
    "L1 = n1*lam1 # Length of total IDT the grating, in m\n",
    "#W1 = 22*lam1 # Width of IDT (acoustic aperture), in m\n",
    "#d = sc.delta(freq,v1,lam1) - 500j\n",
    "Ct=n1*W1*epsilon # Static capacitance of total IDT\n",
    "#d1 = sc.delta(freq,v1,lam1)\n",
    "d1 = sc.thetau(c,delta,delta_v,kb,v_delta)\n",
    "C1 = sc.C0(freq,Ct)\n",
    "idt_ref_1 = sc.pmatrix(lam1,c12,a1,L1,d1,C1) #The P-Matrix of SAW resonator with refelection \n",
    "y11_3 = 20 * np.log10(abs(idt_ref_1.p33)/5)\n",
    "# y = np.stack((freq,y11), axis=-1)\n",
    "# return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d8dec373a0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=[12, 6])\n",
    "plt.plot(freq,y11,'-o',lw=1)\n",
    "plt.plot(freq,y11_1,'-o',lw=1)\n",
    "plt.plot(freq,y11_2,'-o',lw=1)\n",
    "plt.plot(freq,y11_3,'-o',lw=1)\n",
    "\n",
    "plt.xlabel(\"Frequency (GHz)\")\n",
    "plt.ylabel(\"Magnitude (log scale)\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_generatev2\n",
    "# code by Zhengcaizhi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import special\n",
    "import cmath\n",
    "from scipy.signal import argrelextrema\n",
    "import sawcom7 as sc\n",
    "\n",
    "def cacul(paras):\n",
    "    npiezo_1 = paras[0]\n",
    "    eta = paras[1]\n",
    "    e = paras[2]\n",
    "    alpha = paras[3]\n",
    "    c = paras[4]\n",
    "    k2 = paras[5]\n",
    "    vb = paras[6]\n",
    "    epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "    pI = 3600E-9 #The period of IDT, normally is one wavelength\n",
    "    h = 0.08*pI #The thickness of IDT, Al or Al-Cu1%\n",
    "    W1 = 20*pI # Width of IDT (acoustic aperture), in m\n",
    "    m_ratio = 0.6 #The metallization ratio\n",
    "\n",
    "    eta_b = (eta+2*abs(e))/2\n",
    "    epsilon = npiezo_1*epsilon_0\n",
    "    x1 = np.cos(np.pi*m_ratio )\n",
    "    m1 = math.sqrt((1-x1)/2) \n",
    "    km1 = special.ellipk(m1, out=None)\n",
    "    p1 = 2*km1/np.pi\n",
    "    x2 = -np.cos(np.pi*m_ratio )\n",
    "    m2 = math.sqrt((1-x2)/2) \n",
    "    km2 = special.ellipk(m2, out=None)\n",
    "    p2 = 2*km2/np.pi\n",
    "    p_factor = p1/p2\n",
    "    freq = np.linspace(0.5E9, 1.5E9, 501)\n",
    "    # freq_mhz = freq/1e6\n",
    "    delta_v = - (eta**2)/2\n",
    "    k = abs(e)*(eta+abs(e)/2)\n",
    "    kb = -(abs(e)**2)*eta/(eta+2*abs(e))\n",
    "    delta_b = -((eta**2)-2*((abs(e)**2)))/4\n",
    "    omega = freq*2*np.pi\n",
    "    delta = omega/vb - 2*np.pi/pI - 1j*alpha\n",
    "\n",
    "    v_delta = []\n",
    "    for i in range(0,len(delta)):\n",
    "        v_delta_0 = eta_b/((cmath.sqrt(delta_b-delta[i]))+ eta_b)# wave velocity in m/s\n",
    "        v_delta.append(v_delta_0)\n",
    "    v_delta = np.array(v_delta)\n",
    "    omega = freq*2*np.pi\n",
    "    C = (W1*epsilon*p_factor)/pI ##To check\n",
    "    xi = []\n",
    "    for i in range(0,len(omega)):\n",
    "        xi_0 = c*cmath.sqrt((omega[i]*C*k2)/(pI*np.pi))\n",
    "        xi_0 = -1j*xi_0\n",
    "        xi.append(xi_0)\n",
    "    xi = np.array(xi)\n",
    "\n",
    "    lam1 = pI # Wavelength in m of SAW filters \n",
    "    c12 =  -1j*c*(k+kb*v_delta) # Reflectivity per unit length (~1.7% reflected per IDT spaced at lam/2)\n",
    "    a1 = -xi # The transduction coefficient\n",
    "    n1 = 100 # The number of IDT pairs\n",
    "    L1 = n1*lam1 # Length of total IDT the grating, in m\n",
    "    #W1 = 22*lam1 # Width of IDT (acoustic aperture), in m\n",
    "    #d = sc.delta(freq,v1,lam1) - 500j\n",
    "    Ct=n1*W1*epsilon # Static capacitance of total IDT\n",
    "    #d1 = sc.delta(freq,v1,lam1)\n",
    "    d1 = sc.thetau(c,delta,delta_v,kb,v_delta)\n",
    "    C1 = sc.C0(freq,Ct)\n",
    "    idt_ref_1 = sc.pmatrix(lam1,c12,a1,L1,d1,C1) #The P-Matrix of SAW resonator with refelection \n",
    "    # y11 = 20 * np.log10(abs(idt_ref_1.p33)/5)\n",
    "    y11 = (idt_ref_1.p33)/5\n",
    "    return y11\n",
    "def YtoZS(Y_COM, freq):\n",
    "    Y11 = 20*np.log10(abs(Y_COM))\n",
    "    Z_COM = 1/Y_COM\n",
    "    z = 50 \n",
    "    S11_COM = (1-Y_COM*z)/(1+Y_COM*z)\n",
    "\n",
    "    group_delay = -np.diff(np.unwrap(np.angle(S11_COM))) / np.diff(2 * np.pi * freq)\n",
    "    group_delay = np.concatenate(([group_delay[0]], group_delay))\n",
    "    Q_COM = 2*np.pi*(freq)*group_delay *abs(S11_COM)/(1-abs(S11_COM)**2)\n",
    "\n",
    "    DSP_COM = 10*np.log10(1-abs(S11_COM)**2)\n",
    "    return Y11, Z_COM.real, S11_COM.real, S11_COM.imag, Q_COM, DSP_COM\n",
    "results = []\n",
    "origin_paras = np.genfromtxt('D:\\\\data\\\\7p\\\\out/MP60_0.5.csv',delimiter=',')\n",
    "for i in range(0,len(origin_paras)):\n",
    "    # x = np.array([eta,e,alpha,c,k2,npiezo_1,vb,m_ratio])\n",
    "    # x = np.append(origin_paras[i], 0.6)\n",
    "    freq = np.linspace(0.5E9, 1.5E9, 501)\n",
    "    x = origin_paras[i]\n",
    "    y = cacul(x)\n",
    "    [Y0_Y11, Y0_Z, Y0_SR, Y0_SI, Y0_Q, Y0_D] = YtoZS(y, freq)\n",
    "    Inputs = np.stack((Y0_Y11, Y0_Z, Y0_SR, Y0_SI, Y0_Q, Y0_D),axis=-1)\n",
    "    results.append(Inputs)\n",
    "\n",
    "result = np.array(results)\n",
    "file_path = 'D:\\\\data\\\\7p\\\\input2w/2w.npymusi.csv'\n",
    "mu = np.loadtxt(file_path)[:6]\n",
    "sigma = np.loadtxt(file_path)[6:]\n",
    "result = (result - mu)/ sigma\n",
    "mu = result.reshape(-1,6).mean(axis = 0)\n",
    "sigma = result.reshape(-1,6).std(axis = 0)\n",
    "# if (sigma == 0).all() != True:\n",
    "#     result = (result - mu )/ sigma\n",
    "# else:\n",
    "#     result = result - mu\n",
    "\n",
    "file_path = 'D:\\\\data\\\\7p\\\\input2w/2w_0.5.npy'\n",
    "with open(file_path,'wb') as f:\n",
    "    np.save(f, result)\n",
    "file_path1 = file_path + 'musi.csv'\n",
    "with open(file_path1,'w') as f:\n",
    "    np.savetxt(f,mu)   \n",
    "    np.savetxt(f,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:\\\\data\\\\7p\\\\input2w/2w.npymusi.csv'\n",
    "mu = np.loadtxt(file_path)[:6]\n",
    "sigma = np.loadtxt(file_path)[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607520"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(result[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501, 6)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_o = result*sigma + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0=np.exp((result_o[0,:,0]/20)*np.log(10))\n",
    "y1=np.exp((result[0,:,0]/20)*np.log(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4940381 , -1.22807298, -0.7273927 , -0.5615703 , -0.61782843,\n",
       "       -0.92798947, -1.950366  , -1.35165828, -0.75722462, -0.5523599 ,\n",
       "       -0.57464965, -0.83729094, -1.63186739, -1.51385693, -0.79513863,\n",
       "       -0.54851926, -0.53705737, -0.75673186, -1.40400544, -1.74011796,\n",
       "       -0.84224273, -0.55015648, -0.5048445 , -0.68481992, -1.22580658,\n",
       "       -2.09667539, -0.90009817, -0.5574383 , -0.47786759, -0.62048044,\n",
       "       -1.07927356, -2.89400254, -0.97095881, -0.5706047 , -0.45603443,\n",
       "       -0.56292745, -0.95490401, -2.82425274, -1.05819414, -0.58999031,\n",
       "       -0.43929586, -0.51158093, -0.84708354, -2.02619827, -1.16709727,\n",
       "       -0.61605534, -0.4276413 , -0.46601149, -0.7522355 , -1.64124737,\n",
       "       -1.30659655, -0.64943058, -0.42109755, -0.42590208, -0.66795779,\n",
       "       -1.38238275, -1.49344518, -0.69098394, -0.41973109, -0.39102071,\n",
       "       -0.59257494, -1.18595592, -1.7650276 , -0.74192259, -0.42365398,\n",
       "       -0.36120062, -0.52488633, -1.027166  , -2.23816954, -0.80395609,\n",
       "       -0.43303388, -0.33632598, -0.46401525, -0.89375101, -4.19829487,\n",
       "       -0.87957133, -0.44810923, -0.31632173, -0.4093136 , -0.77875956,\n",
       "       -2.25785721, -0.97252706, -0.4692109 , -0.30114691, -0.36029882,\n",
       "       -0.67788417, -1.72213493, -1.08882099, -0.49679318, -0.29079107,\n",
       "       -0.31661057, -0.58828586, -1.40598932, -1.23880482, -0.53147825,\n",
       "       -0.28527374, -0.27797982, -0.5080088 , -1.17887114, -1.44260972,\n",
       "       -0.57412179, -0.28464668, -0.24420638, -0.43566151, -1.00056212,\n",
       "       -1.74807619, -0.62591309, -0.28899946, -0.21514216, -0.37023057,\n",
       "       -0.85330083, -2.3286052 , -0.6885346 , -0.29846838, -0.19067869,\n",
       "       -0.31096533, -0.72767296, -3.22372112, -0.76442999, -0.31324967,\n",
       "       -0.17073806, -0.25730304, -0.61809546, -1.97297642, -0.85728473,\n",
       "       -0.33361795, -0.15526676, -0.20881801, -0.52099619, -1.51886623,\n",
       "       -0.97296196, -0.35995194, -0.14423195, -0.16518579, -0.43396252,\n",
       "       -1.23104947, -1.12153729, -0.39277066, -0.13762023, -0.12615695,\n",
       "       -0.3552987 , -1.01794943, -1.32247288, -0.43278551, -0.13543858,\n",
       "       -0.09153746, -0.28377651, -0.8476547 , -1.621462  , -0.48097789,\n",
       "       -0.13771769, -0.06117358, -0.21848566, -0.70524143, -2.17917117,\n",
       "       -0.53871938, -0.14451749, -0.03494016, -0.15873894, -0.58252063,\n",
       "       -3.26500461, -0.60796722, -0.15593537, -0.01273166, -0.10400933,\n",
       "       -0.47451701, -1.89121447, -0.69160108, -0.17211712,  0.00554467,\n",
       "       -0.05388631, -0.3779863 , -1.42395354, -0.79404596, -0.19327167,\n",
       "        0.01997366, -0.00804446, -0.29070044, -1.13054175, -0.92253592,\n",
       "       -0.21969049,  0.03063552,  0.03378015, -0.21106735, -0.91384661,\n",
       "       -1.09002135, -0.25177392,  0.03760761,  0.07180821, -0.13791286,\n",
       "       -0.74065378, -1.32321959, -0.29006757,  0.04096518,  0.10623176,\n",
       "       -0.07034772, -0.59556116, -1.6930631 , -0.33531446,  0.04078109,\n",
       "        0.13722603, -0.00768218, -0.47015104, -2.55338072, -0.38853244,\n",
       "        0.03712508,  0.16495934,  0.05063161, -0.35931404, -2.30843809,\n",
       "       -0.45113425,  0.03006277,  0.18960134,  0.10504006, -0.25971214,\n",
       "       -1.56279799, -0.52512261,  0.01965489,  0.2113301 ,  0.1559179 ,\n",
       "       -0.16904125, -1.18756945, -0.61342634,  0.00595717,  0.23033822,\n",
       "        0.203591  , -0.08564018, -0.93123222, -0.72052167, -0.01097854,\n",
       "        0.24683843,  0.24835446, -0.00826691, -0.73436004, -0.8536912 ,\n",
       "       -0.03110109,  0.26106883,  0.29048766,  0.06403765, -0.57329077,\n",
       "       -1.0259151 , -0.05435393,  0.27329839,  0.33026707,  0.13203716,\n",
       "       -0.43616961, -1.26385631, -0.08066448,  0.28383305,  0.36797755,\n",
       "        0.19635851, -0.31620128, -1.63789829, -0.10992686,  0.29302288,\n",
       "        0.4039226 ,  0.25753305, -0.20912694, -2.49148728, -0.14197498,\n",
       "        0.30127092,  0.4384337 ,  0.31602642, -0.11210311, -2.26977331,\n",
       "       -0.17654107,  0.30904403,  0.47187935,  0.37226091, -0.02314189,\n",
       "       -1.53259327, -0.21319361,  0.31688619,  0.50467366,  0.42663238,\n",
       "        0.05919265, -1.16581257, -0.2512456 ,  0.32543445,  0.53728485,\n",
       "        0.47952295,  0.13595828, -0.91953481, -0.28962339,  0.33543733,\n",
       "        0.57024372,  0.53130987,  0.20793624, -0.73471892, -0.32668731,\n",
       "        0.34777503,  0.60415196,  0.58237007,  0.27568845, -0.5882639 ,\n",
       "       -0.36000567,  0.36348004,  0.6396901 ,  0.63307937,  0.33958318,\n",
       "       -0.46911569, -0.38611149,  0.38375569,  0.67762433,  0.68380376,\n",
       "        0.39979384, -0.37161833, -0.40032907,  0.40998913,  0.71881084,\n",
       "        0.73487845,  0.45626909, -0.29316399, -0.39684005,  0.44375452,\n",
       "        0.76419495,  0.7865666 ,  0.50866643, -0.23337483, -0.36921312,\n",
       "        0.48680207,  0.81479979,  0.83898411,  0.55623234, -0.19410156,\n",
       "       -0.31151441,  0.54103063,  0.87169398,  0.89196549,  0.59759727,\n",
       "       -0.18023905, -0.21974111,  0.60844584,  0.93591566,  0.94482724,\n",
       "        0.63042482, -0.20208074, -0.09287036,  0.69111146,  1.00830156,\n",
       "        0.99595171,  0.6507891 , -0.28202455,  0.06718767,  0.79110294,\n",
       "        1.08909902,  1.04206095,  0.65198309, -0.47896297,  0.25680016,\n",
       "        0.91044856,  1.17706331,  1.07696922,  0.62192268, -1.05225707,\n",
       "        0.47238068,  1.05092126,  1.26732998,  1.0894643 ,  0.5362058 ,\n",
       "       -1.0622456 ,  0.7120751 ,  1.21301655,  1.34659576,  1.05943987,\n",
       "        0.33247821, -0.00541093,  0.97709123,  1.39114698,  1.38398362,\n",
       "        0.94791077, -0.27498466,  0.59137889,  1.27165749,  1.55282687,\n",
       "        1.31995415,  0.64888357, -0.12728164,  1.09135644,  1.58793369,\n",
       "        1.57805018,  1.04250795, -0.92845152,  0.94095961,  1.59066491,\n",
       "        1.75192019,  1.23505107, -0.23651184,  0.95802307,  1.68543168,\n",
       "        1.84301403,  1.16088626, -0.68029524,  1.28911065,  1.98166833,\n",
       "        1.53900462, -0.13594019,  1.1582382 ,  2.07844937,  1.08286008,\n",
       "        0.56750009,  2.06854834, -0.70037057,  0.91201806,  1.19682979,\n",
       "        1.34958601,  1.44963377,  1.52013261,  1.57116994,  1.60824143,\n",
       "        1.63481086,  1.65328254,  1.66543094,  1.67261464,  1.67589557,\n",
       "        1.67611292,  1.67393353,  1.66988856,  1.66440158,  1.65781051,\n",
       "        1.65038504,  1.6423405 ,  1.63384891,  1.62504785,  1.6160475 ,\n",
       "        1.60693627,  1.59778535,  1.58865235,  1.57958423,  1.5706198 ,\n",
       "        1.56179174,  1.55312841,  1.54465559,  1.53639818,  1.52838224,\n",
       "        1.52063759,  1.51320183,  1.50612712,  1.4994937 ,  1.49344073,\n",
       "        1.48825615,  1.48477607,  1.54776405,  1.66392883,  1.67639982,\n",
       "        1.6673768 ,  1.65261303,  1.63548631,  1.62377981,  1.6143401 ,\n",
       "        1.60123419,  1.5852422 ,  1.57193674,  1.56303558,  1.55391322,\n",
       "        1.54096049,  1.52548474,  1.51222105,  1.50334468,  1.49535104,\n",
       "        1.48418619,  1.46957279,  1.45519339,  1.44495345,  1.43782336,\n",
       "        1.42936233,  1.41701077,  1.40227543,  1.38956333,  1.38141869,\n",
       "        1.37514008,  1.36634256,  1.35350013,  1.339161  ,  1.32792194,\n",
       "        1.32127625,  1.31563813,  1.30676932,  1.29379974,  1.27994264,\n",
       "        1.26986885,  1.26439133,  1.25938587,  1.25066367,  1.23774265,\n",
       "        1.22429258,  1.21509641,  1.21058966,  1.20626627,  1.1978597 ,\n",
       "        1.18507326,  1.17190333,  1.16333058,  1.1596565 ,  1.15607627,\n",
       "        1.14812416,  1.13552271,  1.12249849,  1.11432014,  1.11136192,\n",
       "        1.10858174])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0,:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obs.PackClass as pk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16, 12])\n",
    "plt.subplot(3,2,1)\n",
    "plt.plot(freq,result[0][:,0])\n",
    "plt.plot(freq,result_o[0][:,0])\n",
    "# plt.grid('on')\n",
    "plt.subplot(3,2,2)\n",
    "plt.plot(freq,result[0][:,1])\n",
    "plt.plot(freq,result_o[0][:,1])\n",
    "plt.subplot(3,2,3)\n",
    "plt.plot(freq,result[0][:,2])\n",
    "plt.plot(freq,result_o[0][:,2])\n",
    "plt.subplot(3,2,4)\n",
    "plt.plot(freq,result[0][:,3])\n",
    "plt.plot(freq,result_o[0][:,3])\n",
    "plt.subplot(3,2,5)\n",
    "plt.plot(freq,result[0][:,4])   \n",
    "plt.plot(freq,result_o[0][:,4])\n",
    "plt.subplot(3,2,6)\n",
    "plt.plot(freq,result[0][:,5])\n",
    "plt.plot(freq,result_o[0][:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = pk.Plot_all(y1,y1,freq,name1='0',name2='1')\n",
    "plot.plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 501, 6)]     0           []                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1003)         503506      ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1003)         503506      ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1003)         503506      ['lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1003)         503506      ['lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1003)         503506      ['lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1003)         503506      ['lambda_5[0][0]']               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1003)         1007012     ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1003)         1007012     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1003)         1007012     ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1003)         1007012     ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1003)         1007012     ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1003)         1007012     ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1003)         1007012     ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1003)         1007012     ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1003)         1007012     ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 1003)         1007012     ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 1003)         1007012     ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 1003)         1007012     ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 6018)         0           ['dense_12[0][0]',               \n",
      "                                                                  'dense_13[0][0]',               \n",
      "                                                                  'dense_14[0][0]',               \n",
      "                                                                  'dense_15[0][0]',               \n",
      "                                                                  'dense_16[0][0]',               \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 511)          3075709     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 255)          130560      ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 127)          32512       ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 63)           8064        ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 31)           1984        ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 15)           480         ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 7)            112         ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,354,601\n",
      "Trainable params: 18,354,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.1730\n",
      "Epoch 1: val_loss improved from inf to 0.24869, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 15s 145ms/step - loss: 0.3079 - accuracy: 0.1730 - val_loss: 0.2487 - val_accuracy: 0.2188\n",
      "Epoch 2/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1817 - accuracy: 0.2631\n",
      "Epoch 2: val_loss improved from 0.24869 to 0.18164, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.1817 - accuracy: 0.2631 - val_loss: 0.1816 - val_accuracy: 0.2500\n",
      "Epoch 3/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.3225\n",
      "Epoch 3: val_loss improved from 0.18164 to 0.13899, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.1327 - accuracy: 0.3225 - val_loss: 0.1390 - val_accuracy: 0.3125\n",
      "Epoch 4/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.3829\n",
      "Epoch 4: val_loss improved from 0.13899 to 0.10951, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.1010 - accuracy: 0.3829 - val_loss: 0.1095 - val_accuracy: 0.3594\n",
      "Epoch 5/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.4259\n",
      "Epoch 5: val_loss improved from 0.10951 to 0.08989, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 0.0794 - accuracy: 0.4259 - val_loss: 0.0899 - val_accuracy: 0.4219\n",
      "Epoch 6/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.4587\n",
      "Epoch 6: val_loss improved from 0.08989 to 0.08034, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0655 - accuracy: 0.4587 - val_loss: 0.0803 - val_accuracy: 0.4609\n",
      "Epoch 7/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.4646\n",
      "Epoch 7: val_loss improved from 0.08034 to 0.07292, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0572 - accuracy: 0.4646 - val_loss: 0.0729 - val_accuracy: 0.4453\n",
      "Epoch 8/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.4583\n",
      "Epoch 8: val_loss improved from 0.07292 to 0.06537, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0499 - accuracy: 0.4583 - val_loss: 0.0654 - val_accuracy: 0.4453\n",
      "Epoch 9/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.4672\n",
      "Epoch 9: val_loss improved from 0.06537 to 0.06100, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0448 - accuracy: 0.4672 - val_loss: 0.0610 - val_accuracy: 0.4297\n",
      "Epoch 10/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.4779\n",
      "Epoch 10: val_loss improved from 0.06100 to 0.05695, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0408 - accuracy: 0.4779 - val_loss: 0.0569 - val_accuracy: 0.4688\n",
      "Epoch 11/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.4846\n",
      "Epoch 11: val_loss improved from 0.05695 to 0.05399, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0376 - accuracy: 0.4846 - val_loss: 0.0540 - val_accuracy: 0.4688\n",
      "Epoch 12/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.4869\n",
      "Epoch 12: val_loss improved from 0.05399 to 0.05066, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0350 - accuracy: 0.4869 - val_loss: 0.0507 - val_accuracy: 0.4375\n",
      "Epoch 13/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.4822\n",
      "Epoch 13: val_loss improved from 0.05066 to 0.04845, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 0.0335 - accuracy: 0.4822 - val_loss: 0.0485 - val_accuracy: 0.4609\n",
      "Epoch 14/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.4905\n",
      "Epoch 14: val_loss improved from 0.04845 to 0.04605, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 0.0308 - accuracy: 0.4905 - val_loss: 0.0460 - val_accuracy: 0.4609\n",
      "Epoch 15/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.4881\n",
      "Epoch 15: val_loss improved from 0.04605 to 0.04297, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 0.0290 - accuracy: 0.4881 - val_loss: 0.0430 - val_accuracy: 0.4766\n",
      "Epoch 16/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.5023\n",
      "Epoch 16: val_loss improved from 0.04297 to 0.04056, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0273 - accuracy: 0.5023 - val_loss: 0.0406 - val_accuracy: 0.5078\n",
      "Epoch 17/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.5134\n",
      "Epoch 17: val_loss improved from 0.04056 to 0.03975, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0256 - accuracy: 0.5134 - val_loss: 0.0398 - val_accuracy: 0.5312\n",
      "Epoch 18/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.5404\n",
      "Epoch 18: val_loss improved from 0.03975 to 0.03838, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0243 - accuracy: 0.5404 - val_loss: 0.0384 - val_accuracy: 0.5469\n",
      "Epoch 19/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.5401\n",
      "Epoch 19: val_loss improved from 0.03838 to 0.03689, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0235 - accuracy: 0.5401 - val_loss: 0.0369 - val_accuracy: 0.5391\n",
      "Epoch 20/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.5347\n",
      "Epoch 20: val_loss improved from 0.03689 to 0.03583, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 0.0227 - accuracy: 0.5347 - val_loss: 0.0358 - val_accuracy: 0.5234\n",
      "Epoch 21/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.5546\n",
      "Epoch 21: val_loss improved from 0.03583 to 0.03547, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 132ms/step - loss: 0.0220 - accuracy: 0.5546 - val_loss: 0.0355 - val_accuracy: 0.5469\n",
      "Epoch 22/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.5413\n",
      "Epoch 22: val_loss improved from 0.03547 to 0.03410, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 0.0218 - accuracy: 0.5413 - val_loss: 0.0341 - val_accuracy: 0.5156\n",
      "Epoch 23/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.5542\n",
      "Epoch 23: val_loss did not improve from 0.03410\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0212 - accuracy: 0.5542 - val_loss: 0.0342 - val_accuracy: 0.5703\n",
      "Epoch 24/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.5546\n",
      "Epoch 24: val_loss improved from 0.03410 to 0.03365, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 0.0204 - accuracy: 0.5546 - val_loss: 0.0336 - val_accuracy: 0.5234\n",
      "Epoch 25/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.5503\n",
      "Epoch 25: val_loss improved from 0.03365 to 0.03292, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0204 - accuracy: 0.5503 - val_loss: 0.0329 - val_accuracy: 0.5469\n",
      "Epoch 26/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.5607\n",
      "Epoch 26: val_loss improved from 0.03292 to 0.03237, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 0.0201 - accuracy: 0.5607 - val_loss: 0.0324 - val_accuracy: 0.5547\n",
      "Epoch 27/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.5618\n",
      "Epoch 27: val_loss improved from 0.03237 to 0.03220, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0198 - accuracy: 0.5618 - val_loss: 0.0322 - val_accuracy: 0.5156\n",
      "Epoch 28/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.5642\n",
      "Epoch 28: val_loss improved from 0.03220 to 0.03186, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0193 - accuracy: 0.5642 - val_loss: 0.0319 - val_accuracy: 0.5312\n",
      "Epoch 29/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.5617\n",
      "Epoch 29: val_loss did not improve from 0.03186\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0191 - accuracy: 0.5617 - val_loss: 0.0322 - val_accuracy: 0.5312\n",
      "Epoch 30/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.5651\n",
      "Epoch 30: val_loss improved from 0.03186 to 0.03145, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0190 - accuracy: 0.5651 - val_loss: 0.0315 - val_accuracy: 0.5781\n",
      "Epoch 31/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.5666\n",
      "Epoch 31: val_loss did not improve from 0.03145\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0186 - accuracy: 0.5666 - val_loss: 0.0317 - val_accuracy: 0.5469\n",
      "Epoch 32/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.5614\n",
      "Epoch 32: val_loss improved from 0.03145 to 0.03112, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0183 - accuracy: 0.5614 - val_loss: 0.0311 - val_accuracy: 0.5547\n",
      "Epoch 33/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.5729\n",
      "Epoch 33: val_loss did not improve from 0.03112\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0185 - accuracy: 0.5729 - val_loss: 0.0312 - val_accuracy: 0.5469\n",
      "Epoch 34/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.5653\n",
      "Epoch 34: val_loss did not improve from 0.03112\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0182 - accuracy: 0.5653 - val_loss: 0.0312 - val_accuracy: 0.5391\n",
      "Epoch 35/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.5714\n",
      "Epoch 35: val_loss improved from 0.03112 to 0.03076, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0181 - accuracy: 0.5714 - val_loss: 0.0308 - val_accuracy: 0.5312\n",
      "Epoch 36/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.5714\n",
      "Epoch 36: val_loss improved from 0.03076 to 0.03064, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0178 - accuracy: 0.5714 - val_loss: 0.0306 - val_accuracy: 0.5312\n",
      "Epoch 37/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.5644\n",
      "Epoch 37: val_loss did not improve from 0.03064\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0179 - accuracy: 0.5644 - val_loss: 0.0308 - val_accuracy: 0.5625\n",
      "Epoch 38/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.5692\n",
      "Epoch 38: val_loss did not improve from 0.03064\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0177 - accuracy: 0.5692 - val_loss: 0.0312 - val_accuracy: 0.5156\n",
      "Epoch 39/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.5685\n",
      "Epoch 39: val_loss improved from 0.03064 to 0.03006, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0175 - accuracy: 0.5685 - val_loss: 0.0301 - val_accuracy: 0.5781\n",
      "Epoch 40/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.5744\n",
      "Epoch 40: val_loss improved from 0.03006 to 0.02994, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0171 - accuracy: 0.5744 - val_loss: 0.0299 - val_accuracy: 0.5703\n",
      "Epoch 41/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.5756\n",
      "Epoch 41: val_loss did not improve from 0.02994\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0172 - accuracy: 0.5756 - val_loss: 0.0305 - val_accuracy: 0.5547\n",
      "Epoch 42/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.5678\n",
      "Epoch 42: val_loss improved from 0.02994 to 0.02985, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 0.0172 - accuracy: 0.5678 - val_loss: 0.0298 - val_accuracy: 0.5391\n",
      "Epoch 43/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.5755\n",
      "Epoch 43: val_loss improved from 0.02985 to 0.02968, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0173 - accuracy: 0.5755 - val_loss: 0.0297 - val_accuracy: 0.5625\n",
      "Epoch 44/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.5763\n",
      "Epoch 44: val_loss improved from 0.02968 to 0.02958, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 0.0170 - accuracy: 0.5763 - val_loss: 0.0296 - val_accuracy: 0.5469\n",
      "Epoch 45/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.5745\n",
      "Epoch 45: val_loss did not improve from 0.02958\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0169 - accuracy: 0.5745 - val_loss: 0.0299 - val_accuracy: 0.5625\n",
      "Epoch 46/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.5693\n",
      "Epoch 46: val_loss did not improve from 0.02958\n",
      "80/80 [==============================] - 7s 89ms/step - loss: 0.0169 - accuracy: 0.5693 - val_loss: 0.0297 - val_accuracy: 0.5312\n",
      "Epoch 47/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.5787\n",
      "Epoch 47: val_loss did not improve from 0.02958\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0168 - accuracy: 0.5787 - val_loss: 0.0297 - val_accuracy: 0.5547\n",
      "Epoch 48/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.5794\n",
      "Epoch 48: val_loss improved from 0.02958 to 0.02928, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 0.0166 - accuracy: 0.5794 - val_loss: 0.0293 - val_accuracy: 0.5469\n",
      "Epoch 49/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.5735\n",
      "Epoch 49: val_loss improved from 0.02928 to 0.02900, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 0.0164 - accuracy: 0.5735 - val_loss: 0.0290 - val_accuracy: 0.5625\n",
      "Epoch 50/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.5809\n",
      "Epoch 50: val_loss did not improve from 0.02900\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0165 - accuracy: 0.5809 - val_loss: 0.0292 - val_accuracy: 0.5547\n",
      "Epoch 51/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.5833\n",
      "Epoch 51: val_loss did not improve from 0.02900\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0164 - accuracy: 0.5833 - val_loss: 0.0292 - val_accuracy: 0.5703\n",
      "Epoch 52/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.5794\n",
      "Epoch 52: val_loss improved from 0.02900 to 0.02877, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0163 - accuracy: 0.5794 - val_loss: 0.0288 - val_accuracy: 0.5938\n",
      "Epoch 53/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.5693\n",
      "Epoch 53: val_loss improved from 0.02877 to 0.02849, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0165 - accuracy: 0.5693 - val_loss: 0.0285 - val_accuracy: 0.5469\n",
      "Epoch 54/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.5791\n",
      "Epoch 54: val_loss did not improve from 0.02849\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0163 - accuracy: 0.5791 - val_loss: 0.0294 - val_accuracy: 0.5703\n",
      "Epoch 55/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.5803\n",
      "Epoch 55: val_loss did not improve from 0.02849\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0163 - accuracy: 0.5803 - val_loss: 0.0287 - val_accuracy: 0.5469\n",
      "Epoch 56/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.5811\n",
      "Epoch 56: val_loss did not improve from 0.02849\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0161 - accuracy: 0.5811 - val_loss: 0.0291 - val_accuracy: 0.6016\n",
      "Epoch 57/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.5903\n",
      "Epoch 57: val_loss did not improve from 0.02849\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0159 - accuracy: 0.5903 - val_loss: 0.0287 - val_accuracy: 0.5625\n",
      "Epoch 58/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.5850\n",
      "Epoch 58: val_loss did not improve from 0.02849\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0161 - accuracy: 0.5850 - val_loss: 0.0289 - val_accuracy: 0.5234\n",
      "Epoch 59/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.5796\n",
      "Epoch 59: val_loss improved from 0.02849 to 0.02842, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0157 - accuracy: 0.5796 - val_loss: 0.0284 - val_accuracy: 0.5938\n",
      "Epoch 60/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.5813\n",
      "Epoch 60: val_loss did not improve from 0.02842\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0159 - accuracy: 0.5813 - val_loss: 0.0284 - val_accuracy: 0.5547\n",
      "Epoch 61/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.5702\n",
      "Epoch 61: val_loss did not improve from 0.02842\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0160 - accuracy: 0.5702 - val_loss: 0.0286 - val_accuracy: 0.5859\n",
      "Epoch 62/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.5869\n",
      "Epoch 62: val_loss did not improve from 0.02842\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0159 - accuracy: 0.5869 - val_loss: 0.0285 - val_accuracy: 0.5703\n",
      "Epoch 63/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.5787\n",
      "Epoch 63: val_loss improved from 0.02842 to 0.02830, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0155 - accuracy: 0.5787 - val_loss: 0.0283 - val_accuracy: 0.5547\n",
      "Epoch 64/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.5890\n",
      "Epoch 64: val_loss did not improve from 0.02830\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0155 - accuracy: 0.5890 - val_loss: 0.0286 - val_accuracy: 0.5625\n",
      "Epoch 65/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.5839\n",
      "Epoch 65: val_loss improved from 0.02830 to 0.02818, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0159 - accuracy: 0.5839 - val_loss: 0.0282 - val_accuracy: 0.5859\n",
      "Epoch 66/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.5799\n",
      "Epoch 66: val_loss did not improve from 0.02818\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0157 - accuracy: 0.5799 - val_loss: 0.0285 - val_accuracy: 0.5625\n",
      "Epoch 67/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.5842\n",
      "Epoch 67: val_loss did not improve from 0.02818\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0159 - accuracy: 0.5842 - val_loss: 0.0293 - val_accuracy: 0.5547\n",
      "Epoch 68/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.5836\n",
      "Epoch 68: val_loss did not improve from 0.02818\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0155 - accuracy: 0.5836 - val_loss: 0.0287 - val_accuracy: 0.5703\n",
      "Epoch 69/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.5780\n",
      "Epoch 69: val_loss did not improve from 0.02818\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0152 - accuracy: 0.5780 - val_loss: 0.0283 - val_accuracy: 0.5391\n",
      "Epoch 70/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.5800\n",
      "Epoch 70: val_loss improved from 0.02818 to 0.02817, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 0.0154 - accuracy: 0.5800 - val_loss: 0.0282 - val_accuracy: 0.5781\n",
      "Epoch 71/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.5870\n",
      "Epoch 71: val_loss improved from 0.02817 to 0.02804, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 0.0153 - accuracy: 0.5870 - val_loss: 0.0280 - val_accuracy: 0.5781\n",
      "Epoch 72/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.5845\n",
      "Epoch 72: val_loss improved from 0.02804 to 0.02795, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0154 - accuracy: 0.5845 - val_loss: 0.0279 - val_accuracy: 0.5703\n",
      "Epoch 73/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.5747\n",
      "Epoch 73: val_loss did not improve from 0.02795\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0152 - accuracy: 0.5747 - val_loss: 0.0282 - val_accuracy: 0.5859\n",
      "Epoch 74/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.5918\n",
      "Epoch 74: val_loss did not improve from 0.02795\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0150 - accuracy: 0.5918 - val_loss: 0.0281 - val_accuracy: 0.5781\n",
      "Epoch 75/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.5876\n",
      "Epoch 75: val_loss did not improve from 0.02795\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0151 - accuracy: 0.5876 - val_loss: 0.0281 - val_accuracy: 0.5547\n",
      "Epoch 76/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.5807\n",
      "Epoch 76: val_loss did not improve from 0.02795\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0152 - accuracy: 0.5807 - val_loss: 0.0288 - val_accuracy: 0.5391\n",
      "Epoch 77/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.5801\n",
      "Epoch 77: val_loss did not improve from 0.02795\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0151 - accuracy: 0.5801 - val_loss: 0.0280 - val_accuracy: 0.5312\n",
      "Epoch 78/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.5847\n",
      "Epoch 78: val_loss did not improve from 0.02795\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0150 - accuracy: 0.5847 - val_loss: 0.0281 - val_accuracy: 0.5781\n",
      "Epoch 79/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.5846\n",
      "Epoch 79: val_loss did not improve from 0.02795\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0148 - accuracy: 0.5846 - val_loss: 0.0281 - val_accuracy: 0.5547\n",
      "Epoch 80/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.5872\n",
      "Epoch 80: val_loss did not improve from 0.02795\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0149 - accuracy: 0.5872 - val_loss: 0.0285 - val_accuracy: 0.5859\n",
      "Epoch 81/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.5852\n",
      "Epoch 81: val_loss did not improve from 0.02795\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0150 - accuracy: 0.5852 - val_loss: 0.0281 - val_accuracy: 0.5781\n",
      "Epoch 82/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.5864\n",
      "Epoch 82: val_loss improved from 0.02795 to 0.02791, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 0.0149 - accuracy: 0.5864 - val_loss: 0.0279 - val_accuracy: 0.5859\n",
      "Epoch 83/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.5930\n",
      "Epoch 83: val_loss improved from 0.02791 to 0.02773, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0146 - accuracy: 0.5930 - val_loss: 0.0277 - val_accuracy: 0.6094\n",
      "Epoch 84/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.5767\n",
      "Epoch 84: val_loss improved from 0.02773 to 0.02761, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0145 - accuracy: 0.5767 - val_loss: 0.0276 - val_accuracy: 0.5859\n",
      "Epoch 85/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.5743\n",
      "Epoch 85: val_loss did not improve from 0.02761\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0148 - accuracy: 0.5743 - val_loss: 0.0278 - val_accuracy: 0.5469\n",
      "Epoch 86/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.5893\n",
      "Epoch 86: val_loss did not improve from 0.02761\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0147 - accuracy: 0.5893 - val_loss: 0.0285 - val_accuracy: 0.5781\n",
      "Epoch 87/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.5894\n",
      "Epoch 87: val_loss improved from 0.02761 to 0.02756, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0143 - accuracy: 0.5894 - val_loss: 0.0276 - val_accuracy: 0.6094\n",
      "Epoch 88/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.5846\n",
      "Epoch 88: val_loss did not improve from 0.02756\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0145 - accuracy: 0.5846 - val_loss: 0.0281 - val_accuracy: 0.5781\n",
      "Epoch 89/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.5830\n",
      "Epoch 89: val_loss did not improve from 0.02756\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0145 - accuracy: 0.5830 - val_loss: 0.0279 - val_accuracy: 0.5781\n",
      "Epoch 90/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.5870\n",
      "Epoch 90: val_loss did not improve from 0.02756\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0142 - accuracy: 0.5870 - val_loss: 0.0277 - val_accuracy: 0.6016\n",
      "Epoch 91/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.5875\n",
      "Epoch 91: val_loss did not improve from 0.02756\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0142 - accuracy: 0.5875 - val_loss: 0.0282 - val_accuracy: 0.5625\n",
      "Epoch 92/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.5876\n",
      "Epoch 92: val_loss did not improve from 0.02756\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0142 - accuracy: 0.5876 - val_loss: 0.0281 - val_accuracy: 0.5703\n",
      "Epoch 93/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.5796\n",
      "Epoch 93: val_loss did not improve from 0.02756\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0141 - accuracy: 0.5796 - val_loss: 0.0290 - val_accuracy: 0.5859\n",
      "Epoch 94/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.5891\n",
      "Epoch 94: val_loss did not improve from 0.02756\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0139 - accuracy: 0.5891 - val_loss: 0.0277 - val_accuracy: 0.5547\n",
      "Epoch 95/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.5797\n",
      "Epoch 95: val_loss did not improve from 0.02756\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0140 - accuracy: 0.5797 - val_loss: 0.0279 - val_accuracy: 0.5703\n",
      "Epoch 96/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.5896\n",
      "Epoch 96: val_loss did not improve from 0.02756\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0137 - accuracy: 0.5896 - val_loss: 0.0285 - val_accuracy: 0.5625\n",
      "Epoch 97/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.5934\n",
      "Epoch 97: val_loss improved from 0.02756 to 0.02723, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 0.0141 - accuracy: 0.5934 - val_loss: 0.0272 - val_accuracy: 0.5547\n",
      "Epoch 98/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.5971\n",
      "Epoch 98: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0138 - accuracy: 0.5971 - val_loss: 0.0278 - val_accuracy: 0.5703\n",
      "Epoch 99/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.5920\n",
      "Epoch 99: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0136 - accuracy: 0.5920 - val_loss: 0.0277 - val_accuracy: 0.6172\n",
      "Epoch 100/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.5849\n",
      "Epoch 100: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0136 - accuracy: 0.5849 - val_loss: 0.0278 - val_accuracy: 0.5703\n",
      "Epoch 101/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.5809\n",
      "Epoch 101: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0137 - accuracy: 0.5809 - val_loss: 0.0276 - val_accuracy: 0.5703\n",
      "Epoch 102/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.5975\n",
      "Epoch 102: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0136 - accuracy: 0.5975 - val_loss: 0.0280 - val_accuracy: 0.6094\n",
      "Epoch 103/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.5974\n",
      "Epoch 103: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0133 - accuracy: 0.5974 - val_loss: 0.0281 - val_accuracy: 0.5391\n",
      "Epoch 104/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.5874\n",
      "Epoch 104: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0132 - accuracy: 0.5874 - val_loss: 0.0276 - val_accuracy: 0.5859\n",
      "Epoch 105/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.5842\n",
      "Epoch 105: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0138 - accuracy: 0.5842 - val_loss: 0.0275 - val_accuracy: 0.5859\n",
      "Epoch 106/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.6004\n",
      "Epoch 106: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0129 - accuracy: 0.6004 - val_loss: 0.0287 - val_accuracy: 0.5781\n",
      "Epoch 107/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.5944\n",
      "Epoch 107: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0131 - accuracy: 0.5944 - val_loss: 0.0285 - val_accuracy: 0.5703\n",
      "Epoch 108/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.5959\n",
      "Epoch 108: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0130 - accuracy: 0.5959 - val_loss: 0.0275 - val_accuracy: 0.6094\n",
      "Epoch 109/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.5897\n",
      "Epoch 109: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0132 - accuracy: 0.5897 - val_loss: 0.0277 - val_accuracy: 0.5625\n",
      "Epoch 110/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.5945\n",
      "Epoch 110: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0127 - accuracy: 0.5945 - val_loss: 0.0283 - val_accuracy: 0.5625\n",
      "Epoch 111/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.5914\n",
      "Epoch 111: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0129 - accuracy: 0.5914 - val_loss: 0.0275 - val_accuracy: 0.5781\n",
      "Epoch 112/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.6009\n",
      "Epoch 112: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0129 - accuracy: 0.6009 - val_loss: 0.0289 - val_accuracy: 0.5469\n",
      "Epoch 113/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.6044\n",
      "Epoch 113: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0130 - accuracy: 0.6044 - val_loss: 0.0281 - val_accuracy: 0.6016\n",
      "Epoch 114/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.5990\n",
      "Epoch 114: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0126 - accuracy: 0.5990 - val_loss: 0.0273 - val_accuracy: 0.5703\n",
      "Epoch 115/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.5970\n",
      "Epoch 115: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0125 - accuracy: 0.5970 - val_loss: 0.0280 - val_accuracy: 0.5938\n",
      "Epoch 116/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.6012\n",
      "Epoch 116: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0127 - accuracy: 0.6012 - val_loss: 0.0280 - val_accuracy: 0.6094\n",
      "Epoch 117/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.6011\n",
      "Epoch 117: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0125 - accuracy: 0.6011 - val_loss: 0.0283 - val_accuracy: 0.6016\n",
      "Epoch 118/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.5988\n",
      "Epoch 118: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0122 - accuracy: 0.5988 - val_loss: 0.0280 - val_accuracy: 0.5703\n",
      "Epoch 119/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.6044\n",
      "Epoch 119: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0124 - accuracy: 0.6044 - val_loss: 0.0277 - val_accuracy: 0.5938\n",
      "Epoch 120/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.6134\n",
      "Epoch 120: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0124 - accuracy: 0.6134 - val_loss: 0.0283 - val_accuracy: 0.5938\n",
      "Epoch 121/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.6020\n",
      "Epoch 121: val_loss did not improve from 0.02723\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0123 - accuracy: 0.6020 - val_loss: 0.0277 - val_accuracy: 0.6094\n",
      "Epoch 122/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.6027\n",
      "Epoch 122: val_loss improved from 0.02723 to 0.02693, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0124 - accuracy: 0.6027 - val_loss: 0.0269 - val_accuracy: 0.5781\n",
      "Epoch 123/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.6065\n",
      "Epoch 123: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0120 - accuracy: 0.6065 - val_loss: 0.0286 - val_accuracy: 0.5625\n",
      "Epoch 124/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.5948\n",
      "Epoch 124: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0123 - accuracy: 0.5948 - val_loss: 0.0284 - val_accuracy: 0.5781\n",
      "Epoch 125/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.6140\n",
      "Epoch 125: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0119 - accuracy: 0.6140 - val_loss: 0.0287 - val_accuracy: 0.5391\n",
      "Epoch 126/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.6097\n",
      "Epoch 126: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0120 - accuracy: 0.6097 - val_loss: 0.0275 - val_accuracy: 0.6016\n",
      "Epoch 127/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.6061\n",
      "Epoch 127: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0119 - accuracy: 0.6061 - val_loss: 0.0295 - val_accuracy: 0.5781\n",
      "Epoch 128/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.6136\n",
      "Epoch 128: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0120 - accuracy: 0.6136 - val_loss: 0.0276 - val_accuracy: 0.5703\n",
      "Epoch 129/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.6056\n",
      "Epoch 129: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0117 - accuracy: 0.6056 - val_loss: 0.0273 - val_accuracy: 0.5859\n",
      "Epoch 130/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.6113\n",
      "Epoch 130: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0118 - accuracy: 0.6113 - val_loss: 0.0273 - val_accuracy: 0.6016\n",
      "Epoch 131/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.6114\n",
      "Epoch 131: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0119 - accuracy: 0.6114 - val_loss: 0.0277 - val_accuracy: 0.5703\n",
      "Epoch 132/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.6124\n",
      "Epoch 132: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0117 - accuracy: 0.6124 - val_loss: 0.0282 - val_accuracy: 0.5859\n",
      "Epoch 133/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.6135\n",
      "Epoch 133: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0117 - accuracy: 0.6135 - val_loss: 0.0272 - val_accuracy: 0.6016\n",
      "Epoch 134/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.6135\n",
      "Epoch 134: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0114 - accuracy: 0.6135 - val_loss: 0.0277 - val_accuracy: 0.5938\n",
      "Epoch 135/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.6203\n",
      "Epoch 135: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0116 - accuracy: 0.6203 - val_loss: 0.0273 - val_accuracy: 0.6172\n",
      "Epoch 136/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.6063\n",
      "Epoch 136: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0116 - accuracy: 0.6063 - val_loss: 0.0275 - val_accuracy: 0.5938\n",
      "Epoch 137/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.6169\n",
      "Epoch 137: val_loss did not improve from 0.02693\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0112 - accuracy: 0.6169 - val_loss: 0.0270 - val_accuracy: 0.6172\n",
      "Epoch 138/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.6139\n",
      "Epoch 138: val_loss improved from 0.02693 to 0.02692, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 0.0116 - accuracy: 0.6139 - val_loss: 0.0269 - val_accuracy: 0.5859\n",
      "Epoch 139/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.6126\n",
      "Epoch 139: val_loss did not improve from 0.02692\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0114 - accuracy: 0.6126 - val_loss: 0.0274 - val_accuracy: 0.5938\n",
      "Epoch 140/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.6241\n",
      "Epoch 140: val_loss did not improve from 0.02692\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0115 - accuracy: 0.6241 - val_loss: 0.0277 - val_accuracy: 0.6172\n",
      "Epoch 141/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.6105\n",
      "Epoch 141: val_loss improved from 0.02692 to 0.02661, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0114 - accuracy: 0.6105 - val_loss: 0.0266 - val_accuracy: 0.5938\n",
      "Epoch 142/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.6214\n",
      "Epoch 142: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0113 - accuracy: 0.6214 - val_loss: 0.0282 - val_accuracy: 0.5938\n",
      "Epoch 143/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.6206\n",
      "Epoch 143: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0112 - accuracy: 0.6206 - val_loss: 0.0270 - val_accuracy: 0.6094\n",
      "Epoch 144/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.6105\n",
      "Epoch 144: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0114 - accuracy: 0.6105 - val_loss: 0.0274 - val_accuracy: 0.6250\n",
      "Epoch 145/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.6235\n",
      "Epoch 145: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0109 - accuracy: 0.6235 - val_loss: 0.0275 - val_accuracy: 0.6016\n",
      "Epoch 146/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.6175\n",
      "Epoch 146: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0109 - accuracy: 0.6175 - val_loss: 0.0269 - val_accuracy: 0.5938\n",
      "Epoch 147/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.6216\n",
      "Epoch 147: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0114 - accuracy: 0.6216 - val_loss: 0.0270 - val_accuracy: 0.6016\n",
      "Epoch 148/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.6150\n",
      "Epoch 148: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0114 - accuracy: 0.6150 - val_loss: 0.0269 - val_accuracy: 0.6172\n",
      "Epoch 149/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.6180\n",
      "Epoch 149: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0111 - accuracy: 0.6180 - val_loss: 0.0272 - val_accuracy: 0.6016\n",
      "Epoch 150/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.6193\n",
      "Epoch 150: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0110 - accuracy: 0.6193 - val_loss: 0.0278 - val_accuracy: 0.5781\n",
      "Epoch 151/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.6317\n",
      "Epoch 151: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0108 - accuracy: 0.6317 - val_loss: 0.0270 - val_accuracy: 0.6016\n",
      "Epoch 152/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.6240\n",
      "Epoch 152: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0110 - accuracy: 0.6240 - val_loss: 0.0273 - val_accuracy: 0.6328\n",
      "Epoch 153/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.6255\n",
      "Epoch 153: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0108 - accuracy: 0.6255 - val_loss: 0.0276 - val_accuracy: 0.6172\n",
      "Epoch 154/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.6190\n",
      "Epoch 154: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0109 - accuracy: 0.6190 - val_loss: 0.0271 - val_accuracy: 0.5938\n",
      "Epoch 155/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.6231\n",
      "Epoch 155: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0110 - accuracy: 0.6231 - val_loss: 0.0270 - val_accuracy: 0.6016\n",
      "Epoch 156/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.6228\n",
      "Epoch 156: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0109 - accuracy: 0.6228 - val_loss: 0.0269 - val_accuracy: 0.6016\n",
      "Epoch 157/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.6293\n",
      "Epoch 157: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0108 - accuracy: 0.6293 - val_loss: 0.0274 - val_accuracy: 0.5781\n",
      "Epoch 158/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.6192\n",
      "Epoch 158: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0107 - accuracy: 0.6192 - val_loss: 0.0268 - val_accuracy: 0.6328\n",
      "Epoch 159/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.6256\n",
      "Epoch 159: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0108 - accuracy: 0.6256 - val_loss: 0.0269 - val_accuracy: 0.6016\n",
      "Epoch 160/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.6285\n",
      "Epoch 160: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0107 - accuracy: 0.6285 - val_loss: 0.0284 - val_accuracy: 0.5625\n",
      "Epoch 161/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.6246\n",
      "Epoch 161: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0103 - accuracy: 0.6246 - val_loss: 0.0277 - val_accuracy: 0.6016\n",
      "Epoch 162/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.6186\n",
      "Epoch 162: val_loss did not improve from 0.02661\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0108 - accuracy: 0.6186 - val_loss: 0.0273 - val_accuracy: 0.6016\n",
      "Epoch 163/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.6302\n",
      "Epoch 163: val_loss improved from 0.02661 to 0.02646, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 0.0108 - accuracy: 0.6302 - val_loss: 0.0265 - val_accuracy: 0.5859\n",
      "Epoch 164/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.6322\n",
      "Epoch 164: val_loss did not improve from 0.02646\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0106 - accuracy: 0.6322 - val_loss: 0.0268 - val_accuracy: 0.6172\n",
      "Epoch 165/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.6341\n",
      "Epoch 165: val_loss did not improve from 0.02646\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0106 - accuracy: 0.6341 - val_loss: 0.0269 - val_accuracy: 0.6562\n",
      "Epoch 166/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.6263\n",
      "Epoch 166: val_loss did not improve from 0.02646\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0106 - accuracy: 0.6263 - val_loss: 0.0267 - val_accuracy: 0.5859\n",
      "Epoch 167/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.6290\n",
      "Epoch 167: val_loss improved from 0.02646 to 0.02644, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0105 - accuracy: 0.6290 - val_loss: 0.0264 - val_accuracy: 0.6250\n",
      "Epoch 168/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.6259\n",
      "Epoch 168: val_loss did not improve from 0.02644\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0103 - accuracy: 0.6259 - val_loss: 0.0272 - val_accuracy: 0.6016\n",
      "Epoch 169/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.6334\n",
      "Epoch 169: val_loss did not improve from 0.02644\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0103 - accuracy: 0.6334 - val_loss: 0.0269 - val_accuracy: 0.5703\n",
      "Epoch 170/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.6380\n",
      "Epoch 170: val_loss did not improve from 0.02644\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0104 - accuracy: 0.6380 - val_loss: 0.0266 - val_accuracy: 0.6094\n",
      "Epoch 171/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.6315\n",
      "Epoch 171: val_loss did not improve from 0.02644\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0107 - accuracy: 0.6315 - val_loss: 0.0267 - val_accuracy: 0.6172\n",
      "Epoch 172/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.6409\n",
      "Epoch 172: val_loss did not improve from 0.02644\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0102 - accuracy: 0.6409 - val_loss: 0.0272 - val_accuracy: 0.5859\n",
      "Epoch 173/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.6273\n",
      "Epoch 173: val_loss did not improve from 0.02644\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0103 - accuracy: 0.6273 - val_loss: 0.0269 - val_accuracy: 0.5859\n",
      "Epoch 174/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.6326\n",
      "Epoch 174: val_loss did not improve from 0.02644\n",
      "80/80 [==============================] - 12s 156ms/step - loss: 0.0105 - accuracy: 0.6326 - val_loss: 0.0265 - val_accuracy: 0.6016\n",
      "Epoch 175/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.6324\n",
      "Epoch 175: val_loss did not improve from 0.02644\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0105 - accuracy: 0.6324 - val_loss: 0.0266 - val_accuracy: 0.6016\n",
      "Epoch 176/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.6364\n",
      "Epoch 176: val_loss improved from 0.02644 to 0.02589, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 11s 137ms/step - loss: 0.0103 - accuracy: 0.6364 - val_loss: 0.0259 - val_accuracy: 0.6016\n",
      "Epoch 177/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.6354\n",
      "Epoch 177: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 9s 107ms/step - loss: 0.0101 - accuracy: 0.6354 - val_loss: 0.0266 - val_accuracy: 0.6172\n",
      "Epoch 178/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.6347\n",
      "Epoch 178: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0104 - accuracy: 0.6347 - val_loss: 0.0264 - val_accuracy: 0.5938\n",
      "Epoch 179/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.6362\n",
      "Epoch 179: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 8s 105ms/step - loss: 0.0103 - accuracy: 0.6362 - val_loss: 0.0265 - val_accuracy: 0.6016\n",
      "Epoch 180/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.6319\n",
      "Epoch 180: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 0.0103 - accuracy: 0.6319 - val_loss: 0.0261 - val_accuracy: 0.6172\n",
      "Epoch 181/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.6412\n",
      "Epoch 181: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 9s 107ms/step - loss: 0.0103 - accuracy: 0.6412 - val_loss: 0.0263 - val_accuracy: 0.5938\n",
      "Epoch 182/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.6343\n",
      "Epoch 182: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 9s 107ms/step - loss: 0.0101 - accuracy: 0.6343 - val_loss: 0.0262 - val_accuracy: 0.5859\n",
      "Epoch 183/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.6338\n",
      "Epoch 183: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 12s 150ms/step - loss: 0.0101 - accuracy: 0.6338 - val_loss: 0.0271 - val_accuracy: 0.5781\n",
      "Epoch 184/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.6423\n",
      "Epoch 184: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 12s 150ms/step - loss: 0.0099 - accuracy: 0.6423 - val_loss: 0.0260 - val_accuracy: 0.6094\n",
      "Epoch 185/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.6372\n",
      "Epoch 185: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 0.0103 - accuracy: 0.6372 - val_loss: 0.0267 - val_accuracy: 0.5938\n",
      "Epoch 186/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.6359\n",
      "Epoch 186: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 9s 106ms/step - loss: 0.0102 - accuracy: 0.6359 - val_loss: 0.0261 - val_accuracy: 0.6094\n",
      "Epoch 187/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.6391\n",
      "Epoch 187: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 0.0102 - accuracy: 0.6391 - val_loss: 0.0271 - val_accuracy: 0.5703\n",
      "Epoch 188/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.6392\n",
      "Epoch 188: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 9s 115ms/step - loss: 0.0103 - accuracy: 0.6392 - val_loss: 0.0263 - val_accuracy: 0.6094\n",
      "Epoch 189/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.6468\n",
      "Epoch 189: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 0.0100 - accuracy: 0.6468 - val_loss: 0.0264 - val_accuracy: 0.6016\n",
      "Epoch 190/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.6346\n",
      "Epoch 190: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 9s 110ms/step - loss: 0.0100 - accuracy: 0.6346 - val_loss: 0.0263 - val_accuracy: 0.6094\n",
      "Epoch 191/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.6371\n",
      "Epoch 191: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0100 - accuracy: 0.6371 - val_loss: 0.0263 - val_accuracy: 0.5938\n",
      "Epoch 192/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.6368\n",
      "Epoch 192: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0099 - accuracy: 0.6368 - val_loss: 0.0263 - val_accuracy: 0.5938\n",
      "Epoch 193/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.6466\n",
      "Epoch 193: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0100 - accuracy: 0.6466 - val_loss: 0.0266 - val_accuracy: 0.6016\n",
      "Epoch 194/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.6491\n",
      "Epoch 194: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0100 - accuracy: 0.6491 - val_loss: 0.0268 - val_accuracy: 0.5938\n",
      "Epoch 195/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.6335\n",
      "Epoch 195: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0102 - accuracy: 0.6335 - val_loss: 0.0266 - val_accuracy: 0.5938\n",
      "Epoch 196/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.6441\n",
      "Epoch 196: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0098 - accuracy: 0.6441 - val_loss: 0.0260 - val_accuracy: 0.6094\n",
      "Epoch 197/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.6530\n",
      "Epoch 197: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0099 - accuracy: 0.6530 - val_loss: 0.0269 - val_accuracy: 0.5938\n",
      "Epoch 198/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.6411\n",
      "Epoch 198: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0100 - accuracy: 0.6411 - val_loss: 0.0263 - val_accuracy: 0.6094\n",
      "Epoch 199/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.6422\n",
      "Epoch 199: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0098 - accuracy: 0.6422 - val_loss: 0.0264 - val_accuracy: 0.5938\n",
      "Epoch 200/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.6397\n",
      "Epoch 200: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0099 - accuracy: 0.6397 - val_loss: 0.0260 - val_accuracy: 0.6016\n",
      "Epoch 201/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.6479\n",
      "Epoch 201: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0100 - accuracy: 0.6479 - val_loss: 0.0266 - val_accuracy: 0.5859\n",
      "Epoch 202/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.6403\n",
      "Epoch 202: val_loss did not improve from 0.02589\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0100 - accuracy: 0.6403 - val_loss: 0.0260 - val_accuracy: 0.5938\n",
      "Epoch 203/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.6418\n",
      "Epoch 203: val_loss improved from 0.02589 to 0.02561, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0098 - accuracy: 0.6418 - val_loss: 0.0256 - val_accuracy: 0.5781\n",
      "Epoch 204/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.6498\n",
      "Epoch 204: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 89ms/step - loss: 0.0098 - accuracy: 0.6498 - val_loss: 0.0262 - val_accuracy: 0.5781\n",
      "Epoch 205/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.6504\n",
      "Epoch 205: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 89ms/step - loss: 0.0097 - accuracy: 0.6504 - val_loss: 0.0268 - val_accuracy: 0.5781\n",
      "Epoch 206/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.6433\n",
      "Epoch 206: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0099 - accuracy: 0.6433 - val_loss: 0.0262 - val_accuracy: 0.5938\n",
      "Epoch 207/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.6495\n",
      "Epoch 207: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0098 - accuracy: 0.6495 - val_loss: 0.0260 - val_accuracy: 0.6016\n",
      "Epoch 208/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.6479\n",
      "Epoch 208: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0098 - accuracy: 0.6479 - val_loss: 0.0265 - val_accuracy: 0.6016\n",
      "Epoch 209/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.6480\n",
      "Epoch 209: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0098 - accuracy: 0.6480 - val_loss: 0.0270 - val_accuracy: 0.5781\n",
      "Epoch 210/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.6418\n",
      "Epoch 210: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0099 - accuracy: 0.6418 - val_loss: 0.0261 - val_accuracy: 0.6094\n",
      "Epoch 211/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.6417\n",
      "Epoch 211: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0098 - accuracy: 0.6417 - val_loss: 0.0269 - val_accuracy: 0.5625\n",
      "Epoch 212/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6580\n",
      "Epoch 212: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 89ms/step - loss: 0.0096 - accuracy: 0.6580 - val_loss: 0.0258 - val_accuracy: 0.6328\n",
      "Epoch 213/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.6558\n",
      "Epoch 213: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 89ms/step - loss: 0.0098 - accuracy: 0.6558 - val_loss: 0.0263 - val_accuracy: 0.6094\n",
      "Epoch 214/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.6501\n",
      "Epoch 214: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0097 - accuracy: 0.6501 - val_loss: 0.0258 - val_accuracy: 0.5938\n",
      "Epoch 215/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6556\n",
      "Epoch 215: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0096 - accuracy: 0.6556 - val_loss: 0.0257 - val_accuracy: 0.6328\n",
      "Epoch 216/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.6506\n",
      "Epoch 216: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0097 - accuracy: 0.6506 - val_loss: 0.0261 - val_accuracy: 0.6328\n",
      "Epoch 217/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.6453\n",
      "Epoch 217: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0098 - accuracy: 0.6453 - val_loss: 0.0265 - val_accuracy: 0.5859\n",
      "Epoch 218/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.6486\n",
      "Epoch 218: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0097 - accuracy: 0.6486 - val_loss: 0.0260 - val_accuracy: 0.6016\n",
      "Epoch 219/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.6506\n",
      "Epoch 219: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0095 - accuracy: 0.6506 - val_loss: 0.0262 - val_accuracy: 0.6094\n",
      "Epoch 220/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.6490\n",
      "Epoch 220: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 89ms/step - loss: 0.0097 - accuracy: 0.6490 - val_loss: 0.0261 - val_accuracy: 0.5859\n",
      "Epoch 221/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.6528\n",
      "Epoch 221: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0097 - accuracy: 0.6528 - val_loss: 0.0256 - val_accuracy: 0.6016\n",
      "Epoch 222/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6520\n",
      "Epoch 222: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0096 - accuracy: 0.6520 - val_loss: 0.0262 - val_accuracy: 0.5938\n",
      "Epoch 223/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6578\n",
      "Epoch 223: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0096 - accuracy: 0.6578 - val_loss: 0.0261 - val_accuracy: 0.5781\n",
      "Epoch 224/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6525\n",
      "Epoch 224: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0096 - accuracy: 0.6525 - val_loss: 0.0259 - val_accuracy: 0.5859\n",
      "Epoch 225/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6517\n",
      "Epoch 225: val_loss did not improve from 0.02561\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0096 - accuracy: 0.6517 - val_loss: 0.0261 - val_accuracy: 0.6016\n",
      "Epoch 226/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6450\n",
      "Epoch 226: val_loss improved from 0.02561 to 0.02553, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 131ms/step - loss: 0.0096 - accuracy: 0.6450 - val_loss: 0.0255 - val_accuracy: 0.6172\n",
      "Epoch 227/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.6529\n",
      "Epoch 227: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0095 - accuracy: 0.6529 - val_loss: 0.0256 - val_accuracy: 0.6094\n",
      "Epoch 228/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6479\n",
      "Epoch 228: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0096 - accuracy: 0.6479 - val_loss: 0.0260 - val_accuracy: 0.5938\n",
      "Epoch 229/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.6546\n",
      "Epoch 229: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0097 - accuracy: 0.6546 - val_loss: 0.0271 - val_accuracy: 0.5625\n",
      "Epoch 230/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.6478\n",
      "Epoch 230: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0097 - accuracy: 0.6478 - val_loss: 0.0260 - val_accuracy: 0.6250\n",
      "Epoch 231/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.6556\n",
      "Epoch 231: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0095 - accuracy: 0.6556 - val_loss: 0.0262 - val_accuracy: 0.6016\n",
      "Epoch 232/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6548\n",
      "Epoch 232: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0096 - accuracy: 0.6548 - val_loss: 0.0259 - val_accuracy: 0.5859\n",
      "Epoch 233/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.6521\n",
      "Epoch 233: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0095 - accuracy: 0.6521 - val_loss: 0.0260 - val_accuracy: 0.6016\n",
      "Epoch 234/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6535\n",
      "Epoch 234: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0094 - accuracy: 0.6535 - val_loss: 0.0257 - val_accuracy: 0.5938\n",
      "Epoch 235/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.6562\n",
      "Epoch 235: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0095 - accuracy: 0.6562 - val_loss: 0.0259 - val_accuracy: 0.5859\n",
      "Epoch 236/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.6490\n",
      "Epoch 236: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0095 - accuracy: 0.6490 - val_loss: 0.0259 - val_accuracy: 0.6094\n",
      "Epoch 237/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6581\n",
      "Epoch 237: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0096 - accuracy: 0.6581 - val_loss: 0.0259 - val_accuracy: 0.5938\n",
      "Epoch 238/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6582\n",
      "Epoch 238: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0094 - accuracy: 0.6582 - val_loss: 0.0258 - val_accuracy: 0.6094\n",
      "Epoch 239/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.6551\n",
      "Epoch 239: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0095 - accuracy: 0.6551 - val_loss: 0.0261 - val_accuracy: 0.6328\n",
      "Epoch 240/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6562\n",
      "Epoch 240: val_loss did not improve from 0.02553\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0092 - accuracy: 0.6562 - val_loss: 0.0258 - val_accuracy: 0.6094\n",
      "Epoch 241/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6622\n",
      "Epoch 241: val_loss improved from 0.02553 to 0.02536, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 11s 132ms/step - loss: 0.0096 - accuracy: 0.6622 - val_loss: 0.0254 - val_accuracy: 0.6094\n",
      "Epoch 242/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.6559\n",
      "Epoch 242: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0095 - accuracy: 0.6559 - val_loss: 0.0259 - val_accuracy: 0.5938\n",
      "Epoch 243/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6546\n",
      "Epoch 243: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0093 - accuracy: 0.6546 - val_loss: 0.0263 - val_accuracy: 0.5859\n",
      "Epoch 244/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6548\n",
      "Epoch 244: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0094 - accuracy: 0.6548 - val_loss: 0.0261 - val_accuracy: 0.6172\n",
      "Epoch 245/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6577\n",
      "Epoch 245: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0093 - accuracy: 0.6577 - val_loss: 0.0260 - val_accuracy: 0.6016\n",
      "Epoch 246/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.6601\n",
      "Epoch 246: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0096 - accuracy: 0.6601 - val_loss: 0.0256 - val_accuracy: 0.6172\n",
      "Epoch 247/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6690\n",
      "Epoch 247: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0093 - accuracy: 0.6690 - val_loss: 0.0258 - val_accuracy: 0.6016\n",
      "Epoch 248/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6576\n",
      "Epoch 248: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0093 - accuracy: 0.6576 - val_loss: 0.0262 - val_accuracy: 0.5938\n",
      "Epoch 249/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6528\n",
      "Epoch 249: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0094 - accuracy: 0.6528 - val_loss: 0.0260 - val_accuracy: 0.6094\n",
      "Epoch 250/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.6550\n",
      "Epoch 250: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0097 - accuracy: 0.6550 - val_loss: 0.0262 - val_accuracy: 0.5938\n",
      "Epoch 251/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6634\n",
      "Epoch 251: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0092 - accuracy: 0.6634 - val_loss: 0.0262 - val_accuracy: 0.5859\n",
      "Epoch 252/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6603\n",
      "Epoch 252: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0094 - accuracy: 0.6603 - val_loss: 0.0257 - val_accuracy: 0.6094\n",
      "Epoch 253/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6652\n",
      "Epoch 253: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0092 - accuracy: 0.6652 - val_loss: 0.0257 - val_accuracy: 0.6172\n",
      "Epoch 254/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6674\n",
      "Epoch 254: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0093 - accuracy: 0.6674 - val_loss: 0.0260 - val_accuracy: 0.6016\n",
      "Epoch 255/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6605\n",
      "Epoch 255: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0092 - accuracy: 0.6605 - val_loss: 0.0262 - val_accuracy: 0.6016\n",
      "Epoch 256/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6606\n",
      "Epoch 256: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0094 - accuracy: 0.6606 - val_loss: 0.0261 - val_accuracy: 0.5859\n",
      "Epoch 257/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6609\n",
      "Epoch 257: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0093 - accuracy: 0.6609 - val_loss: 0.0259 - val_accuracy: 0.6172\n",
      "Epoch 258/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6696\n",
      "Epoch 258: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0093 - accuracy: 0.6696 - val_loss: 0.0255 - val_accuracy: 0.5938\n",
      "Epoch 259/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6636\n",
      "Epoch 259: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0093 - accuracy: 0.6636 - val_loss: 0.0256 - val_accuracy: 0.6328\n",
      "Epoch 260/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6581\n",
      "Epoch 260: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0094 - accuracy: 0.6581 - val_loss: 0.0259 - val_accuracy: 0.5859\n",
      "Epoch 261/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6550\n",
      "Epoch 261: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0092 - accuracy: 0.6550 - val_loss: 0.0266 - val_accuracy: 0.5781\n",
      "Epoch 262/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6656\n",
      "Epoch 262: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0093 - accuracy: 0.6656 - val_loss: 0.0259 - val_accuracy: 0.5938\n",
      "Epoch 263/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6645\n",
      "Epoch 263: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0092 - accuracy: 0.6645 - val_loss: 0.0262 - val_accuracy: 0.6016\n",
      "Epoch 264/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6603\n",
      "Epoch 264: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0094 - accuracy: 0.6603 - val_loss: 0.0259 - val_accuracy: 0.6172\n",
      "Epoch 265/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6671\n",
      "Epoch 265: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0094 - accuracy: 0.6671 - val_loss: 0.0262 - val_accuracy: 0.6094\n",
      "Epoch 266/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6670\n",
      "Epoch 266: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0093 - accuracy: 0.6670 - val_loss: 0.0259 - val_accuracy: 0.6094\n",
      "Epoch 267/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6632\n",
      "Epoch 267: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0092 - accuracy: 0.6632 - val_loss: 0.0258 - val_accuracy: 0.5859\n",
      "Epoch 268/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6661\n",
      "Epoch 268: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0091 - accuracy: 0.6661 - val_loss: 0.0256 - val_accuracy: 0.5938\n",
      "Epoch 269/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6628\n",
      "Epoch 269: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0092 - accuracy: 0.6628 - val_loss: 0.0257 - val_accuracy: 0.5859\n",
      "Epoch 270/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6672\n",
      "Epoch 270: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0091 - accuracy: 0.6672 - val_loss: 0.0260 - val_accuracy: 0.6172\n",
      "Epoch 271/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6671\n",
      "Epoch 271: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0094 - accuracy: 0.6671 - val_loss: 0.0259 - val_accuracy: 0.5859\n",
      "Epoch 272/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6604\n",
      "Epoch 272: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0092 - accuracy: 0.6604 - val_loss: 0.0257 - val_accuracy: 0.5938\n",
      "Epoch 273/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6586\n",
      "Epoch 273: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0091 - accuracy: 0.6586 - val_loss: 0.0262 - val_accuracy: 0.6016\n",
      "Epoch 274/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6729\n",
      "Epoch 274: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0093 - accuracy: 0.6729 - val_loss: 0.0260 - val_accuracy: 0.6094\n",
      "Epoch 275/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6615\n",
      "Epoch 275: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0092 - accuracy: 0.6615 - val_loss: 0.0257 - val_accuracy: 0.5938\n",
      "Epoch 276/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6660\n",
      "Epoch 276: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0090 - accuracy: 0.6660 - val_loss: 0.0256 - val_accuracy: 0.5938\n",
      "Epoch 277/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6686\n",
      "Epoch 277: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0091 - accuracy: 0.6686 - val_loss: 0.0262 - val_accuracy: 0.6016\n",
      "Epoch 278/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6648\n",
      "Epoch 278: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 92ms/step - loss: 0.0091 - accuracy: 0.6648 - val_loss: 0.0260 - val_accuracy: 0.5781\n",
      "Epoch 279/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6703\n",
      "Epoch 279: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0092 - accuracy: 0.6703 - val_loss: 0.0262 - val_accuracy: 0.5781\n",
      "Epoch 280/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.6619\n",
      "Epoch 280: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0094 - accuracy: 0.6619 - val_loss: 0.0267 - val_accuracy: 0.5781\n",
      "Epoch 281/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6621\n",
      "Epoch 281: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0090 - accuracy: 0.6621 - val_loss: 0.0254 - val_accuracy: 0.5938\n",
      "Epoch 282/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6660\n",
      "Epoch 282: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 8s 92ms/step - loss: 0.0091 - accuracy: 0.6660 - val_loss: 0.0257 - val_accuracy: 0.6016\n",
      "Epoch 283/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6659\n",
      "Epoch 283: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0092 - accuracy: 0.6659 - val_loss: 0.0261 - val_accuracy: 0.5859\n",
      "Epoch 284/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6710\n",
      "Epoch 284: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0092 - accuracy: 0.6710 - val_loss: 0.0258 - val_accuracy: 0.6094\n",
      "Epoch 285/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6654\n",
      "Epoch 285: val_loss did not improve from 0.02536\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0092 - accuracy: 0.6654 - val_loss: 0.0256 - val_accuracy: 0.5938\n",
      "Epoch 286/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6635\n",
      "Epoch 286: val_loss improved from 0.02536 to 0.02522, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 130ms/step - loss: 0.0091 - accuracy: 0.6635 - val_loss: 0.0252 - val_accuracy: 0.6016\n",
      "Epoch 287/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.6660\n",
      "Epoch 287: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0092 - accuracy: 0.6660 - val_loss: 0.0258 - val_accuracy: 0.6016\n",
      "Epoch 288/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.6702\n",
      "Epoch 288: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0093 - accuracy: 0.6702 - val_loss: 0.0257 - val_accuracy: 0.6016\n",
      "Epoch 289/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6687\n",
      "Epoch 289: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0091 - accuracy: 0.6687 - val_loss: 0.0253 - val_accuracy: 0.6172\n",
      "Epoch 290/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6740\n",
      "Epoch 290: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0091 - accuracy: 0.6740 - val_loss: 0.0256 - val_accuracy: 0.5938\n",
      "Epoch 291/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6667\n",
      "Epoch 291: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0090 - accuracy: 0.6667 - val_loss: 0.0255 - val_accuracy: 0.5938\n",
      "Epoch 292/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6757\n",
      "Epoch 292: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0091 - accuracy: 0.6757 - val_loss: 0.0259 - val_accuracy: 0.6172\n",
      "Epoch 293/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6725\n",
      "Epoch 293: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0091 - accuracy: 0.6725 - val_loss: 0.0259 - val_accuracy: 0.6172\n",
      "Epoch 294/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6635\n",
      "Epoch 294: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0091 - accuracy: 0.6635 - val_loss: 0.0254 - val_accuracy: 0.5859\n",
      "Epoch 295/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6753\n",
      "Epoch 295: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0090 - accuracy: 0.6753 - val_loss: 0.0255 - val_accuracy: 0.5859\n",
      "Epoch 296/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6702\n",
      "Epoch 296: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0090 - accuracy: 0.6702 - val_loss: 0.0260 - val_accuracy: 0.6250\n",
      "Epoch 297/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6683\n",
      "Epoch 297: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 0.0090 - accuracy: 0.6683 - val_loss: 0.0256 - val_accuracy: 0.5703\n",
      "Epoch 298/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6730\n",
      "Epoch 298: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0089 - accuracy: 0.6730 - val_loss: 0.0254 - val_accuracy: 0.6094\n",
      "Epoch 299/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6710\n",
      "Epoch 299: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0091 - accuracy: 0.6710 - val_loss: 0.0253 - val_accuracy: 0.6016\n",
      "Epoch 300/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6673\n",
      "Epoch 300: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0091 - accuracy: 0.6673 - val_loss: 0.0253 - val_accuracy: 0.5859\n",
      "Epoch 301/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6762\n",
      "Epoch 301: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 0.0090 - accuracy: 0.6762 - val_loss: 0.0256 - val_accuracy: 0.6094\n",
      "Epoch 302/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6645\n",
      "Epoch 302: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0091 - accuracy: 0.6645 - val_loss: 0.0256 - val_accuracy: 0.6016\n",
      "Epoch 303/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6807\n",
      "Epoch 303: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0089 - accuracy: 0.6807 - val_loss: 0.0253 - val_accuracy: 0.6094\n",
      "Epoch 304/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6612\n",
      "Epoch 304: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0090 - accuracy: 0.6612 - val_loss: 0.0253 - val_accuracy: 0.6016\n",
      "Epoch 305/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6746\n",
      "Epoch 305: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0091 - accuracy: 0.6746 - val_loss: 0.0254 - val_accuracy: 0.5938\n",
      "Epoch 306/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6711\n",
      "Epoch 306: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0089 - accuracy: 0.6711 - val_loss: 0.0256 - val_accuracy: 0.5938\n",
      "Epoch 307/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6739\n",
      "Epoch 307: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0089 - accuracy: 0.6739 - val_loss: 0.0257 - val_accuracy: 0.5938\n",
      "Epoch 308/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6712\n",
      "Epoch 308: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0091 - accuracy: 0.6712 - val_loss: 0.0253 - val_accuracy: 0.6094\n",
      "Epoch 309/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6708\n",
      "Epoch 309: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 0.0090 - accuracy: 0.6708 - val_loss: 0.0258 - val_accuracy: 0.5703\n",
      "Epoch 310/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6771\n",
      "Epoch 310: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0090 - accuracy: 0.6771 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 311/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6650\n",
      "Epoch 311: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0089 - accuracy: 0.6650 - val_loss: 0.0262 - val_accuracy: 0.6094\n",
      "Epoch 312/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6722\n",
      "Epoch 312: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0090 - accuracy: 0.6722 - val_loss: 0.0255 - val_accuracy: 0.5859\n",
      "Epoch 313/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.6719\n",
      "Epoch 313: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0091 - accuracy: 0.6719 - val_loss: 0.0258 - val_accuracy: 0.5781\n",
      "Epoch 314/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6789\n",
      "Epoch 314: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0089 - accuracy: 0.6789 - val_loss: 0.0259 - val_accuracy: 0.6094\n",
      "Epoch 315/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6741\n",
      "Epoch 315: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0090 - accuracy: 0.6741 - val_loss: 0.0254 - val_accuracy: 0.6094\n",
      "Epoch 316/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6652\n",
      "Epoch 316: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0089 - accuracy: 0.6652 - val_loss: 0.0255 - val_accuracy: 0.6094\n",
      "Epoch 317/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6782\n",
      "Epoch 317: val_loss did not improve from 0.02522\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0089 - accuracy: 0.6782 - val_loss: 0.0258 - val_accuracy: 0.5938\n",
      "Epoch 318/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6790\n",
      "Epoch 318: val_loss improved from 0.02522 to 0.02498, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 0.0090 - accuracy: 0.6790 - val_loss: 0.0250 - val_accuracy: 0.6172\n",
      "Epoch 319/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6727\n",
      "Epoch 319: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0090 - accuracy: 0.6727 - val_loss: 0.0252 - val_accuracy: 0.6250\n",
      "Epoch 320/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6754\n",
      "Epoch 320: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0089 - accuracy: 0.6754 - val_loss: 0.0252 - val_accuracy: 0.6016\n",
      "Epoch 321/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6729\n",
      "Epoch 321: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0090 - accuracy: 0.6729 - val_loss: 0.0257 - val_accuracy: 0.6172\n",
      "Epoch 322/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6849\n",
      "Epoch 322: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0089 - accuracy: 0.6849 - val_loss: 0.0257 - val_accuracy: 0.5938\n",
      "Epoch 323/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6740\n",
      "Epoch 323: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0089 - accuracy: 0.6740 - val_loss: 0.0255 - val_accuracy: 0.6250\n",
      "Epoch 324/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6759\n",
      "Epoch 324: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0089 - accuracy: 0.6759 - val_loss: 0.0253 - val_accuracy: 0.6016\n",
      "Epoch 325/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6811\n",
      "Epoch 325: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0089 - accuracy: 0.6811 - val_loss: 0.0253 - val_accuracy: 0.6016\n",
      "Epoch 326/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6782\n",
      "Epoch 326: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0089 - accuracy: 0.6782 - val_loss: 0.0254 - val_accuracy: 0.6016\n",
      "Epoch 327/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6736\n",
      "Epoch 327: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0090 - accuracy: 0.6736 - val_loss: 0.0257 - val_accuracy: 0.5781\n",
      "Epoch 328/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6746\n",
      "Epoch 328: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0088 - accuracy: 0.6746 - val_loss: 0.0255 - val_accuracy: 0.6094\n",
      "Epoch 329/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6769\n",
      "Epoch 329: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0090 - accuracy: 0.6769 - val_loss: 0.0254 - val_accuracy: 0.6406\n",
      "Epoch 330/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6800\n",
      "Epoch 330: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0088 - accuracy: 0.6800 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 331/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6787\n",
      "Epoch 331: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0090 - accuracy: 0.6787 - val_loss: 0.0257 - val_accuracy: 0.6094\n",
      "Epoch 332/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6760\n",
      "Epoch 332: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0089 - accuracy: 0.6760 - val_loss: 0.0256 - val_accuracy: 0.6094\n",
      "Epoch 333/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6785\n",
      "Epoch 333: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0087 - accuracy: 0.6785 - val_loss: 0.0252 - val_accuracy: 0.6016\n",
      "Epoch 334/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6771\n",
      "Epoch 334: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0089 - accuracy: 0.6771 - val_loss: 0.0251 - val_accuracy: 0.6172\n",
      "Epoch 335/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6787\n",
      "Epoch 335: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0089 - accuracy: 0.6787 - val_loss: 0.0252 - val_accuracy: 0.6094\n",
      "Epoch 336/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6715\n",
      "Epoch 336: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0089 - accuracy: 0.6715 - val_loss: 0.0253 - val_accuracy: 0.6250\n",
      "Epoch 337/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6758\n",
      "Epoch 337: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0089 - accuracy: 0.6758 - val_loss: 0.0251 - val_accuracy: 0.6250\n",
      "Epoch 338/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6790\n",
      "Epoch 338: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0088 - accuracy: 0.6790 - val_loss: 0.0252 - val_accuracy: 0.6172\n",
      "Epoch 339/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6851\n",
      "Epoch 339: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0089 - accuracy: 0.6851 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 340/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6826\n",
      "Epoch 340: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0088 - accuracy: 0.6826 - val_loss: 0.0254 - val_accuracy: 0.6016\n",
      "Epoch 341/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.6794\n",
      "Epoch 341: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0090 - accuracy: 0.6794 - val_loss: 0.0257 - val_accuracy: 0.6094\n",
      "Epoch 342/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6812\n",
      "Epoch 342: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0088 - accuracy: 0.6812 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 343/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6779\n",
      "Epoch 343: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 93ms/step - loss: 0.0087 - accuracy: 0.6779 - val_loss: 0.0253 - val_accuracy: 0.6016\n",
      "Epoch 344/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6710\n",
      "Epoch 344: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0089 - accuracy: 0.6710 - val_loss: 0.0255 - val_accuracy: 0.5938\n",
      "Epoch 345/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6792\n",
      "Epoch 345: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0088 - accuracy: 0.6792 - val_loss: 0.0251 - val_accuracy: 0.6094\n",
      "Epoch 346/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6868\n",
      "Epoch 346: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0088 - accuracy: 0.6868 - val_loss: 0.0250 - val_accuracy: 0.6094\n",
      "Epoch 347/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6816\n",
      "Epoch 347: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0089 - accuracy: 0.6816 - val_loss: 0.0250 - val_accuracy: 0.6094\n",
      "Epoch 348/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6807\n",
      "Epoch 348: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0089 - accuracy: 0.6807 - val_loss: 0.0257 - val_accuracy: 0.5859\n",
      "Epoch 349/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6813\n",
      "Epoch 349: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0088 - accuracy: 0.6813 - val_loss: 0.0258 - val_accuracy: 0.6016\n",
      "Epoch 350/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6787\n",
      "Epoch 350: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0088 - accuracy: 0.6787 - val_loss: 0.0253 - val_accuracy: 0.6094\n",
      "Epoch 351/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6805\n",
      "Epoch 351: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0088 - accuracy: 0.6805 - val_loss: 0.0255 - val_accuracy: 0.6094\n",
      "Epoch 352/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6759\n",
      "Epoch 352: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0088 - accuracy: 0.6759 - val_loss: 0.0253 - val_accuracy: 0.5938\n",
      "Epoch 353/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6822\n",
      "Epoch 353: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0087 - accuracy: 0.6822 - val_loss: 0.0261 - val_accuracy: 0.6094\n",
      "Epoch 354/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6731\n",
      "Epoch 354: val_loss did not improve from 0.02498\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0089 - accuracy: 0.6731 - val_loss: 0.0251 - val_accuracy: 0.6016\n",
      "Epoch 355/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6875\n",
      "Epoch 355: val_loss improved from 0.02498 to 0.02494, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 0.0088 - accuracy: 0.6875 - val_loss: 0.0249 - val_accuracy: 0.6406\n",
      "Epoch 356/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6799\n",
      "Epoch 356: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 8s 93ms/step - loss: 0.0087 - accuracy: 0.6799 - val_loss: 0.0254 - val_accuracy: 0.6094\n",
      "Epoch 357/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6779\n",
      "Epoch 357: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0088 - accuracy: 0.6779 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 358/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.6819\n",
      "Epoch 358: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0089 - accuracy: 0.6819 - val_loss: 0.0254 - val_accuracy: 0.5938\n",
      "Epoch 359/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6855\n",
      "Epoch 359: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0087 - accuracy: 0.6855 - val_loss: 0.0252 - val_accuracy: 0.6250\n",
      "Epoch 360/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6810\n",
      "Epoch 360: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0087 - accuracy: 0.6810 - val_loss: 0.0252 - val_accuracy: 0.6172\n",
      "Epoch 361/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6880\n",
      "Epoch 361: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0088 - accuracy: 0.6880 - val_loss: 0.0252 - val_accuracy: 0.6406\n",
      "Epoch 362/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6778\n",
      "Epoch 362: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0088 - accuracy: 0.6778 - val_loss: 0.0252 - val_accuracy: 0.5859\n",
      "Epoch 363/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6856\n",
      "Epoch 363: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0088 - accuracy: 0.6856 - val_loss: 0.0252 - val_accuracy: 0.6016\n",
      "Epoch 364/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6774\n",
      "Epoch 364: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0088 - accuracy: 0.6774 - val_loss: 0.0257 - val_accuracy: 0.5859\n",
      "Epoch 365/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6839\n",
      "Epoch 365: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0088 - accuracy: 0.6839 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 366/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6873\n",
      "Epoch 366: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0087 - accuracy: 0.6873 - val_loss: 0.0251 - val_accuracy: 0.6094\n",
      "Epoch 367/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6825\n",
      "Epoch 367: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0088 - accuracy: 0.6825 - val_loss: 0.0257 - val_accuracy: 0.6016\n",
      "Epoch 368/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6856\n",
      "Epoch 368: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0087 - accuracy: 0.6856 - val_loss: 0.0254 - val_accuracy: 0.5781\n",
      "Epoch 369/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6816\n",
      "Epoch 369: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0087 - accuracy: 0.6816 - val_loss: 0.0254 - val_accuracy: 0.6016\n",
      "Epoch 370/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6857\n",
      "Epoch 370: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0087 - accuracy: 0.6857 - val_loss: 0.0252 - val_accuracy: 0.6172\n",
      "Epoch 371/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6793\n",
      "Epoch 371: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0087 - accuracy: 0.6793 - val_loss: 0.0254 - val_accuracy: 0.6328\n",
      "Epoch 372/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6799\n",
      "Epoch 372: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0088 - accuracy: 0.6799 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 373/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6819\n",
      "Epoch 373: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0087 - accuracy: 0.6819 - val_loss: 0.0251 - val_accuracy: 0.6016\n",
      "Epoch 374/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6877\n",
      "Epoch 374: val_loss did not improve from 0.02494\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0088 - accuracy: 0.6877 - val_loss: 0.0251 - val_accuracy: 0.6094\n",
      "Epoch 375/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6876\n",
      "Epoch 375: val_loss improved from 0.02494 to 0.02488, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\\1\n",
      "80/80 [==============================] - 11s 134ms/step - loss: 0.0086 - accuracy: 0.6876 - val_loss: 0.0249 - val_accuracy: 0.5938\n",
      "Epoch 376/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6777\n",
      "Epoch 376: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 91ms/step - loss: 0.0088 - accuracy: 0.6777 - val_loss: 0.0249 - val_accuracy: 0.6094\n",
      "Epoch 377/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6763\n",
      "Epoch 377: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0088 - accuracy: 0.6763 - val_loss: 0.0254 - val_accuracy: 0.5859\n",
      "Epoch 378/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6979\n",
      "Epoch 378: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0087 - accuracy: 0.6979 - val_loss: 0.0252 - val_accuracy: 0.6328\n",
      "Epoch 379/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6890\n",
      "Epoch 379: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0087 - accuracy: 0.6890 - val_loss: 0.0258 - val_accuracy: 0.6094\n",
      "Epoch 380/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6787\n",
      "Epoch 380: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 93ms/step - loss: 0.0088 - accuracy: 0.6787 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 381/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6863\n",
      "Epoch 381: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0087 - accuracy: 0.6863 - val_loss: 0.0253 - val_accuracy: 0.6016\n",
      "Epoch 382/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6802\n",
      "Epoch 382: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0088 - accuracy: 0.6802 - val_loss: 0.0254 - val_accuracy: 0.6094\n",
      "Epoch 383/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6789\n",
      "Epoch 383: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0087 - accuracy: 0.6789 - val_loss: 0.0254 - val_accuracy: 0.6094\n",
      "Epoch 384/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6888\n",
      "Epoch 384: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0087 - accuracy: 0.6888 - val_loss: 0.0257 - val_accuracy: 0.6250\n",
      "Epoch 385/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6874\n",
      "Epoch 385: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0086 - accuracy: 0.6874 - val_loss: 0.0252 - val_accuracy: 0.5938\n",
      "Epoch 386/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6838\n",
      "Epoch 386: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0087 - accuracy: 0.6838 - val_loss: 0.0251 - val_accuracy: 0.5938\n",
      "Epoch 387/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6891\n",
      "Epoch 387: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0087 - accuracy: 0.6891 - val_loss: 0.0258 - val_accuracy: 0.6094\n",
      "Epoch 388/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6827\n",
      "Epoch 388: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0088 - accuracy: 0.6827 - val_loss: 0.0255 - val_accuracy: 0.5938\n",
      "Epoch 389/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6896\n",
      "Epoch 389: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 93ms/step - loss: 0.0086 - accuracy: 0.6896 - val_loss: 0.0256 - val_accuracy: 0.6094\n",
      "Epoch 390/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6875\n",
      "Epoch 390: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0087 - accuracy: 0.6875 - val_loss: 0.0253 - val_accuracy: 0.6250\n",
      "Epoch 391/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6889\n",
      "Epoch 391: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0087 - accuracy: 0.6889 - val_loss: 0.0252 - val_accuracy: 0.6172\n",
      "Epoch 392/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6864\n",
      "Epoch 392: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0086 - accuracy: 0.6864 - val_loss: 0.0253 - val_accuracy: 0.6250\n",
      "Epoch 393/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6803\n",
      "Epoch 393: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 0.0087 - accuracy: 0.6803 - val_loss: 0.0254 - val_accuracy: 0.5938\n",
      "Epoch 394/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6886\n",
      "Epoch 394: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0088 - accuracy: 0.6886 - val_loss: 0.0253 - val_accuracy: 0.6094\n",
      "Epoch 395/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6862\n",
      "Epoch 395: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0086 - accuracy: 0.6862 - val_loss: 0.0254 - val_accuracy: 0.6094\n",
      "Epoch 396/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6851\n",
      "Epoch 396: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0086 - accuracy: 0.6851 - val_loss: 0.0252 - val_accuracy: 0.6172\n",
      "Epoch 397/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6831\n",
      "Epoch 397: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0087 - accuracy: 0.6831 - val_loss: 0.0251 - val_accuracy: 0.6016\n",
      "Epoch 398/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6808\n",
      "Epoch 398: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0084 - accuracy: 0.6808 - val_loss: 0.0253 - val_accuracy: 0.6250\n",
      "Epoch 399/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.6960\n",
      "Epoch 399: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0088 - accuracy: 0.6960 - val_loss: 0.0253 - val_accuracy: 0.6094\n",
      "Epoch 400/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6830\n",
      "Epoch 400: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 93ms/step - loss: 0.0087 - accuracy: 0.6830 - val_loss: 0.0252 - val_accuracy: 0.6094\n",
      "Epoch 401/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6896\n",
      "Epoch 401: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0086 - accuracy: 0.6896 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 402/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6900\n",
      "Epoch 402: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 0.0086 - accuracy: 0.6900 - val_loss: 0.0252 - val_accuracy: 0.5938\n",
      "Epoch 403/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6863\n",
      "Epoch 403: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0086 - accuracy: 0.6863 - val_loss: 0.0257 - val_accuracy: 0.6016\n",
      "Epoch 404/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6872\n",
      "Epoch 404: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0087 - accuracy: 0.6872 - val_loss: 0.0251 - val_accuracy: 0.6094\n",
      "Epoch 405/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6861\n",
      "Epoch 405: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0085 - accuracy: 0.6861 - val_loss: 0.0253 - val_accuracy: 0.6094\n",
      "Epoch 406/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6922\n",
      "Epoch 406: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0087 - accuracy: 0.6922 - val_loss: 0.0250 - val_accuracy: 0.6172\n",
      "Epoch 407/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6841\n",
      "Epoch 407: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0086 - accuracy: 0.6841 - val_loss: 0.0256 - val_accuracy: 0.5859\n",
      "Epoch 408/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6898\n",
      "Epoch 408: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0086 - accuracy: 0.6898 - val_loss: 0.0254 - val_accuracy: 0.5938\n",
      "Epoch 409/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6814\n",
      "Epoch 409: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0087 - accuracy: 0.6814 - val_loss: 0.0252 - val_accuracy: 0.5938\n",
      "Epoch 410/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6824\n",
      "Epoch 410: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0086 - accuracy: 0.6824 - val_loss: 0.0253 - val_accuracy: 0.6328\n",
      "Epoch 411/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6873\n",
      "Epoch 411: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0086 - accuracy: 0.6873 - val_loss: 0.0251 - val_accuracy: 0.6094\n",
      "Epoch 412/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6896\n",
      "Epoch 412: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0086 - accuracy: 0.6896 - val_loss: 0.0251 - val_accuracy: 0.5781\n",
      "Epoch 413/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6905\n",
      "Epoch 413: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0086 - accuracy: 0.6905 - val_loss: 0.0256 - val_accuracy: 0.6172\n",
      "Epoch 414/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6972\n",
      "Epoch 414: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0084 - accuracy: 0.6972 - val_loss: 0.0259 - val_accuracy: 0.5781\n",
      "Epoch 415/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6880\n",
      "Epoch 415: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0087 - accuracy: 0.6880 - val_loss: 0.0253 - val_accuracy: 0.6172\n",
      "Epoch 416/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6844\n",
      "Epoch 416: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0087 - accuracy: 0.6844 - val_loss: 0.0250 - val_accuracy: 0.6094\n",
      "Epoch 417/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6865\n",
      "Epoch 417: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 9s 109ms/step - loss: 0.0086 - accuracy: 0.6865 - val_loss: 0.0252 - val_accuracy: 0.6016\n",
      "Epoch 418/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6901\n",
      "Epoch 418: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 0.0085 - accuracy: 0.6901 - val_loss: 0.0253 - val_accuracy: 0.6172\n",
      "Epoch 419/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6919\n",
      "Epoch 419: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 105ms/step - loss: 0.0087 - accuracy: 0.6919 - val_loss: 0.0261 - val_accuracy: 0.5859\n",
      "Epoch 420/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6929\n",
      "Epoch 420: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0085 - accuracy: 0.6929 - val_loss: 0.0253 - val_accuracy: 0.6094\n",
      "Epoch 421/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6868\n",
      "Epoch 421: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0086 - accuracy: 0.6868 - val_loss: 0.0256 - val_accuracy: 0.5938\n",
      "Epoch 422/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6858\n",
      "Epoch 422: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 103ms/step - loss: 0.0085 - accuracy: 0.6858 - val_loss: 0.0255 - val_accuracy: 0.6172\n",
      "Epoch 423/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6937\n",
      "Epoch 423: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0087 - accuracy: 0.6937 - val_loss: 0.0254 - val_accuracy: 0.5781\n",
      "Epoch 424/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6917\n",
      "Epoch 424: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0085 - accuracy: 0.6917 - val_loss: 0.0254 - val_accuracy: 0.6250\n",
      "Epoch 425/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6901\n",
      "Epoch 425: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0085 - accuracy: 0.6901 - val_loss: 0.0252 - val_accuracy: 0.6172\n",
      "Epoch 426/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.7012\n",
      "Epoch 426: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 104ms/step - loss: 0.0084 - accuracy: 0.7012 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 427/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6825\n",
      "Epoch 427: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0085 - accuracy: 0.6825 - val_loss: 0.0253 - val_accuracy: 0.5859\n",
      "Epoch 428/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6916\n",
      "Epoch 428: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0086 - accuracy: 0.6916 - val_loss: 0.0251 - val_accuracy: 0.5938\n",
      "Epoch 429/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6889\n",
      "Epoch 429: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0087 - accuracy: 0.6889 - val_loss: 0.0252 - val_accuracy: 0.6250\n",
      "Epoch 430/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6939\n",
      "Epoch 430: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0086 - accuracy: 0.6939 - val_loss: 0.0251 - val_accuracy: 0.6016\n",
      "Epoch 431/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6866\n",
      "Epoch 431: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0087 - accuracy: 0.6866 - val_loss: 0.0253 - val_accuracy: 0.6094\n",
      "Epoch 432/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6914\n",
      "Epoch 432: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0083 - accuracy: 0.6914 - val_loss: 0.0257 - val_accuracy: 0.6172\n",
      "Epoch 433/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6904\n",
      "Epoch 433: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 104ms/step - loss: 0.0086 - accuracy: 0.6904 - val_loss: 0.0255 - val_accuracy: 0.5859\n",
      "Epoch 434/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6937\n",
      "Epoch 434: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0085 - accuracy: 0.6937 - val_loss: 0.0254 - val_accuracy: 0.5859\n",
      "Epoch 435/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6879\n",
      "Epoch 435: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0084 - accuracy: 0.6879 - val_loss: 0.0256 - val_accuracy: 0.5781\n",
      "Epoch 436/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6941\n",
      "Epoch 436: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0087 - accuracy: 0.6941 - val_loss: 0.0255 - val_accuracy: 0.5859\n",
      "Epoch 437/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6895\n",
      "Epoch 437: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 103ms/step - loss: 0.0084 - accuracy: 0.6895 - val_loss: 0.0257 - val_accuracy: 0.6016\n",
      "Epoch 438/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6969\n",
      "Epoch 438: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0086 - accuracy: 0.6969 - val_loss: 0.0257 - val_accuracy: 0.6328\n",
      "Epoch 439/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6899\n",
      "Epoch 439: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0086 - accuracy: 0.6899 - val_loss: 0.0258 - val_accuracy: 0.6016\n",
      "Epoch 440/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6917\n",
      "Epoch 440: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0084 - accuracy: 0.6917 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 441/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6998\n",
      "Epoch 441: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 103ms/step - loss: 0.0084 - accuracy: 0.6998 - val_loss: 0.0255 - val_accuracy: 0.5938\n",
      "Epoch 442/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6853\n",
      "Epoch 442: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0086 - accuracy: 0.6853 - val_loss: 0.0258 - val_accuracy: 0.5781\n",
      "Epoch 443/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6892\n",
      "Epoch 443: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0087 - accuracy: 0.6892 - val_loss: 0.0253 - val_accuracy: 0.6172\n",
      "Epoch 444/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6945\n",
      "Epoch 444: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0084 - accuracy: 0.6945 - val_loss: 0.0257 - val_accuracy: 0.5859\n",
      "Epoch 445/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6937\n",
      "Epoch 445: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 105ms/step - loss: 0.0084 - accuracy: 0.6937 - val_loss: 0.0256 - val_accuracy: 0.6172\n",
      "Epoch 446/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6891\n",
      "Epoch 446: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0085 - accuracy: 0.6891 - val_loss: 0.0257 - val_accuracy: 0.6172\n",
      "Epoch 447/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6971\n",
      "Epoch 447: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0083 - accuracy: 0.6971 - val_loss: 0.0253 - val_accuracy: 0.6016\n",
      "Epoch 448/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.6984\n",
      "Epoch 448: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0087 - accuracy: 0.6984 - val_loss: 0.0252 - val_accuracy: 0.6172\n",
      "Epoch 449/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6959\n",
      "Epoch 449: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 103ms/step - loss: 0.0086 - accuracy: 0.6959 - val_loss: 0.0256 - val_accuracy: 0.6250\n",
      "Epoch 450/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6886\n",
      "Epoch 450: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0083 - accuracy: 0.6886 - val_loss: 0.0254 - val_accuracy: 0.6406\n",
      "Epoch 451/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6943\n",
      "Epoch 451: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0086 - accuracy: 0.6943 - val_loss: 0.0253 - val_accuracy: 0.6016\n",
      "Epoch 452/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6959\n",
      "Epoch 452: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0084 - accuracy: 0.6959 - val_loss: 0.0254 - val_accuracy: 0.6094\n",
      "Epoch 453/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6888\n",
      "Epoch 453: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0085 - accuracy: 0.6888 - val_loss: 0.0254 - val_accuracy: 0.6172\n",
      "Epoch 454/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6942\n",
      "Epoch 454: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0085 - accuracy: 0.6942 - val_loss: 0.0259 - val_accuracy: 0.6250\n",
      "Epoch 455/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6937\n",
      "Epoch 455: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0085 - accuracy: 0.6937 - val_loss: 0.0255 - val_accuracy: 0.5938\n",
      "Epoch 456/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6875\n",
      "Epoch 456: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 103ms/step - loss: 0.0085 - accuracy: 0.6875 - val_loss: 0.0252 - val_accuracy: 0.6016\n",
      "Epoch 457/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6983\n",
      "Epoch 457: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0084 - accuracy: 0.6983 - val_loss: 0.0257 - val_accuracy: 0.6094\n",
      "Epoch 458/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6929\n",
      "Epoch 458: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0084 - accuracy: 0.6929 - val_loss: 0.0257 - val_accuracy: 0.5938\n",
      "Epoch 459/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6907\n",
      "Epoch 459: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0085 - accuracy: 0.6907 - val_loss: 0.0257 - val_accuracy: 0.6094\n",
      "Epoch 460/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6978\n",
      "Epoch 460: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 105ms/step - loss: 0.0084 - accuracy: 0.6978 - val_loss: 0.0258 - val_accuracy: 0.6094\n",
      "Epoch 461/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6832\n",
      "Epoch 461: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0085 - accuracy: 0.6832 - val_loss: 0.0253 - val_accuracy: 0.6328\n",
      "Epoch 462/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6979\n",
      "Epoch 462: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0085 - accuracy: 0.6979 - val_loss: 0.0256 - val_accuracy: 0.5859\n",
      "Epoch 463/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6918\n",
      "Epoch 463: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0085 - accuracy: 0.6918 - val_loss: 0.0258 - val_accuracy: 0.6172\n",
      "Epoch 464/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6927\n",
      "Epoch 464: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 9s 117ms/step - loss: 0.0085 - accuracy: 0.6927 - val_loss: 0.0257 - val_accuracy: 0.6016\n",
      "Epoch 465/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6822\n",
      "Epoch 465: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0084 - accuracy: 0.6822 - val_loss: 0.0258 - val_accuracy: 0.6250\n",
      "Epoch 466/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6943\n",
      "Epoch 466: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0084 - accuracy: 0.6943 - val_loss: 0.0253 - val_accuracy: 0.6172\n",
      "Epoch 467/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6991\n",
      "Epoch 467: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0084 - accuracy: 0.6991 - val_loss: 0.0257 - val_accuracy: 0.5938\n",
      "Epoch 468/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6942\n",
      "Epoch 468: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 0.0085 - accuracy: 0.6942 - val_loss: 0.0254 - val_accuracy: 0.6172\n",
      "Epoch 469/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6975\n",
      "Epoch 469: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0084 - accuracy: 0.6975 - val_loss: 0.0257 - val_accuracy: 0.6094\n",
      "Epoch 470/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6947\n",
      "Epoch 470: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0084 - accuracy: 0.6947 - val_loss: 0.0256 - val_accuracy: 0.6328\n",
      "Epoch 471/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6929\n",
      "Epoch 471: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 0.0085 - accuracy: 0.6929 - val_loss: 0.0258 - val_accuracy: 0.6016\n",
      "Epoch 472/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6954\n",
      "Epoch 472: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0083 - accuracy: 0.6954 - val_loss: 0.0258 - val_accuracy: 0.6016\n",
      "Epoch 473/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6906\n",
      "Epoch 473: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0083 - accuracy: 0.6906 - val_loss: 0.0257 - val_accuracy: 0.6250\n",
      "Epoch 474/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6985\n",
      "Epoch 474: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0085 - accuracy: 0.6985 - val_loss: 0.0256 - val_accuracy: 0.6094\n",
      "Epoch 475/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6953\n",
      "Epoch 475: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 103ms/step - loss: 0.0084 - accuracy: 0.6953 - val_loss: 0.0255 - val_accuracy: 0.6250\n",
      "Epoch 476/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6909\n",
      "Epoch 476: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0084 - accuracy: 0.6909 - val_loss: 0.0257 - val_accuracy: 0.6172\n",
      "Epoch 477/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.6975\n",
      "Epoch 477: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0086 - accuracy: 0.6975 - val_loss: 0.0254 - val_accuracy: 0.6094\n",
      "Epoch 478/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6914\n",
      "Epoch 478: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 0.0083 - accuracy: 0.6914 - val_loss: 0.0259 - val_accuracy: 0.6328\n",
      "Epoch 479/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6942\n",
      "Epoch 479: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 105ms/step - loss: 0.0082 - accuracy: 0.6942 - val_loss: 0.0256 - val_accuracy: 0.6016\n",
      "Epoch 480/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.7028\n",
      "Epoch 480: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0084 - accuracy: 0.7028 - val_loss: 0.0258 - val_accuracy: 0.5938\n",
      "Epoch 481/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6906\n",
      "Epoch 481: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0084 - accuracy: 0.6906 - val_loss: 0.0255 - val_accuracy: 0.6406\n",
      "Epoch 482/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6894\n",
      "Epoch 482: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0084 - accuracy: 0.6894 - val_loss: 0.0255 - val_accuracy: 0.6094\n",
      "Epoch 483/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6974\n",
      "Epoch 483: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0084 - accuracy: 0.6974 - val_loss: 0.0257 - val_accuracy: 0.6094\n",
      "Epoch 484/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6965\n",
      "Epoch 484: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 9s 107ms/step - loss: 0.0084 - accuracy: 0.6965 - val_loss: 0.0258 - val_accuracy: 0.5938\n",
      "Epoch 485/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6934\n",
      "Epoch 485: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0083 - accuracy: 0.6934 - val_loss: 0.0259 - val_accuracy: 0.6016\n",
      "Epoch 486/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6982\n",
      "Epoch 486: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0084 - accuracy: 0.6982 - val_loss: 0.0257 - val_accuracy: 0.5938\n",
      "Epoch 487/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6822\n",
      "Epoch 487: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0085 - accuracy: 0.6822 - val_loss: 0.0258 - val_accuracy: 0.6016\n",
      "Epoch 488/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6898\n",
      "Epoch 488: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0085 - accuracy: 0.6898 - val_loss: 0.0262 - val_accuracy: 0.6094\n",
      "Epoch 489/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6952\n",
      "Epoch 489: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0083 - accuracy: 0.6952 - val_loss: 0.0256 - val_accuracy: 0.6016\n",
      "Epoch 490/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6997\n",
      "Epoch 490: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0082 - accuracy: 0.6997 - val_loss: 0.0257 - val_accuracy: 0.6250\n",
      "Epoch 491/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.7019\n",
      "Epoch 491: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0084 - accuracy: 0.7019 - val_loss: 0.0256 - val_accuracy: 0.6016\n",
      "Epoch 492/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.7040\n",
      "Epoch 492: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0083 - accuracy: 0.7040 - val_loss: 0.0258 - val_accuracy: 0.6250\n",
      "Epoch 493/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6942\n",
      "Epoch 493: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0084 - accuracy: 0.6942 - val_loss: 0.0255 - val_accuracy: 0.6016\n",
      "Epoch 494/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6946\n",
      "Epoch 494: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0085 - accuracy: 0.6946 - val_loss: 0.0259 - val_accuracy: 0.5859\n",
      "Epoch 495/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6948\n",
      "Epoch 495: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0083 - accuracy: 0.6948 - val_loss: 0.0257 - val_accuracy: 0.6094\n",
      "Epoch 496/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.7028\n",
      "Epoch 496: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0082 - accuracy: 0.7028 - val_loss: 0.0259 - val_accuracy: 0.6172\n",
      "Epoch 497/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6871\n",
      "Epoch 497: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0084 - accuracy: 0.6871 - val_loss: 0.0257 - val_accuracy: 0.5859\n",
      "Epoch 498/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.7020\n",
      "Epoch 498: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0084 - accuracy: 0.7020 - val_loss: 0.0260 - val_accuracy: 0.6016\n",
      "Epoch 499/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6897\n",
      "Epoch 499: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0084 - accuracy: 0.6897 - val_loss: 0.0255 - val_accuracy: 0.6172\n",
      "Epoch 500/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6894\n",
      "Epoch 500: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0083 - accuracy: 0.6894 - val_loss: 0.0258 - val_accuracy: 0.5938\n",
      "Epoch 501/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.7084\n",
      "Epoch 501: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0082 - accuracy: 0.7084 - val_loss: 0.0258 - val_accuracy: 0.6016\n",
      "Epoch 502/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6879\n",
      "Epoch 502: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0084 - accuracy: 0.6879 - val_loss: 0.0258 - val_accuracy: 0.6328\n",
      "Epoch 503/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6960\n",
      "Epoch 503: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0082 - accuracy: 0.6960 - val_loss: 0.0255 - val_accuracy: 0.6094\n",
      "Epoch 504/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6988\n",
      "Epoch 504: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0082 - accuracy: 0.6988 - val_loss: 0.0256 - val_accuracy: 0.6328\n",
      "Epoch 505/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.7011\n",
      "Epoch 505: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0084 - accuracy: 0.7011 - val_loss: 0.0258 - val_accuracy: 0.6016\n",
      "Epoch 506/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6933\n",
      "Epoch 506: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0084 - accuracy: 0.6933 - val_loss: 0.0258 - val_accuracy: 0.6016\n",
      "Epoch 507/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.7036\n",
      "Epoch 507: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0081 - accuracy: 0.7036 - val_loss: 0.0252 - val_accuracy: 0.6016\n",
      "Epoch 508/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6952\n",
      "Epoch 508: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0083 - accuracy: 0.6952 - val_loss: 0.0255 - val_accuracy: 0.5938\n",
      "Epoch 509/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6912\n",
      "Epoch 509: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0082 - accuracy: 0.6912 - val_loss: 0.0257 - val_accuracy: 0.6016\n",
      "Epoch 510/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6968\n",
      "Epoch 510: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0085 - accuracy: 0.6968 - val_loss: 0.0258 - val_accuracy: 0.6094\n",
      "Epoch 511/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6978\n",
      "Epoch 511: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0082 - accuracy: 0.6978 - val_loss: 0.0257 - val_accuracy: 0.6016\n",
      "Epoch 512/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.6982\n",
      "Epoch 512: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0085 - accuracy: 0.6982 - val_loss: 0.0258 - val_accuracy: 0.5938\n",
      "Epoch 513/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6996\n",
      "Epoch 513: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0084 - accuracy: 0.6996 - val_loss: 0.0256 - val_accuracy: 0.5859\n",
      "Epoch 514/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6910\n",
      "Epoch 514: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0081 - accuracy: 0.6910 - val_loss: 0.0261 - val_accuracy: 0.6016\n",
      "Epoch 515/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.7013\n",
      "Epoch 515: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0084 - accuracy: 0.7013 - val_loss: 0.0257 - val_accuracy: 0.6094\n",
      "Epoch 516/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6892\n",
      "Epoch 516: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0083 - accuracy: 0.6892 - val_loss: 0.0256 - val_accuracy: 0.6094\n",
      "Epoch 517/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6963\n",
      "Epoch 517: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0082 - accuracy: 0.6963 - val_loss: 0.0259 - val_accuracy: 0.6094\n",
      "Epoch 518/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6984\n",
      "Epoch 518: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 0.0082 - accuracy: 0.6984 - val_loss: 0.0261 - val_accuracy: 0.6016\n",
      "Epoch 519/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.7075\n",
      "Epoch 519: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0081 - accuracy: 0.7075 - val_loss: 0.0260 - val_accuracy: 0.5938\n",
      "Epoch 520/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6925\n",
      "Epoch 520: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0084 - accuracy: 0.6925 - val_loss: 0.0258 - val_accuracy: 0.5938\n",
      "Epoch 521/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.7025\n",
      "Epoch 521: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0083 - accuracy: 0.7025 - val_loss: 0.0259 - val_accuracy: 0.6094\n",
      "Epoch 522/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6956\n",
      "Epoch 522: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0083 - accuracy: 0.6956 - val_loss: 0.0258 - val_accuracy: 0.5859\n",
      "Epoch 523/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6909\n",
      "Epoch 523: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0082 - accuracy: 0.6909 - val_loss: 0.0262 - val_accuracy: 0.5938\n",
      "Epoch 524/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6967\n",
      "Epoch 524: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0082 - accuracy: 0.6967 - val_loss: 0.0257 - val_accuracy: 0.6016\n",
      "Epoch 525/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.6973\n",
      "Epoch 525: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0084 - accuracy: 0.6973 - val_loss: 0.0259 - val_accuracy: 0.6094\n",
      "Epoch 526/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6953\n",
      "Epoch 526: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0082 - accuracy: 0.6953 - val_loss: 0.0263 - val_accuracy: 0.5938\n",
      "Epoch 527/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6933\n",
      "Epoch 527: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0083 - accuracy: 0.6933 - val_loss: 0.0261 - val_accuracy: 0.5938\n",
      "Epoch 528/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.7019\n",
      "Epoch 528: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0081 - accuracy: 0.7019 - val_loss: 0.0258 - val_accuracy: 0.6172\n",
      "Epoch 529/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6992\n",
      "Epoch 529: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0081 - accuracy: 0.6992 - val_loss: 0.0259 - val_accuracy: 0.6094\n",
      "Epoch 530/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.7037\n",
      "Epoch 530: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0082 - accuracy: 0.7037 - val_loss: 0.0259 - val_accuracy: 0.6094\n",
      "Epoch 531/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6990\n",
      "Epoch 531: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0082 - accuracy: 0.6990 - val_loss: 0.0259 - val_accuracy: 0.6016\n",
      "Epoch 532/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.7027\n",
      "Epoch 532: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0082 - accuracy: 0.7027 - val_loss: 0.0259 - val_accuracy: 0.6172\n",
      "Epoch 533/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.6993\n",
      "Epoch 533: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0083 - accuracy: 0.6993 - val_loss: 0.0256 - val_accuracy: 0.6094\n",
      "Epoch 534/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6849\n",
      "Epoch 534: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0081 - accuracy: 0.6849 - val_loss: 0.0258 - val_accuracy: 0.6172\n",
      "Epoch 535/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.7059\n",
      "Epoch 535: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0081 - accuracy: 0.7059 - val_loss: 0.0257 - val_accuracy: 0.5859\n",
      "Epoch 536/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.7001\n",
      "Epoch 536: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0083 - accuracy: 0.7001 - val_loss: 0.0261 - val_accuracy: 0.6172\n",
      "Epoch 537/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6906\n",
      "Epoch 537: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0082 - accuracy: 0.6906 - val_loss: 0.0263 - val_accuracy: 0.5859\n",
      "Epoch 538/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.7024\n",
      "Epoch 538: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0081 - accuracy: 0.7024 - val_loss: 0.0261 - val_accuracy: 0.6406\n",
      "Epoch 539/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.7023\n",
      "Epoch 539: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 0.0081 - accuracy: 0.7023 - val_loss: 0.0260 - val_accuracy: 0.5859\n",
      "Epoch 540/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6968\n",
      "Epoch 540: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0082 - accuracy: 0.6968 - val_loss: 0.0260 - val_accuracy: 0.6094\n",
      "Epoch 541/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6878\n",
      "Epoch 541: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0082 - accuracy: 0.6878 - val_loss: 0.0264 - val_accuracy: 0.5859\n",
      "Epoch 542/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6928\n",
      "Epoch 542: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0082 - accuracy: 0.6928 - val_loss: 0.0258 - val_accuracy: 0.6094\n",
      "Epoch 543/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.7024\n",
      "Epoch 543: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0082 - accuracy: 0.7024 - val_loss: 0.0260 - val_accuracy: 0.6094\n",
      "Epoch 544/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.7019\n",
      "Epoch 544: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0082 - accuracy: 0.7019 - val_loss: 0.0266 - val_accuracy: 0.5938\n",
      "Epoch 545/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.6946\n",
      "Epoch 545: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0080 - accuracy: 0.6946 - val_loss: 0.0265 - val_accuracy: 0.6094\n",
      "Epoch 546/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6975\n",
      "Epoch 546: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0081 - accuracy: 0.6975 - val_loss: 0.0258 - val_accuracy: 0.6094\n",
      "Epoch 547/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6956\n",
      "Epoch 547: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0081 - accuracy: 0.6956 - val_loss: 0.0263 - val_accuracy: 0.6016\n",
      "Epoch 548/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6985\n",
      "Epoch 548: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0081 - accuracy: 0.6985 - val_loss: 0.0265 - val_accuracy: 0.6016\n",
      "Epoch 549/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.7003\n",
      "Epoch 549: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0082 - accuracy: 0.7003 - val_loss: 0.0260 - val_accuracy: 0.5781\n",
      "Epoch 550/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6937\n",
      "Epoch 550: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0081 - accuracy: 0.6937 - val_loss: 0.0263 - val_accuracy: 0.6094\n",
      "Epoch 551/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.7001\n",
      "Epoch 551: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0082 - accuracy: 0.7001 - val_loss: 0.0263 - val_accuracy: 0.6094\n",
      "Epoch 552/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.6998\n",
      "Epoch 552: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0080 - accuracy: 0.6998 - val_loss: 0.0260 - val_accuracy: 0.6094\n",
      "Epoch 553/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.7044\n",
      "Epoch 553: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0081 - accuracy: 0.7044 - val_loss: 0.0258 - val_accuracy: 0.5859\n",
      "Epoch 554/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6978\n",
      "Epoch 554: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0081 - accuracy: 0.6978 - val_loss: 0.0259 - val_accuracy: 0.6094\n",
      "Epoch 555/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6978\n",
      "Epoch 555: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 0.0081 - accuracy: 0.6978 - val_loss: 0.0267 - val_accuracy: 0.5859\n",
      "Epoch 556/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6997\n",
      "Epoch 556: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 0.0081 - accuracy: 0.6997 - val_loss: 0.0261 - val_accuracy: 0.5859\n",
      "Epoch 557/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6966\n",
      "Epoch 557: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 0.0081 - accuracy: 0.6966 - val_loss: 0.0262 - val_accuracy: 0.5859\n",
      "Epoch 558/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.6960\n",
      "Epoch 558: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0080 - accuracy: 0.6960 - val_loss: 0.0262 - val_accuracy: 0.5938\n",
      "Epoch 559/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6984\n",
      "Epoch 559: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0081 - accuracy: 0.6984 - val_loss: 0.0263 - val_accuracy: 0.5703\n",
      "Epoch 560/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6945\n",
      "Epoch 560: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0081 - accuracy: 0.6945 - val_loss: 0.0263 - val_accuracy: 0.6094\n",
      "Epoch 561/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6985\n",
      "Epoch 561: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0081 - accuracy: 0.6985 - val_loss: 0.0263 - val_accuracy: 0.6094\n",
      "Epoch 562/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6966\n",
      "Epoch 562: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0081 - accuracy: 0.6966 - val_loss: 0.0263 - val_accuracy: 0.6016\n",
      "Epoch 563/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.7055\n",
      "Epoch 563: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0080 - accuracy: 0.7055 - val_loss: 0.0262 - val_accuracy: 0.6016\n",
      "Epoch 564/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6920\n",
      "Epoch 564: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0081 - accuracy: 0.6920 - val_loss: 0.0262 - val_accuracy: 0.5938\n",
      "Epoch 565/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.6919\n",
      "Epoch 565: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0080 - accuracy: 0.6919 - val_loss: 0.0266 - val_accuracy: 0.5859\n",
      "Epoch 566/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.6975\n",
      "Epoch 566: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0082 - accuracy: 0.6975 - val_loss: 0.0265 - val_accuracy: 0.5781\n",
      "Epoch 567/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.6978\n",
      "Epoch 567: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0079 - accuracy: 0.6978 - val_loss: 0.0264 - val_accuracy: 0.6094\n",
      "Epoch 568/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6997\n",
      "Epoch 568: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0081 - accuracy: 0.6997 - val_loss: 0.0262 - val_accuracy: 0.5938\n",
      "Epoch 569/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.6940\n",
      "Epoch 569: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.0080 - accuracy: 0.6940 - val_loss: 0.0266 - val_accuracy: 0.5859\n",
      "Epoch 570/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.6938\n",
      "Epoch 570: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.0079 - accuracy: 0.6938 - val_loss: 0.0264 - val_accuracy: 0.6016\n",
      "Epoch 571/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.7022\n",
      "Epoch 571: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 9s 117ms/step - loss: 0.0080 - accuracy: 0.7022 - val_loss: 0.0266 - val_accuracy: 0.5859\n",
      "Epoch 572/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.7003\n",
      "Epoch 572: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 104ms/step - loss: 0.0081 - accuracy: 0.7003 - val_loss: 0.0264 - val_accuracy: 0.5938\n",
      "Epoch 573/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.7017\n",
      "Epoch 573: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 9s 109ms/step - loss: 0.0080 - accuracy: 0.7017 - val_loss: 0.0262 - val_accuracy: 0.5859\n",
      "Epoch 574/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.6982\n",
      "Epoch 574: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 0.0081 - accuracy: 0.6982 - val_loss: 0.0264 - val_accuracy: 0.6094\n",
      "Epoch 575/5000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.6986\n",
      "Epoch 575: val_loss did not improve from 0.02488\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0079 - accuracy: 0.6986 - val_loss: 0.0266 - val_accuracy: 0.6094\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0266 - accuracy: 0.6094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.026613418012857437, 0.609375]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load net_2paras\n",
    "# code by Zhengcaizhi\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import keras.optimizers as optimizer\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import models, layers, Input, Model\n",
    "import skrf as rf\n",
    "from data_gene_2paras import cacul\n",
    "import keras.backend as K \n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Lambda, Concatenate, Dense, Conv1D, Flatten, Dropout\n",
    "\n",
    "# model2v3-v5\n",
    "def IANN():\n",
    "    input = Input(shape=(501,6))\n",
    "    branch1 = Lambda(lambda x:x[:,:,0])(input)  \n",
    "    branch2 = Lambda(lambda x:x[:,:,1])(input)\n",
    "    branch3 = Lambda(lambda x:x[:,:,2])(input)\n",
    "    branch4 = Lambda(lambda x:x[:,:,3])(input)\n",
    "    branch5 = Lambda(lambda x:x[:,:,4])(input)\n",
    "    branch6 = Lambda(lambda x:x[:,:,5])(input)\n",
    "    h1 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch1)\n",
    "    h2 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch2)\n",
    "    h3 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch3)\n",
    "    h4 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch4)\n",
    "    h5 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch5)\n",
    "    h6 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch6)\n",
    "    h1 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h1)\n",
    "    # h1 = Dropout(0.2)(h1)\n",
    "    h2 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h2)\n",
    "    # h2 = Dropout(0.2)(h2)\n",
    "    h3 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h3)\n",
    "    # h3 = Dropout(0.2)(h3)\n",
    "    h4 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h4)\n",
    "    # h4 = Dropout(0.2)(h4)\n",
    "    h5 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h5)\n",
    "    # h5 = Dropout(0.2)(h5)\n",
    "    h6 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h6)\n",
    "    # h6 = Dropout(0.2)(h6)\n",
    "    h1 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h1)\n",
    "    # h1 = Dropout(0.2)(h1)\n",
    "    h2 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h2)\n",
    "    # h2 = Dropout(0.2)(h2)\n",
    "    h3 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h3)\n",
    "    # h3 = Dropout(0.2)(h3)\n",
    "    h4 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h4)\n",
    "    # h4 = Dropout(0.2)(h4)\n",
    "    h5 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h5)\n",
    "    # h5 = Dropout(0.2)(h5)\n",
    "    h6 = Dense(1003,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h6)\n",
    "    # h6 = Dropout(0.2)(h6)\n",
    "\n",
    "    # h1 = Dense(3207,activation='relu', kernel_initializer='he_uniform')(h1)\n",
    "    # h2 = Dense(3207,activation='relu', kernel_initializer='he_uniform')(h2)\n",
    "    # h3 = Dense(3207,activation='relu', kernel_initializer='he_uniform')(h3)\n",
    "    # h4 = Dense(3207,activation='relu', kernel_initializer='he_uniform')(h4)\n",
    "    # h5 = Dense(3207,activation='relu', kernel_initializer='he_uniform')(h5)\n",
    "    # hn drop?\n",
    "    # o1 = Dense(16,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h1)\n",
    "    # o2 = Dense(16,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h2)\n",
    "    # o3 = Dense(16,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h3)\n",
    "    # o4 = Dense(16,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h4)\n",
    "    # o5 = Dense(16,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h5)\n",
    "    # o6 = Dense(16,activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(h6)\n",
    "    # out1 = Concatenate(axis = -1)((o1,o2,o3,o4,o5,o6))\n",
    "    out1 = Concatenate(axis = -1)((h1,h2,h3,h4,h5,h6))\n",
    "    # out1 = Dense(100, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(out1) #model2v6\n",
    "    # out1 = Dense(100, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(out1) #model2v6\n",
    "    out1 = Dense(511, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(out1) #model2v6\n",
    "    # out1 = Dropout(0.2)(out1)\n",
    "    out1 = Dense(255, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(out1) #model2v6\n",
    "    # out1 = Dropout(0.2)(out1)\n",
    "    out1 = Dense(127, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(out1) #model2v6\n",
    "    # out1 = Dropout(0.2)(out1)\n",
    "    out1 = Dense(63, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(out1) #model2v6\n",
    "    # out1 = Dropout(0.2)(out1)\n",
    "    out1 = Dense(31, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(out1) #model2v6\n",
    "    # out1 = Dropout(0.2)(out1)\n",
    "    h2 = Dense(15, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00,l2=0.00001))(out1)\n",
    "    out2 = Dense(7,activation='sigmoid', kernel_initializer='he_uniform')(h2)\n",
    "    model = Model(inputs=input, outputs=out2)\n",
    "    model.summary() \n",
    "    return model\n",
    "\n",
    "def conv(): #model3v1\n",
    "    input = Input(shape=(801,5))\n",
    "    branchs = Lambda(lambda x: tf.split(x, num_or_size_splits=5, axis=-1))(input)\n",
    "    o1 = Conv1D(4,256,activation='relu', kernel_initializer='he_uniform')(branchs[0])\n",
    "    o2 = Conv1D(4,256,activation='relu', kernel_initializer='he_uniform')(branchs[1])\n",
    "    o3 = Conv1D(4,256,activation='relu', kernel_initializer='he_uniform')(branchs[2])\n",
    "    o4 = Conv1D(4,256,activation='relu', kernel_initializer='he_uniform')(branchs[3])\n",
    "    o5 = Conv1D(4,256,activation='relu', kernel_initializer='he_uniform')(branchs[4])\n",
    "    o1 = Conv1D(16,256,activation='relu', kernel_initializer='he_uniform')(o1)\n",
    "    o2 = Conv1D(16,256,activation='relu', kernel_initializer='he_uniform')(o2)\n",
    "    o3 = Conv1D(16,256,activation='relu', kernel_initializer='he_uniform')(o3)\n",
    "    o4 = Conv1D(16,256,activation='relu', kernel_initializer='he_uniform')(o4)\n",
    "    o5 = Conv1D(16,256,activation='relu', kernel_initializer='he_uniform')(o5)\n",
    "    out1 = Concatenate(axis = -1)((o1,o2,o3,o4,o5))\n",
    "    c1 = Conv1D(64,256,activation='relu', kernel_initializer='he_uniform')(out1)\n",
    "    out1 = Flatten()(c1)\n",
    "    out2 = Dense(2,activation='sigmoid', kernel_initializer='he_uniform')(h2)\n",
    "    model = Model(inputs=input, outputs=out2)\n",
    "    model.summary() \n",
    "    return model\n",
    "# names = [[150,45,80],[150,50,80],[150,55,80]\n",
    "#          ,[60,45,80],[60,50,80],[60,55,80]\n",
    "#          ,[90,45,80],[90,50,80],[90,55,80]]\n",
    "\n",
    "# MP_num = [45,60,65,55,50]\n",
    "# MP_num = [45]\n",
    "# Pitch_num = [180,200,225,260]\n",
    "# PF = [[180,8.5],[200,7.5],[225,6.5],[260,5.5]]\n",
    "# PF = [[200,7.5]]\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "0.001,\n",
    "decay_steps=1000,\n",
    "#   10decay\n",
    "decay_rate=1,\n",
    "staircase=False)\n",
    "\n",
    "\n",
    "\n",
    "model = IANN()\n",
    "freq = np.linspace(5*1e8,1.5*1e9,501)\n",
    "data_path = 'D:\\\\data\\\\7p/input2w/2w.npy'\n",
    "data1 = np.load(data_path)\n",
    "data2_path= 'D:\\\\data\\\\7p/input2w/2w_0.5.npy'\n",
    "data2 = np.load(data2_path)\n",
    "data = np.concatenate((data1,data2))\n",
    "label_path1 = 'D:\\\\data\\\\7p/out/MP60.csv'\n",
    "label_path2 = 'D:\\\\data\\\\7p/out/MP60_0.5.csv'\n",
    "label1 = np.genfromtxt(label_path1,delimiter=',')\n",
    "label2 = np.genfromtxt(label_path2,delimiter=',')\n",
    "label = np.concatenate((label1,label2))\n",
    "label_mm_path = label_path1 + 'maxmin.csv'\n",
    "label_mm = np.genfromtxt(label_mm_path, delimiter=',')\n",
    "label_max = label_mm[0]\n",
    "label_min = label_mm[1]\n",
    "label = (label-label_min) / (label_max - label_min)\n",
    "data_set = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(data), tf.data.Dataset.from_tensor_slices(label)))\n",
    "BATCHSIZE=128\n",
    "dataset = data_set\n",
    "valiset = dataset.take(128).batch(BATCHSIZE).cache().prefetch(tf.data.AUTOTUNE) \n",
    "trainset = dataset.skip(128)\n",
    "trainset = trainset.cache().shuffle(trainset.cardinality(),seed=1919810,reshuffle_each_iteration=True).repeat().batch(BATCHSIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "checkpoint_filepath = os.path.join(os.getcwd(),'obs','weights','1')\n",
    "# shutil.rmtree(checkpoint_filepath)\n",
    "# checkpoint_filepath = checkpoint_filepath + '\\\\' +str(Pitch)\n",
    "# log_dir=\"c:\\\\Users\\\\caizhi.zheng\\\\code\\\\For AI\\\\logs2v5\\\\\"+str(MP)+'-'+str(Pitch)\n",
    "log_dir=\"c:\\\\Users\\\\caizhi.zheng\\\\code\\\\For AI\\\\logs\"\n",
    "# shutil.rmtree(log_dir,ignore_errors=True)  \n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', verbose=1, save_weights_only=True, \n",
    "                                    save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=200)\n",
    "Board = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
    "# opt = optimizer.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=optimizer.Adam(lr_schedule), loss='mse', metrics='accuracy')\n",
    "# model.reset_states()\n",
    "# model.reset_metrics()\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "# reduce_lr = LearningRateScheduler(scheduler)\n",
    "# model.fit(dataset, epochs=25)\n",
    "history = model.fit(trainset, validation_data=valiset, steps_per_epoch=80, epochs=5000, callbacks=[model_checkpoint,early_stop,Board])#validation_split=0.02,)\n",
    "model.evaluate(valiset)\n",
    "# test\n",
    "# if Pitch != 225:\n",
    "#     pitch = int(0.1*Pitch)\n",
    "# test_path = 'C:\\\\Users\\\\caizhi.zheng\\\\code\\\\For AI\\\\SNP Selection/'+'LT'+str(NT)+ 'MP'+str(MP) +'P'+str(pitch) +'/'\n",
    "# saw_set = rf.read_all_networks(test_path)\n",
    "# for name in saw_set:\n",
    "#     saw = saw_set[name]\n",
    "# y0 = saw['900-1100MHZ'].y[:,0,0]\n",
    "# test_set = np.ones((801,5))*0.01\n",
    "# # test_set = np.zeros((801,5))\n",
    "# # data_mm_path = data_path + 'musi.csv'\n",
    "# # data_mm = np.genfromtxt(data_mm_path, delimiter=',')\n",
    "# # data_mu = data_mm[:5]\n",
    "# # data_sigma = data_mm[5:10]\n",
    "# # test_set = (test_set * data_sigma) + data_mu\n",
    "\n",
    "# test_set = tf.data.Dataset.from_tensors(test_set).batch(1) \n",
    "# # model = models.load_model(checkpoint_filepath)\n",
    "# # model.evaluate(dataset.batch(BATCHSIZE))\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "# test_result = model.predict(test_set)\n",
    "# print(test_result) \n",
    "# # print(test_result.shape) \n",
    "# # \n",
    "# test_result = (test_result[0]*(label_max - label_min)) + label_min\n",
    "# print(test_result) \n",
    "# result_path = 'c:\\\\Users\\\\caizhi.zheng\\\\code\\\\For AI\\\\loss_picture/result' + suffix + '.csv'\n",
    "# with open(result_path, 'w') as f:\n",
    "#     np.savetxt(f, test_result, delimiter=',')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'D:\\\\data\\\\7p/input2w/2w.npy'\n",
    "data = np.load(data_path)\n",
    "label_path = 'D:\\\\data\\\\7p/out/MP60.csv'\n",
    "label = np.genfromtxt(label_path,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path1 = 'D:\\\\data\\\\7p/out/MP60.csv'\n",
    "label_path2 = 'D:\\\\data\\\\7p/out/MP60_0.5.csv'\n",
    "label1 = np.genfromtxt(label_path1,delimiter=',')\n",
    "label2 = np.genfromtxt(label_path2,delimiter=',')\n",
    "label = np.concatenate((label1,label2))\n",
    "label_mm_path = label_path1 + 'maxmin.csv'\n",
    "label_mm = np.genfromtxt(label_mm_path, delimiter=',')\n",
    "label_max = label_mm[0]\n",
    "label_min = label_mm[1]\n",
    "label = (label-label_min) / (label_max - label_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.loadtxt('./h-0.01-0.01-1.txt',skiprows=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:\\\\data\\\\7p\\\\input2w/2w.npymusi.csv'\n",
    "mu = np.loadtxt(file_path)[:6]\n",
    "sigma = np.loadtxt(file_path)[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.028123775244771"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'D:\\\\data\\\\7p/input2w/2w.npy'\n",
    "ori = np.load(data_path)\n",
    "ori[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.494421620953023"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_su.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 74s 4ms/step\n",
      "[[0.16790366 0.525216   0.2542184  ... 0.4867785  0.80161285 0.9639446 ]\n",
      " [0.8079351  0.4037457  0.24644332 ... 0.18190743 0.17474867 0.18649206]\n",
      " [0.7203641  0.6463587  0.8350606  ... 0.02908006 0.8745965  0.5481163 ]\n",
      " ...\n",
      " [0.48834985 0.27014872 0.8148661  ... 0.520596   0.8461431  0.33669502]\n",
      " [0.9132136  0.66640425 0.44110692 ... 0.6573584  0.9021854  0.8030246 ]\n",
      " [0.30980247 0.590984   0.12329486 ... 0.04711048 0.7284624  0.36888653]]\n"
     ]
    }
   ],
   "source": [
    "def YtoZS(Y_COM, freq):\n",
    "    Y11 = 20*np.log10(abs(Y_COM))\n",
    "    Z_COM = 1/Y_COM\n",
    "    z = 50 \n",
    "    S11_COM = (1-Y_COM*z)/(1+Y_COM*z)\n",
    "\n",
    "    group_delay = -np.diff(np.unwrap(np.angle(S11_COM))) / np.diff(2 * np.pi * freq)\n",
    "    group_delay = np.concatenate(([group_delay[0]], group_delay))\n",
    "    Q_COM = 2*np.pi*(freq)*group_delay *abs(S11_COM)/(1-abs(S11_COM)**2)\n",
    "\n",
    "    DSP_COM = 10*np.log10(1-abs(S11_COM)**2)\n",
    "    return Y11, Z_COM.real, S11_COM.real, S11_COM.imag, Q_COM, DSP_COM\n",
    "\n",
    "# ttt = tt[:,97:99]\n",
    "# test = np.vectorize(complex)(ttt[:,0],ttt[:,1])\n",
    "# test6 = np.array(YtoZS(test,freq))\n",
    "# test_su = (test6.T - mu)/sigma\n",
    "test_set = tf.data.Dataset.from_tensor_slices(ori).batch(1)\n",
    "# model = models.load_model(checkpoint_filepath)\n",
    "# model.evaluate(dataset.batch(BATCHSIZE))\n",
    "model.load_weights(checkpoint_filepath)\n",
    "test_result = model.predict(test_set)\n",
    "print(test_result)\n",
    "# test_result = (test_result[0]*(label_max - label_min)) + label_min\n",
    "# print(test_result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result=test_result*(label_max-label_min)+label_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00018828 -0.00317849  0.00464396  0.00156591  0.07295183  0.04812163\n",
      "  0.00018815]\n",
      "[ 0.07675327  0.36135997  0.33178932  0.40056308 24.91903023  9.84221781\n",
      "  0.15463551]\n"
     ]
    }
   ],
   "source": [
    "# label = label*(label_max-label_min)+label_min\n",
    "# print(label[0])\n",
    "# print(test_result[0])\n",
    "print(np.mean((test_result-label)/label,axis=0))\n",
    "print(np.max((test_result-label)/label,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "[[2.8952600e-06 9.9999911e-01 1.5353864e-03 1.9839723e-03 2.2314771e-03\n",
      "  8.4994686e-01 5.7632917e-01]]\n",
      "[4.02850821e+01 3.77054993e+02 1.45631215e+02 4.00396794e-02\n",
      " 1.55011743e-02 3.34890812e-01 4.35558332e+03]\n"
     ]
    }
   ],
   "source": [
    "def YtoZS(Y_COM, freq):\n",
    "    Y11 = 20*np.log10(abs(Y_COM))\n",
    "    Z_COM = 1/Y_COM\n",
    "    z = 50 \n",
    "    S11_COM = (1-Y_COM*z)/(1+Y_COM*z)\n",
    "\n",
    "    group_delay = -np.diff(np.unwrap(np.angle(S11_COM))) / np.diff(2 * np.pi * freq)\n",
    "    group_delay = np.concatenate(([group_delay[0]], group_delay))\n",
    "    Q_COM = 2*np.pi*(freq)*group_delay *abs(S11_COM)/(1-abs(S11_COM)**2)\n",
    "\n",
    "    DSP_COM = 10*np.log10(1-abs(S11_COM)**2)\n",
    "    return Y11, Z_COM.real, S11_COM.real, S11_COM.imag, Q_COM, DSP_COM\n",
    "\n",
    "ttt = tt[:,97:99]\n",
    "test = np.vectorize(complex)(ttt[:,0],ttt[:,1])\n",
    "test6 = np.array(YtoZS(test,freq))\n",
    "test_su = (test6.T - mu)/sigma\n",
    "test_set = tf.data.Dataset.from_tensors(test_su).batch(1) \n",
    "# model = models.load_model(checkpoint_filepath)\n",
    "# model.evaluate(dataset.batch(BATCHSIZE))\n",
    "model.load_weights(checkpoint_filepath)\n",
    "test_result = model.predict(test_set)\n",
    "print(test_result)\n",
    "test_result = (test_result[0]*(label_max - label_min)) + label_min\n",
    "print(test_result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import special\n",
    "import cmath\n",
    "import sawcom7 as sc\n",
    "def cacul(paras):\n",
    "    npiezo_1 = paras[0]\n",
    "    eta = paras[1]\n",
    "    e = paras[2]\n",
    "    alpha = paras[3]\n",
    "    c = paras[4]\n",
    "    k2 = paras[5]\n",
    "    vb = paras[6]\n",
    "    epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "    pI = 3600E-9 #The period of IDT, normally is one wavelength\n",
    "    h = 0.08*pI #The thickness of IDT, Al or Al-Cu1%\n",
    "    W1 = 20*pI # Width of IDT (acoustic aperture), in m\n",
    "    m_ratio = 0.6 #The metallization ratio\n",
    "\n",
    "    eta_b = (eta+2*abs(e))/2\n",
    "    epsilon = npiezo_1*epsilon_0\n",
    "    x1 = np.cos(np.pi*m_ratio )\n",
    "    m1 = math.sqrt((1-x1)/2) \n",
    "    km1 = special.ellipk(m1, out=None)\n",
    "    p1 = 2*km1/np.pi\n",
    "    x2 = -np.cos(np.pi*m_ratio )\n",
    "    m2 = math.sqrt((1-x2)/2) \n",
    "    km2 = special.ellipk(m2, out=None)\n",
    "    p2 = 2*km2/np.pi\n",
    "    p_factor = p1/p2\n",
    "    freq = np.linspace(0.5E9, 1.5E9, 501)\n",
    "    # freq_mhz = freq/1e6\n",
    "    delta_v = - (eta**2)/2\n",
    "    k = abs(e)*(eta+abs(e)/2)\n",
    "    kb = -(abs(e)**2)*eta/(eta+2*abs(e))\n",
    "    delta_b = -((eta**2)-2*((abs(e)**2)))/4\n",
    "    omega = freq*2*np.pi\n",
    "    delta = omega/vb - 2*np.pi/pI - 1j*alpha\n",
    "\n",
    "    v_delta = []\n",
    "    for i in range(0,len(delta)):\n",
    "        v_delta_0 = eta_b/((cmath.sqrt(delta_b-delta[i]))+ eta_b)# wave velocity in m/s\n",
    "        v_delta.append(v_delta_0)\n",
    "    v_delta = np.array(v_delta)\n",
    "    omega = freq*2*np.pi\n",
    "    C = (W1*epsilon*p_factor)/pI ##To check\n",
    "    xi = []\n",
    "    for i in range(0,len(omega)):\n",
    "        xi_0 = c*cmath.sqrt((omega[i]*C*k2)/(pI*np.pi))\n",
    "        xi_0 = -1j*xi_0\n",
    "        xi.append(xi_0)\n",
    "    xi = np.array(xi)\n",
    "\n",
    "    lam1 = pI # Wavelength in m of SAW filters \n",
    "    c12 =  -1j*c*(k+kb*v_delta) # Reflectivity per unit length (~1.7% reflected per IDT spaced at lam/2)\n",
    "    a1 = -xi # The transduction coefficient\n",
    "    n1 = 100 # The number of IDT pairs\n",
    "    L1 = n1*lam1 # Length of total IDT the grating, in m\n",
    "    #W1 = 22*lam1 # Width of IDT (acoustic aperture), in m\n",
    "    #d = sc.delta(freq,v1,lam1) - 500j\n",
    "    Ct=n1*W1*epsilon # Static capacitance of total IDT\n",
    "    #d1 = sc.delta(freq,v1,lam1)\n",
    "    d1 = sc.thetau(c,delta,delta_v,kb,v_delta)\n",
    "    C1 = sc.C0(freq,Ct)\n",
    "    idt_ref_1 = sc.pmatrix(lam1,c12,a1,L1,d1,C1) #The P-Matrix of SAW resonator with refelection \n",
    "    # y11 = 20 * np.log10(abs(idt_ref_1.p33)/5)\n",
    "    y11 = (idt_ref_1.p33)/5\n",
    "    return y11\n",
    "\n",
    "y_dl = cacul(test_result)\n",
    "plt.plot(freq,abs(y_dl))\n",
    "plt.plot(freq,abs(test))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33489081228854656"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_su = (test.T - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 239ms/step\n"
     ]
    }
   ],
   "source": [
    "test_set = tf.data.Dataset.from_tensors(test_su).batch(1) \n",
    "# model = models.load_model(checkpoint_filepath)\n",
    "# model.evaluate(dataset.batch(BATCHSIZE))\n",
    "model.load_weights(checkpoint_filepath)\n",
    "test_result = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0016182 , 0.0016247 , 0.0016312 , 0.00163769, 0.00164419,\n",
       "       0.00165069, 0.00165719, 0.00166369, 0.00167019, 0.00167669,\n",
       "       0.00168318, 0.00168968, 0.00169618, 0.00170268, 0.00170918,\n",
       "       0.00171569, 0.00172219, 0.00172869, 0.00173519, 0.00174169,\n",
       "       0.00174819, 0.00175469, 0.0017612 , 0.0017677 , 0.0017742 ,\n",
       "       0.0017807 , 0.00178721, 0.00179371, 0.00180021, 0.00180672,\n",
       "       0.00181322, 0.00181973, 0.00182623, 0.00183273, 0.00183924,\n",
       "       0.00184574, 0.00185225, 0.00185875, 0.00186526, 0.00187176,\n",
       "       0.00187827, 0.00188477, 0.00189128, 0.00189779, 0.00190429,\n",
       "       0.0019108 , 0.00191731, 0.00192381, 0.00193032, 0.00193682,\n",
       "       0.00194333, 0.00194984, 0.00195634, 0.00196285, 0.00196936,\n",
       "       0.00197587, 0.00198237, 0.00198888, 0.00199539, 0.00200189,\n",
       "       0.0020084 , 0.00201491, 0.00202141, 0.00202792, 0.00203443,\n",
       "       0.00204094, 0.00204744, 0.00205395, 0.00206046, 0.00206696,\n",
       "       0.00207347, 0.00207998, 0.00208648, 0.00209299, 0.0020995 ,\n",
       "       0.002106  , 0.00211251, 0.00211901, 0.00212552, 0.00213203,\n",
       "       0.00213853, 0.00214504, 0.00215154, 0.00215805, 0.00216455,\n",
       "       0.00217105, 0.00217756, 0.00218406, 0.00219056, 0.00219707,\n",
       "       0.00220357, 0.00221007, 0.00221658, 0.00222308, 0.00222958,\n",
       "       0.00223608, 0.00224258, 0.00224908, 0.00225558, 0.00226208,\n",
       "       0.00226858, 0.00227508, 0.00228157, 0.00228807, 0.00229457,\n",
       "       0.00230106, 0.00230756, 0.00231406, 0.00232055, 0.00232704,\n",
       "       0.00233354, 0.00234003, 0.00234652, 0.00235301, 0.00235951,\n",
       "       0.002366  , 0.00237248, 0.00237897, 0.00238546, 0.00239195,\n",
       "       0.00239844, 0.00240492, 0.00241141, 0.00241789, 0.00242437,\n",
       "       0.00243086, 0.00243734, 0.00244382, 0.0024503 , 0.00245678,\n",
       "       0.00246326, 0.00246973, 0.00247621, 0.00248268, 0.00248916,\n",
       "       0.00249563, 0.0025021 , 0.00250857, 0.00251504, 0.00252151,\n",
       "       0.00252798, 0.00253445, 0.00254091, 0.00254738, 0.00255384,\n",
       "       0.00256031, 0.00256677, 0.00257323, 0.00257969, 0.00258614,\n",
       "       0.0025926 , 0.00259905, 0.00260551, 0.00261196, 0.00261841,\n",
       "       0.00262486, 0.00263131, 0.00263776, 0.00264421, 0.00265065,\n",
       "       0.0026571 , 0.00266354, 0.00266998, 0.00267642, 0.00268286,\n",
       "       0.0026893 , 0.00269573, 0.00270217, 0.0027086 , 0.00271503,\n",
       "       0.00272146, 0.00272789, 0.00273432, 0.00274074, 0.00274717,\n",
       "       0.00275359, 0.00276002, 0.00276644, 0.00277285, 0.00277927,\n",
       "       0.00278569, 0.0027921 , 0.00279852, 0.00280493, 0.00281134,\n",
       "       0.00281775, 0.00282416, 0.00283057, 0.00283697, 0.00284337,\n",
       "       0.00284978, 0.00285618, 0.00286258, 0.00286898, 0.00287537,\n",
       "       0.00288177, 0.00288816, 0.00289456, 0.00290095, 0.00290734,\n",
       "       0.00291373, 0.00292012, 0.0029265 , 0.00293289, 0.00293927,\n",
       "       0.00294565, 0.00295204, 0.00295842, 0.00296479, 0.00297117,\n",
       "       0.00297755, 0.00298392, 0.0029903 , 0.00299667, 0.00300304,\n",
       "       0.00300941, 0.00301578, 0.00302215, 0.00302852, 0.00303488,\n",
       "       0.00304125, 0.00304761, 0.00305398, 0.00306034, 0.0030667 ,\n",
       "       0.00307306, 0.00307942, 0.00308578, 0.00309213, 0.00309849,\n",
       "       0.00310485, 0.0031112 , 0.00311755, 0.00312391, 0.00313026,\n",
       "       0.00313661, 0.00314296, 0.00314931, 0.00315566, 0.00316201,\n",
       "       0.00316835, 0.0031747 , 0.00318105, 0.00318739, 0.00319374,\n",
       "       0.00320008, 0.00320642, 0.00321277, 0.00321911, 0.00322545,\n",
       "       0.00323179, 0.00323813, 0.00324447, 0.00325081, 0.00325715,\n",
       "       0.00326349, 0.00326983, 0.00327617, 0.00328251, 0.00328884,\n",
       "       0.00329518, 0.00330152, 0.00330785, 0.00331419, 0.00332052,\n",
       "       0.00332686, 0.00333319, 0.00333953, 0.00334586, 0.0033522 ,\n",
       "       0.00335853, 0.00336487, 0.0033712 , 0.00337754, 0.00338387,\n",
       "       0.0033902 , 0.00339654, 0.00340287, 0.0034092 , 0.00341554,\n",
       "       0.00342187, 0.0034282 , 0.00343454, 0.00344087, 0.0034472 ,\n",
       "       0.00345354, 0.00345987, 0.00346621, 0.00347254, 0.00347887,\n",
       "       0.00348521, 0.00349154, 0.00349788, 0.00350421, 0.00351054,\n",
       "       0.00351688, 0.00352321, 0.00352955, 0.00353588, 0.00354222,\n",
       "       0.00354856, 0.00355489, 0.00356123, 0.00356756, 0.0035739 ,\n",
       "       0.00358024, 0.00358657, 0.00359291, 0.00359925, 0.00360559,\n",
       "       0.00361193, 0.00361827, 0.0036246 , 0.00363094, 0.00363728,\n",
       "       0.00364362, 0.00364996, 0.00365631, 0.00366265, 0.00366899,\n",
       "       0.00367533, 0.00368167, 0.00368802, 0.00369436, 0.0037007 ,\n",
       "       0.00370705, 0.00371339, 0.00371974, 0.00372608, 0.00373243,\n",
       "       0.00373878, 0.00374512, 0.00375147, 0.00375782, 0.00376417,\n",
       "       0.00377052, 0.00377687, 0.00378322, 0.00378957, 0.00379592,\n",
       "       0.00380228, 0.00380863, 0.00381499, 0.00382135, 0.00382771,\n",
       "       0.00383407, 0.00384045, 0.00384686, 0.00385322, 0.00385957,\n",
       "       0.00386593, 0.00387228, 0.00387863, 0.00388498, 0.00389134,\n",
       "       0.00389769, 0.00390404, 0.0039104 , 0.00391675, 0.0039231 ,\n",
       "       0.00392946, 0.00393582, 0.00394217, 0.00394853, 0.00395489,\n",
       "       0.00396124, 0.0039676 , 0.00397396, 0.00398032, 0.00398668,\n",
       "       0.00399304, 0.0039994 , 0.00400577, 0.00401213, 0.00401849,\n",
       "       0.00402486, 0.00403122, 0.00403758, 0.00404395, 0.00405032,\n",
       "       0.00405668, 0.00406305, 0.00406942, 0.00407579, 0.00408216,\n",
       "       0.00408853, 0.0040949 , 0.00410127, 0.00410764, 0.00411401,\n",
       "       0.00412038, 0.00412675, 0.00413313, 0.0041395 , 0.00414588,\n",
       "       0.00415225, 0.00415863, 0.004165  , 0.00417138, 0.00417776,\n",
       "       0.00418413, 0.00419051, 0.00419689, 0.00420327, 0.00420965,\n",
       "       0.00421603, 0.00422241, 0.00422879, 0.00423518, 0.00424156,\n",
       "       0.00424794, 0.00425433, 0.00426071, 0.00426709, 0.00427348,\n",
       "       0.00427986, 0.00428625, 0.00429264, 0.00429903, 0.00430541,\n",
       "       0.0043118 , 0.00431819, 0.00432458, 0.00433097, 0.00433736,\n",
       "       0.00434375, 0.00435014, 0.00435653, 0.00436292, 0.00436932,\n",
       "       0.00437571, 0.0043821 , 0.0043885 , 0.00439489, 0.00440129,\n",
       "       0.00440768, 0.00441408, 0.00442048, 0.00442687, 0.00443327,\n",
       "       0.00443967, 0.00444607, 0.00445247, 0.00445887, 0.00446527,\n",
       "       0.00447167, 0.00447807, 0.00448447, 0.00449087, 0.00449727,\n",
       "       0.00450368, 0.00451008, 0.00451648, 0.00452289, 0.00452929,\n",
       "       0.0045357 , 0.00454211, 0.00454851, 0.00455492, 0.00456133,\n",
       "       0.00456773, 0.00457414, 0.00458055, 0.00458696, 0.00459337,\n",
       "       0.00459978, 0.00460619, 0.0046126 , 0.00461901, 0.00462542,\n",
       "       0.00463184, 0.00463825, 0.00464466, 0.00465108, 0.00465749,\n",
       "       0.0046639 , 0.00467032, 0.00467674, 0.00468315, 0.00468957,\n",
       "       0.00469599, 0.0047024 , 0.00470882, 0.00471524, 0.00472166,\n",
       "       0.00472808, 0.0047345 , 0.00474092, 0.00474734, 0.00475376,\n",
       "       0.00476018, 0.00476661, 0.00477303, 0.00477945, 0.00478588,\n",
       "       0.0047923 , 0.00479873, 0.00480515, 0.00481158, 0.004818  ,\n",
       "       0.00482443])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(y_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.85496286e-05, 9.90440931e-05, 9.95380852e-05, 1.00031775e-04,\n",
       "       1.00525325e-04, 1.01018895e-04, 1.01512639e-04, 1.02006711e-04,\n",
       "       1.02501261e-04, 1.02996439e-04, 1.03492397e-04, 1.03989284e-04,\n",
       "       1.04487257e-04, 1.04986472e-04, 1.05487090e-04, 1.05989280e-04,\n",
       "       1.06493213e-04, 1.06999072e-04, 1.07507047e-04, 1.08017338e-04,\n",
       "       1.08530158e-04, 1.09045735e-04, 1.09564310e-04, 1.10086145e-04,\n",
       "       1.10611520e-04, 1.11140741e-04, 1.11674138e-04, 1.12212074e-04,\n",
       "       1.12754946e-04, 1.13303190e-04, 1.13857288e-04, 1.14417776e-04,\n",
       "       1.14985250e-04, 1.15560376e-04, 1.16143901e-04, 1.16736669e-04,\n",
       "       1.17339635e-04, 1.17953887e-04, 1.18580668e-04, 1.19221407e-04,\n",
       "       1.19877756e-04, 1.20551635e-04, 1.21245293e-04, 1.21961377e-04,\n",
       "       1.22703027e-04, 1.23473999e-04, 1.24278825e-04, 1.25123019e-04,\n",
       "       1.26013367e-04, 1.26958312e-04, 1.27968494e-04, 1.29057513e-04,\n",
       "       1.30243033e-04, 1.31548423e-04, 1.33005250e-04, 1.34657201e-04,\n",
       "       1.36566488e-04, 1.38824742e-04, 1.41572554e-04, 1.45036710e-04,\n",
       "       1.49606759e-04, 1.56007798e-04, 1.65737652e-04, 1.82315695e-04,\n",
       "       2.14526806e-04, 2.44338850e-04, 1.00771027e-04, 6.97927966e-05,\n",
       "       8.27284702e-05, 9.30540284e-05, 1.00077474e-04, 1.05079556e-04,\n",
       "       1.08838407e-04, 1.11793481e-04, 1.14203228e-04, 1.16227560e-04,\n",
       "       1.17970092e-04, 1.19500687e-04, 1.20868062e-04, 1.22107191e-04,\n",
       "       1.23243803e-04, 1.24297244e-04, 1.25282330e-04, 1.26210601e-04,\n",
       "       1.27091177e-04, 1.27931357e-04, 1.28737056e-04, 1.29513116e-04,\n",
       "       1.30263540e-04, 1.30991666e-04, 1.31700301e-04, 1.32391821e-04,\n",
       "       1.33068253e-04, 1.33731337e-04, 1.34382574e-04, 1.35023268e-04,\n",
       "       1.35654557e-04, 1.36277439e-04, 1.36892793e-04, 1.37501396e-04,\n",
       "       1.38103941e-04, 1.38701043e-04, 1.39293256e-04, 1.39881075e-04,\n",
       "       1.40464949e-04, 1.41045283e-04, 1.41622446e-04, 1.42196773e-04,\n",
       "       1.42768570e-04, 1.43338120e-04, 1.43905680e-04, 1.44471489e-04,\n",
       "       1.45035768e-04, 1.45598721e-04, 1.46160540e-04, 1.46721401e-04,\n",
       "       1.47281471e-04, 1.47840907e-04, 1.48399854e-04, 1.48958453e-04,\n",
       "       1.49516833e-04, 1.50075119e-04, 1.50633429e-04, 1.51191877e-04,\n",
       "       1.51750571e-04, 1.52309613e-04, 1.52869104e-04, 1.53429140e-04,\n",
       "       1.53989814e-04, 1.54551215e-04, 1.55113432e-04, 1.55676550e-04,\n",
       "       1.56240651e-04, 1.56805818e-04, 1.57372130e-04, 1.57939667e-04,\n",
       "       1.58508505e-04, 1.59078723e-04, 1.59650395e-04, 1.60223597e-04,\n",
       "       1.60798405e-04, 1.61374893e-04, 1.61953135e-04, 1.62533207e-04,\n",
       "       1.63115183e-04, 1.63699139e-04, 1.64285150e-04, 1.64873292e-04,\n",
       "       1.65463642e-04, 1.66056278e-04, 1.66651278e-04, 1.67248720e-04,\n",
       "       1.67848687e-04, 1.68451260e-04, 1.69056521e-04, 1.69664556e-04,\n",
       "       1.70275451e-04, 1.70889294e-04, 1.71506174e-04, 1.72126184e-04,\n",
       "       1.72749417e-04, 1.73375969e-04, 1.74005939e-04, 1.74639429e-04,\n",
       "       1.75276540e-04, 1.75917381e-04, 1.76562060e-04, 1.77210690e-04,\n",
       "       1.77863387e-04, 1.78520270e-04, 1.79181461e-04, 1.79847089e-04,\n",
       "       1.80517283e-04, 1.81192179e-04, 1.81871916e-04, 1.82556638e-04,\n",
       "       1.83246495e-04, 1.83941642e-04, 1.84642237e-04, 1.85348446e-04,\n",
       "       1.86060442e-04, 1.86778403e-04, 1.87502512e-04, 1.88232963e-04,\n",
       "       1.88969954e-04, 1.89713692e-04, 1.90464393e-04, 1.91222280e-04,\n",
       "       1.91987587e-04, 1.92760557e-04, 1.93541442e-04, 1.94330506e-04,\n",
       "       1.95128023e-04, 1.95934282e-04, 1.96749581e-04, 1.97574232e-04,\n",
       "       1.98408562e-04, 1.99252912e-04, 2.00107640e-04, 2.00973118e-04,\n",
       "       2.01849736e-04, 2.02737904e-04, 2.03638049e-04, 2.04550619e-04,\n",
       "       2.05476083e-04, 2.06414933e-04, 2.07367684e-04, 2.08334875e-04,\n",
       "       2.09317072e-04, 2.10314866e-04, 2.11328877e-04, 2.12359754e-04,\n",
       "       2.13408174e-04, 2.14474845e-04, 2.15560507e-04, 2.16665928e-04,\n",
       "       2.17791908e-04, 2.18939276e-04, 2.20108889e-04, 2.21301625e-04,\n",
       "       2.22518386e-04, 2.23760085e-04, 2.25027642e-04, 2.26321970e-04,\n",
       "       2.27643960e-04, 2.28994463e-04, 2.30374262e-04, 2.31784041e-04,\n",
       "       2.33224341e-04, 2.34695506e-04, 2.36197615e-04, 2.37730404e-04,\n",
       "       2.39293155e-04, 2.40884579e-04, 2.42502670e-04, 2.44144535e-04,\n",
       "       2.45806214e-04, 2.47482488e-04, 2.49166705e-04, 2.50850652e-04,\n",
       "       2.52524525e-04, 2.54177068e-04, 2.55795977e-04, 2.57368654e-04,\n",
       "       2.58883414e-04, 2.60331168e-04, 2.61707512e-04, 2.63014982e-04,\n",
       "       2.64265079e-04, 2.65479526e-04, 2.66690242e-04, 2.67937768e-04,\n",
       "       2.69268256e-04, 2.70729587e-04, 2.72367470e-04, 2.74222333e-04,\n",
       "       2.76327568e-04, 2.78709233e-04, 2.81387006e-04, 2.84375928e-04,\n",
       "       2.87688513e-04, 2.91336872e-04, 2.95334655e-04, 2.99698750e-04,\n",
       "       3.04450757e-04, 3.09618347e-04, 3.15236601e-04, 3.21349481e-04,\n",
       "       3.28011559e-04, 3.35290187e-04, 3.43268296e-04, 3.52048103e-04,\n",
       "       3.61756072e-04, 3.72549667e-04, 3.84626665e-04, 3.98238204e-04,\n",
       "       4.13707406e-04, 4.31456543e-04, 4.52047616e-04, 4.76244757e-04,\n",
       "       5.05113384e-04, 5.40184025e-04, 5.83735731e-04, 6.39314462e-04,\n",
       "       7.12748221e-04, 8.14313121e-04, 9.63896937e-04, 1.20526503e-03,\n",
       "       1.65415638e-03, 2.69784253e-03, 4.62745968e-03, 2.54199997e-03,\n",
       "       1.34650462e-03, 8.53875331e-04, 5.95457766e-04, 4.37655859e-04,\n",
       "       3.31551034e-04, 2.55369081e-04, 1.98019401e-04, 1.53279338e-04,\n",
       "       1.17396889e-04, 8.79839006e-05, 6.34631569e-05, 4.27907688e-05,\n",
       "       2.53995121e-05, 1.19790474e-05, 1.05117031e-05, 2.00147465e-05,\n",
       "       3.02291327e-05, 3.97546048e-05, 4.84901762e-05, 5.64922423e-05,\n",
       "       6.38397941e-05, 7.06088411e-05, 7.68670232e-05, 8.26731280e-05,\n",
       "       8.80778989e-05, 9.31251103e-05, 9.78526048e-05, 1.02293204e-04,\n",
       "       1.06475485e-04, 1.10424423e-04, 1.14161938e-04, 1.17707336e-04,\n",
       "       1.21077693e-04, 1.24288160e-04, 1.27352232e-04, 1.30281967e-04,\n",
       "       1.33088173e-04, 1.35780570e-04, 1.38367924e-04, 1.40858164e-04,\n",
       "       1.43258483e-04, 1.45575422e-04, 1.47814948e-04, 1.49982518e-04,\n",
       "       1.52083133e-04, 1.54121395e-04, 1.56101543e-04, 1.58027496e-04,\n",
       "       1.59902890e-04, 1.61731104e-04, 1.63515288e-04, 1.65258395e-04,\n",
       "       1.66963195e-04, 1.68632301e-04, 1.70268189e-04, 1.71873215e-04,\n",
       "       1.73449631e-04, 1.74999606e-04, 1.76525238e-04, 1.78028573e-04,\n",
       "       1.79511620e-04, 1.80976370e-04, 1.82424809e-04, 1.83858936e-04,\n",
       "       1.85280784e-04, 1.86692432e-04, 1.88096021e-04, 1.89493770e-04,\n",
       "       1.90887980e-04, 1.92281025e-04, 1.93675336e-04, 1.95073335e-04,\n",
       "       1.96477331e-04, 1.97889328e-04, 1.99310703e-04, 2.00741706e-04,\n",
       "       2.02180709e-04, 2.03623147e-04, 2.05060163e-04, 2.06477131e-04,\n",
       "       2.07852547e-04, 2.09158240e-04, 2.10362173e-04, 2.11434723e-04,\n",
       "       2.12357663e-04, 2.13132419e-04, 2.13782842e-04, 2.14349919e-04,\n",
       "       2.14880778e-04, 2.15417592e-04, 2.15990897e-04, 2.16618247e-04,\n",
       "       2.17306427e-04, 2.18054919e-04, 2.18859102e-04, 2.19712582e-04,\n",
       "       2.20608640e-04, 2.21541014e-04, 2.22504248e-04, 2.23493778e-04,\n",
       "       2.24505902e-04, 2.25537688e-04, 2.26586860e-04, 2.27651692e-04,\n",
       "       2.28730915e-04, 2.29823637e-04, 2.30929271e-04, 2.32047486e-04,\n",
       "       2.33178162e-04, 2.34321357e-04, 2.35477279e-04, 2.36646264e-04,\n",
       "       2.37828761e-04, 2.39025320e-04, 2.40236576e-04, 2.41463249e-04,\n",
       "       2.42706131e-04, 2.43966083e-04, 2.45244027e-04, 2.46540945e-04,\n",
       "       2.47857869e-04, 2.49195879e-04, 2.50556091e-04, 2.51939647e-04,\n",
       "       2.53347705e-04, 2.54781421e-04, 2.56241927e-04, 2.57730302e-04,\n",
       "       2.59247540e-04, 2.60794500e-04, 2.62371851e-04, 2.63980000e-04,\n",
       "       2.65618997e-04, 2.67288422e-04, 2.68987242e-04, 2.70713635e-04,\n",
       "       2.72464777e-04, 2.74236578e-04, 2.76023372e-04, 2.77817550e-04,\n",
       "       2.79609128e-04, 2.81385273e-04, 2.83129768e-04, 2.84822466e-04,\n",
       "       2.86438757e-04, 2.87949116e-04, 2.89318823e-04, 2.90507983e-04,\n",
       "       2.91471991e-04, 2.92162617e-04, 2.92529876e-04, 2.92524776e-04,\n",
       "       2.92102946e-04, 2.91228945e-04, 2.89880834e-04, 2.88054392e-04,\n",
       "       2.85766187e-04, 2.83054782e-04, 2.79979586e-04, 2.76617270e-04,\n",
       "       2.73056193e-04, 2.69389668e-04, 2.65709120e-04, 2.62098117e-04,\n",
       "       2.58627944e-04, 2.55355051e-04, 2.52320291e-04, 2.49549629e-04,\n",
       "       2.47055866e-04, 2.44840923e-04, 2.42898288e-04, 2.41215357e-04,\n",
       "       2.39775497e-04, 2.38559739e-04, 2.37548111e-04, 2.36720615e-04,\n",
       "       2.36057920e-04, 2.35541799e-04, 2.35155400e-04, 2.34883363e-04,\n",
       "       2.34711853e-04, 2.34628517e-04, 2.34622407e-04, 2.34683872e-04,\n",
       "       2.34804448e-04, 2.34976735e-04, 2.35194283e-04, 2.35451482e-04,\n",
       "       2.35743460e-04, 2.36065993e-04, 2.36415419e-04, 2.36788563e-04,\n",
       "       2.37182678e-04, 2.37595378e-04, 2.38024595e-04, 2.38468532e-04,\n",
       "       2.38925622e-04, 2.39394500e-04, 2.39873968e-04, 2.40362976e-04,\n",
       "       2.40860592e-04, 2.41365990e-04, 2.41878431e-04, 2.42397247e-04,\n",
       "       2.42921829e-04, 2.43451619e-04, 2.43986093e-04, 2.44524761e-04,\n",
       "       2.45067151e-04])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
