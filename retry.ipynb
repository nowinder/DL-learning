{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## out produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "import cmath\n",
    "import sawcom7 as sc\n",
    "import random\n",
    "\n",
    "pI = 3600E-9 #The period of IDT, normally is one wavelength\n",
    "h = 0.08*pI #The thickness of IDT, Al or Al-Cu1%\n",
    "W1 = 20*pI # Width of IDT (acoustic aperture), in m\n",
    "m_ratio = 0.6 #The metallization ratio\n",
    "epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "\n",
    "npiezo_1 = 50.3562796837374\n",
    "eta = (0.182+0.349*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "e = (0.0388+0.618*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "alpha = 0.05\n",
    "c = 1 + (0.0678+1.27*(2*h/pI))**2\n",
    "k2 = 0.0655 + 0.206*(2*h/pI)\n",
    "vb = 4226.54\n",
    "\n",
    "npiezo_1_num = [i for i in np.linspace(npiezo_1*0.8,npiezo_1*1.2,10000)]\n",
    "eta_num = [i for i in np.linspace(eta*0.8,eta*1.2,10000)]\n",
    "e_num = [i for i in np.linspace(e*0.8,e*1.2,10000)]\n",
    "alpha_num = [i for i in np.linspace(alpha*0.8,alpha*1.2,10000)]\n",
    "c_num = [i for i in np.linspace(c*0.01,c*2.0,10000)]\n",
    "k2_num = [i for i in np.linspace(k2*0.01,k2*4.0,10000)]\n",
    "vb_num = [i for i in np.linspace(vb*0.8,vb*1.2,10000)]\n",
    "\n",
    "x = np.zeros((20000,7))\n",
    "for i in range(0,20000):\n",
    "    random.seed()\n",
    "    npi = random.sample(npiezo_1_num,1)[0]\n",
    "    et = random.sample(eta_num,1)[0]\n",
    "    e = random.sample(e_num,1)[0]\n",
    "    al = random.sample(alpha_num,1)[0]\n",
    "    c = random.sample(c_num,1)[0]\n",
    "    k = random.sample(k2_num,1)[0]\n",
    "    v = random.sample(vb_num,1)[0]\n",
    "    x[i,:]=[npi,et,e,al,c,k,v]\n",
    "    \n",
    "x_max = x.max(axis=0).reshape((1,7))\n",
    "x_min = x.min(axis=0).reshape((1,7))\n",
    "file = 'D:/data/7p/out/' +'MP60' + '.csv'\n",
    "with open(file,'w',newline='') as f:\n",
    "    np.savetxt(f,x,delimiter=',',newline='\\n')\n",
    "file = file + 'maxmin.csv'\n",
    "with open(file, 'w', newline='') as f:\n",
    "    np.savetxt(f, x_max, delimiter=',',newline='\\n')\n",
    "    np.savetxt(f, x_min, delimiter=',',newline='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "import cmath\n",
    "import sawcom7 as sc\n",
    "\n",
    "pI = 3600E-9 #The period of IDT, normally is one wavelength\n",
    "h = 0.08*pI #The thickness of IDT, Al or Al-Cu1%\n",
    "W1 = 20*pI # Width of IDT (acoustic aperture), in m\n",
    "m_ratio = 0.6 #The metallization ratio\n",
    "epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "\n",
    "npiezo_1 = 50.3562796837374\n",
    "eta = (0.182+0.349*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "e = (0.0388+0.618*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "alpha = 0.05\n",
    "c = 1 + (0.0678+1.27*(2*h/pI))**2\n",
    "k2 = 0.0655 + 0.206*(2*h/pI)\n",
    "vb = 4226.54\n",
    "\n",
    "eta_b = (eta+2*abs(e))/2\n",
    "epsilon = npiezo_1*epsilon_0\n",
    "\n",
    "x1 = np.cos(np.pi*m_ratio )\n",
    "m1 = math.sqrt((1-x1)/2) \n",
    "km1 = special.ellipk(m1, out=None)\n",
    "p1 = 2*km1/np.pi\n",
    "x2 = -np.cos(np.pi*m_ratio )\n",
    "m2 = math.sqrt((1-x2)/2) \n",
    "km2 = special.ellipk(m2, out=None)\n",
    "p2 = 2*km2/np.pi\n",
    "p_factor = p1/p2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "freq = np.linspace(0.5E9, 1.5E9, 501)\n",
    "# freq_mhz = freq/1e6\n",
    "\n",
    "# eta = (0.182+0.349*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "# e = (0.0388+0.618*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "# eta_b = (eta+2*abs(e))/2\n",
    "# alpha = 10**(-3.39+7.39*(2*h/pI))\n",
    "# c = 1 + (0.0678+1.27*(2*h/pI))**2\n",
    "# k2 = 0.0655 + 0.206*(2*h/pI)\n",
    "# vb = 4226.54\n",
    "\n",
    "delta_v = - (eta**2)/2\n",
    "k = abs(e)*(eta+abs(e)/2)\n",
    "kb = -(abs(e)**2)*eta/(eta+2*abs(e))\n",
    "delta_b = -((eta**2)-2*((abs(e)**2)))/4\n",
    "omega = freq*2*np.pi\n",
    "delta = omega/vb - 2*np.pi/pI - 1j*alpha\n",
    "\n",
    "# epsilon = npiezo_1*epsilon_0\n",
    "v_delta = []\n",
    "for i in range(0,len(delta)):\n",
    "    v_delta_0 = eta_b/((cmath.sqrt(delta_b-delta[i]))+ eta_b)# wave velocity in m/s\n",
    "    v_delta.append(v_delta_0)\n",
    "v_delta = np.array(v_delta)\n",
    "omega = freq*2*np.pi\n",
    "C = (W1*epsilon*p_factor)/pI ##To check\n",
    "xi = []\n",
    "for i in range(0,len(omega)):\n",
    "    xi_0 = c*cmath.sqrt((omega[i]*C*k2)/(pI*np.pi))\n",
    "    xi_0 = -1j*xi_0\n",
    "    xi.append(xi_0)\n",
    "xi = np.array(xi)\n",
    "\n",
    "lam1 = pI # Wavelength in m of SAW filters \n",
    "#v1 = 3925 # wave velocity in m/s\n",
    "# epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "# epsilon = npiezo_1*epsilon_0 # The effective permittivity of piezoelectric layer \n",
    "c12 =  -1j*c*(k+kb*v_delta) # Reflectivity per unit length (~1.7% reflected per IDT spaced at lam/2)\n",
    "a1 = -xi # The transduction coefficient\n",
    "n1 = 100 # The number of IDT pairs\n",
    "L1 = n1*lam1 # Length of total IDT the grating, in m\n",
    "#W1 = 22*lam1 # Width of IDT (acoustic aperture), in m\n",
    "#d = sc.delta(freq,v1,lam1) - 500j\n",
    "Ct=n1*W1*epsilon # Static capacitance of total IDT\n",
    "#d1 = sc.delta(freq,v1,lam1)\n",
    "d1 = sc.thetau(c,delta,delta_v,kb,v_delta)\n",
    "C1 = sc.C0(freq,Ct)\n",
    "idt_ref_1 = sc.pmatrix(lam1,c12,a1,L1,d1,C1) #The P-Matrix of SAW resonator with refelection \n",
    "y11 = 20 * np.log10(abs(idt_ref_1.p33)/5)\n",
    "# y = np.stack((freq,y11), axis=-1)\n",
    "# return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "import cmath\n",
    "import sawcom7 as sc\n",
    "\n",
    "pI = 3600E-9 #The period of IDT, normally is one wavelength\n",
    "h = 0.08*pI #The thickness of IDT, Al or Al-Cu1%\n",
    "W1 = 20*pI # Width of IDT (acoustic aperture), in m\n",
    "m_ratio = 0.6 #The metallization ratio\n",
    "epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "\n",
    "npiezo_1 = 50.3562796837374*1.20\n",
    "eta = (0.182+0.349*(2*h/pI))*math.sqrt(2*np.pi/pI)*1.20\n",
    "e = (0.0388+0.618*(2*h/pI))*math.sqrt(2*np.pi/pI)*1.20\n",
    "alpha = 0.05*1.20\n",
    "c = (1 + (0.0678+1.27*(2*h/pI))**2)*1.20\n",
    "k2 = (0.0655 + 0.206*(2*h/pI))*1.20\n",
    "vb = 4226.54*1.20\n",
    "\n",
    "eta_b = (eta+2*abs(e))/2\n",
    "epsilon = npiezo_1*epsilon_0\n",
    "\n",
    "x1 = np.cos(np.pi*m_ratio )\n",
    "m1 = math.sqrt((1-x1)/2) \n",
    "km1 = special.ellipk(m1, out=None)\n",
    "p1 = 2*km1/np.pi\n",
    "x2 = -np.cos(np.pi*m_ratio )\n",
    "m2 = math.sqrt((1-x2)/2) \n",
    "km2 = special.ellipk(m2, out=None)\n",
    "p2 = 2*km2/np.pi\n",
    "p_factor = p1/p2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "freq = np.linspace(0.5E9, 1.5E9, 501)\n",
    "# freq_mhz = freq/1e6\n",
    "\n",
    "# eta = (0.182+0.349*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "# e = (0.0388+0.618*(2*h/pI))*math.sqrt(2*np.pi/pI)\n",
    "# eta_b = (eta+2*abs(e))/2\n",
    "# alpha = 10**(-3.39+7.39*(2*h/pI))\n",
    "# c = 1 + (0.0678+1.207*(2*h/pI))**2\n",
    "# k2 = 0.0655 + 0.206*(2*h/pI)\n",
    "# vb = 4226.54\n",
    "\n",
    "delta_v = - (eta**2)/2\n",
    "k = abs(e)*(eta+abs(e)/2)\n",
    "kb = -(abs(e)**2)*eta/(eta+2*abs(e))\n",
    "delta_b = -((eta**2)-2*((abs(e)**2)))/4\n",
    "omega = freq*2*np.pi\n",
    "delta = omega/vb - 2*np.pi/pI - 1j*alpha\n",
    "\n",
    "# epsilon = npiezo_1*epsilon_0\n",
    "v_delta = []\n",
    "for i in range(0,len(delta)):\n",
    "    v_delta_0 = eta_b/((cmath.sqrt(delta_b-delta[i]))+ eta_b)# wave velocity in m/s\n",
    "    v_delta.append(v_delta_0)\n",
    "v_delta = np.array(v_delta)\n",
    "omega = freq*2*np.pi\n",
    "C = (W1*epsilon*p_factor)/pI ##To check\n",
    "xi = []\n",
    "for i in range(0,len(omega)):\n",
    "    xi_0 = c*cmath.sqrt((omega[i]*C*k2)/(pI*np.pi))\n",
    "    xi_0 = -1j*xi_0\n",
    "    xi.append(xi_0)\n",
    "xi = np.array(xi)\n",
    "\n",
    "lam1 = pI # Wavelength in m of SAW filters \n",
    "#v1 = 3925 # wave velocity in m/s\n",
    "# epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "# epsilon = npiezo_1*epsilon_0 # The effective permittivity of piezoelectric layer \n",
    "c12 =  -1j*c*(k+kb*v_delta) # Reflectivity per unit length (~1.7% reflected per IDT spaced at lam/2)\n",
    "a1 = -xi # The transduction coefficient\n",
    "n1 = 100 # The number of IDT pairs\n",
    "L1 = n1*lam1 # Length of total IDT the grating, in m\n",
    "#W1 = 22*lam1 # Width of IDT (acoustic aperture), in m\n",
    "#d = sc.delta(freq,v1,lam1) - 500j\n",
    "Ct=n1*W1*epsilon # Static capacitance of total IDT\n",
    "#d1 = sc.delta(freq,v1,lam1)\n",
    "d1 = sc.thetau(c,delta,delta_v,kb,v_delta)\n",
    "C1 = sc.C0(freq,Ct)\n",
    "idt_ref_1 = sc.pmatrix(lam1,c12,a1,L1,d1,C1) #The P-Matrix of SAW resonator with refelection \n",
    "y11_3 = 20 * np.log10(abs(idt_ref_1.p33)/5)\n",
    "# y = np.stack((freq,y11), axis=-1)\n",
    "# return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d8dec373a0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/AAAAISCAYAAABrpCQzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAADc+0lEQVR4nOzde3xT9f0/8NfJya1pm95JWi7l5q3ijUsR1AmjFKbC1G3OORUvY19lfgXRycVNxMlt3qebt13Un9vXy3QKyErA21SQAsUh1ity0dKmV5q26SU55/z+SBOaNmmTNPe+no+HQk5Okk/bQ9t33u/P+y0oiqKAiIiIiIiIiOKaKtYLICIiIiIiIqKBMYAnIiIiIiIiSgAM4ImIiIiIiIgSAAN4IiIiIiIiogTAAJ6IiIiIiIgoATCAJyIiIiIiIkoADOCJiIiIiIiIEgADeCIiIiIiIqIEwACeiIiIiIiIKAEwgCciIiIiIiJKAEkZwP/pT3/CmDFjoNfrMWnSJLz//vuxXhIRERERERHRoCRdAP/SSy9hyZIluOuuu7Bv3z5ccMEF+MEPfoCjR4/GemlEREREREREIRMURVFivYhwmjp1KiZOnIgnnnjCc+y0007DpZdeinXr1sVwZUREREREREShU8d6AeHU1dWFvXv3Yvny5V7HS0tLsWPHDp+P6ezsRGdnp+e2LMtobGxETk4OBEGI6HqJiIiIiIiIFEVBS0sLCgoKoFL5L5RPqgC+vr4ekiTBZDJ5HTeZTKipqfH5mHXr1mH16tXRWB4RERERERGRX99++y1GjBjh9/6kCuDdemfOFUXxm01fsWIFli5d6rnd3NyMUaNG4dChQ0hPT4/oOil+OBwOvPPOO5g5cyY0Gk2sl0PkE69Tine8Rine8RqleMdrdOhqaWnBmDFjBoxBkyqAz83NhSiKfbLttbW1fbLybjqdDjqdrs/x7OxsGI3GiKyT4o/D4YDBYEBOTg6/WVLc4nVK8Y7XKMU7XqMU73iNDl3ur/dA27iTqgu9VqvFpEmTsG3bNq/j27Ztw/Tp02O0KiIiIiIiIqLBS6oMPAAsXboU11xzDSZPnoxp06bh6aefxtGjR3HTTTfFemlEREREREREIUu6AP6nP/0pGhoacO+996K6uhoTJkzAli1bUFhYGOulEREREREREYUs6QJ4AFi0aBEWLVoU62UQERERERHRECBJEhwOh9/7RVGEWq0e9KjypAzgiYiIiIiIiKKhtbUV3333HRRF6fc8g8GA/Px8aLXakF+LATwRERERERFRCCRJwnfffQeDwYC8vDyfGXZFUdDV1YW6ujocOnQIJ510ElSq0PrJM4AnIiIiIiIiCoHD4YCiKMjLy0NKSorf81JSUqDRaHDkyBF0dXVBr9eH9HpJNUaOiIiIiIiIKNoC2dseatbd6zkG/QxEREREREREFHEM4ImIiIiIiIgSAAN4IiIiIiIiogTAAJ6IiIiIiIgoATCAJyIiIiIiIhqEgWbAB3rOQDhGjoiIiIgowSmSBPuevXDW1UGdlwfD5EkQRDHWyyJKemL3v7Ourq5+x8gBgN1uBwBoNJqQX48BPBERERFRArNZLLCuXQdnTY3nmNpshmnlChhLS2O4MqLkp1arYTAYUFdXB41G43NUnKIosNvtqK2tRWZmpifoD+n1BrNYIiIiIiKKHZvFgqrFS4BepblOq9V1/NFHGMQTRZAgCMjPz8ehQ4dw5MiRfs/NzMyE2Wwe1OsxgCciIiIiSkCKJMG6dl2f4N11pwIIAqxr1yF91iyW0xNFkFarxUknnYSuri6/52g0mkFl3t0YwBMRERERJSD7nr1eZfN9KAqcNTWw79mL1KnF0VsY0RCkUqmg1+sj/zoRfwUiIiIiIgo7Z11dWM8jovjHAJ6IiIiIKAGp8/LCeh4RxT8G8ERERERECcgweRLUZjMgCL5PEASozWYYJk+K7sKIKGIYwBMRERERJSBBFGFaucLPna6g3rRyBRvYESURBvBERERERAnKWFqK4Y8+AlVamtdxtcmE4RwhR5R02IWeiIiIiCiBGUtL0X7gABqffgaZP/85jKWlMEyexMw7URJiAE9ERERElOCU1jYAgOHMMzgyjiiJsYSeiIiIiCjBSS0tAADF6YzxSogokhjAExERERElONlmAwAoTinGKyGiSGIAT0RERESU4E5k4B0xXgkRRRIDeCIiIiKiBCe3uDLwYAk9UVJjAE9ERERElOAkmzsDzxJ6omTGAJ6IiIiIKMGxiR3R0MAAnoiIiIgogSkOBxS73fV37oEnSmoM4ImIiIiIEpjU2ur5OzPwRMmNATwRERERUQJzj5ADAHAPPFFSYwBPRERERJTA3A3sAGbgiZIdA3giIiIiogTmHiEnGAwM4ImSHAN4IiIiIqIE5s7Aq7Oy2MSOKMkxgCciIiIiSmBSiw0QBIgZGdwDT5TkGMATERERESUw2dYCVVoaBK2WJfRESY4BPBERERFRApNabBDT0yGo1QzgiZIcA3giIiIiogSlSBI6v/oaCgDJ3sY98ERJTh3rBRARERERUfBsFgusa9fBWVMDAHAeO4aug9/AZrHAWFoa49URUSQwA09ERERElGBsFguqFi/xBO9uSmcnqhYvgc1iidHKiCiSGMATERERESUQRZJgXbsOUBS/51jXroMisSM9UbJhAE9ERERElEDse/b2ybx7URQ4a2pg37M3eosioqhgAE9ERERElECcdXVhPY+IEgcDeCIiIiKiBKLOywvreUSUOBjAExERERElEMPkSVCbzYAg+D5BEKA2m2GYPCm6CyOiiGMAT0RERESUQARRhGnliu4bvoN408oVEEQxiqsiomhgAE9ERERElGCMpaUY/ugjUJtM3neo1Rj+6COcA0+UpNSxXgAREREREQXPWFqK9Fmz8PnZ58B40UVQOjrQ8cUXDN6Jkhgz8ERERERECUoQRUBRYDj7LGhHjgBkOdZLIqIISpgAfs2aNZg+fToMBgMyMzN9nnP06FHMmzcPqampyM3Nxa233oqurq7oLpSIiIiIKEoURQGcTggaDaBWQ3E6Yr0kIoqghCmh7+rqwk9+8hNMmzYNf/nLX/rcL0kSLr74YuTl5eGDDz5AQ0MDFixYAEVR8Nhjj8VgxUREREREEeboDtjVaghqDeBwxnY9RBRRCRPAr169GgDw7LPP+rzfYrGgsrIS3377LQoKCgAADz74IK677jqsWbMGRqMxWkslIiIiIooKxekK2AW1BoJaDUWSYrwiIoqkhAngB7Jz505MmDDBE7wDwJw5c9DZ2Ym9e/di5syZPh/X2dmJzs5Oz22bzQYAcDgccDhYgjRUuL/W/JpTPON1SvGO1yjFu2S8RqX2dgCALLj+U/g7bEJLxmuUAhPo1zxpAviamhqYeo3RyMrKglarRU1Njd/HrVu3zpPd78liscBgMIR9nRTftm3bFuslEA2I1ynFO16jFO+S6RoVW1sxDsDe/fuhaWhAbmcntmzZEutl0SAl0zVKgbHb7QGdF9MA/p577vEZPPe0e/duTJ48OaDnEwShzzFFUXwed1uxYgWWLl3quW2z2TBy5EiUlpay7H4IcTgc2LZtG2bPng2NRhPr5RD5xOuU4h2vUYp3yXiNOq1WHP7dfZg8dSoc336H+q0WXHTRRbFeFoUoGa9RCoy7EnwgMQ3gb7nlFlx55ZX9njN69OiAnstsNmPXrl1ex5qamuBwOPpk5nvS6XTQ6XR9jms0Gv6jGYL4dadEwOuU4h2vUYp3yXSNKnAlqjR6PRSdFpCkpPnYhrJkukYpMIF+vWMawOfm5iI3NzcszzVt2jSsWbMG1dXVyM/PB+Aqg9fpdJg0aVJYXoOIiIiIKK44e3ahVwOyDEWWIagSZlo0EQUhYfbAHz16FI2NjTh69CgkScLHH38MABg/fjzS0tJQWlqKoqIiXHPNNbj//vvR2NiIO+64AwsXLmQpPBERERElJaW78ZV7Djzg6kwvaLWxXBYRRUjCBPB33303nnvuOc/tc845BwDwzjvvYMaMGRBFEW+++SYWLVqE8847DykpKbjqqqvwwAMPxGrJREREREQR5T1GrrsE1+kEGMATJaWECeCfffZZvzPg3UaNGoXNmzdHZ0FERERERDHmCeA1aghq0esYESUfbo4hIiIiIkpQisOdge/eAw8G8ETJjAE8EREREVGC8rkH3sEAnihZMYAnIiIiIkpQSncXelcGvnsPvMQAnihZMYAnIiIiIkpU7nJ5tYZ74ImGAAbwREREREQJyruJHffAEyU7BvBERERERAmq5x54gXvgiZIeA3giIiIiogTVsws9xO4J0dwDT5S0GMATERERESUoTwm9Wg1BwxJ6omTHAJ6IiIiIKEG5u9CDc+CJhgR1rBdARERERHFEloAjO4BWK5BmAgqnAyox1qsiPxSHA9BoIAgCA3iiIYABPBERERG5VG4EypYBtmMnjhkLgLkbgKL5sVsX+ed0egJ3uP9kAE+UtFhCT0RERESu4P3la72DdwCwVbuOV26MzbqoX4rjRADPDDxR8mMAT0RERDTUyZIr8w7Fx53dx8qWu86juKI4GcATDSUM4ImIiIiGuiM7+mbevSiArcp1HsUVxeGAoNG4boiuXgUM4ImSFwN4IiIioqGu1Rre8yhqFKfjRAbeHcgzgCdKWgzgiYiIiIa6NFN4z6PocToBDUvoiYYKBvBEREREQ13hdFe3eQh+ThAA43DXeRRXXE3sXJn3EwE8exUQJSsG8ERERERDnUp0jYrzqTuon7ue8+DjkO898I4YroiIIokBPBERERG55rxf8TxgyPU+bixwHecc+Ljk1YVeEAC1miX0RElMHesFEBEREVGcKJoPqPXAP34CjPs+cP5SV9k8M+9xq2cAD3SX0TOAJ0paDOCJiIiI6ASpy/VnegEw5oLYroUG1LMLPQAIosg98ERJjCX0RERERHSCo737z7bYroMCojgcELQaz22BJfRESY0BPBERERGd4OwO4LvssV0HBcbpBHpk4KHRsIkdURJjAE9EREREJ7gz8F3MwCeCnmPkAPceeJbQEyUr7oEnIqIhQZIlVNRWoM5ehzxDHiYOmwiRjbmI+nJ0Z95ZQp8QFKcTKoPBc9u1B54l9ETJigE8ERElve1HtmN9+XpY7VbPMZPBhOXFy1FSWBLDlRHFIQdL6BOJ1xx4cA88UbJjCT0RESW17Ue2Y+m7S72CdwCotddi6btLsf3I9hitjChOeZrYMYBPBL3HyEGj5h54oiTGAJ6IiJKWJEtYX74eCpQ+97mPbSjfAEnmflEiD08GvjW266CAKE4HoOk5Ro574ImSGQN4IiJKWhW1FX0y7z0pUFBjr0FFbUUUV0UU51hCn1gc3hl4ltATJTcG8ERElLTq7HVhPY9oSHCXzkudAKtT4p5rD7zWc5sBPFFyYwBPRERJK8+QF9bziIYEZ8eJv3OUXNzztQceEgN4omTFAJ6IiJLWxGETYTKYIEDweb8AAWaDGROHTYzyyojimMMOaFJP/J3iWu8AXhDVUBwM4ImSFQN4IiJKWqJKxPLi5T7vcwf1y4qXcR48UU+OdiA1x/V3ZuDjnuJ0QtBwDzzRUMEAnoiIklpJYQkemvEQhhmGeR03GUx4aMZDnANP1JvDDhhyXX9nAB/3OAeeaGhRD3wKERFRYispLMEU0xSc/9L5mDVqFn5+2s8xcdhEZt6JfHF0ANn53X9nCX3ccziAnnvg1ZwDT5TMmIEnIqKhoXsb/NiMsZhinsLgncgfRzsz8AnEtQfeOwMP7oEnSloM4ImIaEhwyq5faJ0Kf7El6pfDzj3wCaRPEzuW0BMlNQbwREQ0JEiKa561xLnWRP3rmYFnCX3c4x54oqGFATwREQ0J7sBdVuQYr4QojikK4GwHUjIBQcUMfJxTZBmQZa8u9FCLUDgHnihpMYAnIqIhwV067y6lJyIfnB2uPzUGQJvGDHycc2favUvoNdwDT5TEGMATEdGQ4M7Au0vpicgHR7vrT02KK4jvYgAf1xzd3ea5B55oyGAAT0REQ4K7dJ4BPMUDSVaw82AD3vi4CjsPNkCSlVgvycWdcdekAFoD0NUa2/VQv5TuAN69B16RJDjr6+A8fhxtu8qhSPx+R5RsOAeeiIiGBJbQU7woO1CN1ZsqUd3c4TmWn6HHqnlFmDshP4Yrg2sGPACoUwBNKkvo49yJEnoNbBYLrGvXwVlTAwA4umAB1GYzTCtXwFhaGstlElEYMQNPRERDAkvoKR6UHajGzS9UeAXvAFDT3IGbX6hA2YHqGK2sm1cGPpUl9HHOHcC3f7IfVYuXeIJ3N6fViqrFS2CzWGKxPCKKAAbwREQ0JHCMHMWaJCtYvakSvorl3cdWb6qMbTm9Zw+8wVVC72AX+njmDuCPv/iSa4JAnxNcx6xr17GcnihJMIAnIqIhwV06zww8xUr5ocY+mfeeFADVzR0oP9QYvUX11jMDrzFwjFycU7pce+ClpqZ+TlLgrKmBfc/eKK2KiCKJATwREQ0J7sCde+ApVmpb/AfvoZwXEZ4xciyhTwSK0xHwuc66ugiuhIiiJSEC+MOHD+PGG2/EmDFjkJKSgnHjxmHVqlXo6uryOu/o0aOYN28eUlNTkZubi1tvvbXPOURENDSxCz3F2rB0fVjPiwh3Bl7UAh02wPYdcOh9gFtP4lMQ4+LUeXkRXAgRRUtCdKH//PPPIcsynnrqKYwfPx4HDhzAwoUL0dbWhgceeAAAIEkSLr74YuTl5eGDDz5AQ0MDFixYAEVR8Nhjj8X4IyAioljzlNAzEKEYKR6TjfwMPWqaO3zugweATIMGxWOyo7ouL+498H+aBrQcc/39uUsAYwEwdwNQND92a6M+3HvgxZwcSI2NvvfBCwLUJhMMkydFeXVEFAkJkYGfO3cu/va3v6G0tBRjx47F/Pnzcccdd+C1117znGOxWFBZWYkXXngB55xzDkpKSvDggw/imWeegc1mi+HqiYgoHnhK6BWW0FNsiCoBq+YV+Q3eAeC43YFtlTX9nBFh35a7/nQH7262auDla4HKjdFfE/nlngOfe9P/uA4IgvcJ3bdNK1dAEMVoLo2IIiQhMvC+NDc3Izv7xDvUO3fuxIQJE1BQUOA5NmfOHHR2dmLv3r2YOXOmz+fp7OxEZ2en57Y72Hc4HHA4At9XRInN/bXm15ziGa/Twel0uL7XOyUnP4cRwmt0YDNOykFmigbH231/jgQAqzd9ihkn5UBUCT7PiRhZgvrTf3nW4U2BAgEoWw7nuFJAlZjBYLJdo44OV88C/XnnwfzQg6hbvwGS1eq5X20yIXfZnUiZOTNpPuZkl2zXKAUu0K95QgbwBw8exGOPPYYHH3zQc6ympgYmk8nrvKysLGi1WtTU+H8ne926dVi9enWf4xaLBQaDIXyLpoSwbdu2WC+BaEC8TkNT2VUJAKhrqMOWLVtivJrkxmvUv6+aBRxv9x/8ujrRd+Lxl8pwUkZ0x8nltHyG8zv9Vy0KUABbFXa98gga0k+L4srCL1muUcOXX2IEgHf+8z6cWZnAksUwlpfD/K/XUXP5ZbBNmQJ0dQH8npdwkuUapcDZ7YE1DY1pAH/PPff4DJ572r17NyZPnuy5fezYMcydOxc/+clP8Itf/MLrXKF32RAARVF8HndbsWIFli5d6rlts9kwcuRIlJaWwmg0BvqhUIJzOBzYtm0bZs+eDY1GE+vlEPnE63RwtEe1+McH/0BGZgYumnNRrJeTlHiNDmzT/mqg8pMBzxt7+tm46Mz8KKzoBOHTduDrgc87d8JoKKcn5r+hZLtG29LSUP2Xv+L7pbM9Teq6JkzA0X+9jkmXzEPKlMkDPAPFm2S7RilwgW77jmkAf8stt+DKK6/s95zRo0d7/n7s2DHMnDkT06ZNw9NPP+11ntlsxq5du7yONTU1weFw9MnM96TT6aDT6foc12g0/EczBPHrTomA12mIuru+yJD5+YswXqP+5WemBnxe1D+HxsDeMFBnDAcS/OubLNeoSnZN19CkpEDd/fEImZmuPzs6kuJjHKqS5RqlwAX69Y5pAJ+bm4vc3NyAzq2qqsLMmTMxadIk/O1vf4NK5d1/b9q0aVizZg2qq6uRn+/6AWSxWKDT6TBpErtuEhENde4mdhwjR7Hk7kRf3ex71rsAwJyhj34n+sqNwL/vHOAkwdWNvnB6VJZEAejuQi/0+MVflep6k0hua4vJkogoshKiC/2xY8cwY8YMjBw5Eg888ADq6upQU1Pjtbe9tLQURUVFuOaaa7Bv3z689dZbuOOOO7Bw4UKWwhMRkWd8nHucHFEsuDvR++Le8LdqXlF0G9hVbnR1mG+p7uek7vXMXZ+wDeySkXuMnKA+kZMTUlIAlYoBPFGSSogmdhaLBV9//TW+/vprjBgxwus+pXvepSiKePPNN7Fo0SKcd955SElJwVVXXeWZE09EREMbM/AUL+ZOyMf543Ow61AjHNKJRnXmDD1WzSvC3AlR3PsuS0DZMqDf4XbongO/nnPg44zi8BHACwJUqamQ21pjtSwiiqCECOCvu+46XHfddQOeN2rUKGzevDnyCyIiooTjzry7M/FEsZSRokXx6GxcNbUQv/pHBZbNPQW//N646I+OO7IDsB0b+Lwf/gkYNyPiy6HgKA6Ha9Z7rxnvqrQ0ZuCJklRClNATERENFjPwFE9sHQ5kGrSYNi4HADAmNy36wTsAtFoHPgcA7PWRXQeFRHE6IKjVfSYuqVINkFqZgSdKRgzgiYhoSOAeeIonze0OGFPU0Kldv4p1SXJsFpLmf1JPSOdR1CiShK6DB6EIAtp2lUORTrw5KaYyA0+UrBjAExHRkMAMPMUTW7sDRr3GE8B3OmJ0XRZOd+1vRz/Zf0MOO8/HGZvFgq9nlaDpH/8HdHXh6IIF+HpWCWwWCwBXJ3q5lQE8UTJiAE9EREOCJ4DnHniKA7YOJ4wpGqhFFUSVgE5njDLwKhGYu6H7Ru8gvvv2WVey83wcsVksqFq8BM4e05gAwGm1omrxEtgsFu6BJ0piDOCJiGhI8JTQKyyhp9hSFKW7hN41u1srqmIXwAOuzvJXPA8Ye3W/Nxa4/hx2evTXRD4pkgTr2nWA4mNqQPcx69p1EAwGyNwDT5SUGMATEdGQ4A7cmYGnWLN3SZBkBUa9axiQTqNCVywDeMAVxC85AKj1wJk/BRZsBpZ8AqjUgLM9tmsjD/uevX0y714UBc6aGshtbczAEyWphBgjR0RENFjuwJ174CnWbB0OAPBk4HVqFTqdcXBdCipA6gJGnQuMucB1TJ0CODpiuy7ycNbVBXaiLEPiHHiipMQMPBERDQlsYkfxwtbuqgbJ8ATwYmxL6N2kLkCRAY3hxDGNHnAwAx8v1Hl5gZ2XkwO5zR7h1RBRLDCAJyKiIcGTgWcJPcVYc3t3Bl7fIwPviIMA3h2oq/UnjqlTWEIfRwyTJ0FtNgOCn6kBggC12QzdqadAbm2F4muvPBElNAbwREQ0JLj3wCtQICtxECzRkGVzB/Aprp2MWrUKXVIcvLHkDuD7ZOBZQh8vBFGEaeUKP3e6gnrTyhUQ09MBSYLS2RnF1RFRNDCAJyKiIaFn0M4sPMWKJCuoONoEAPis2gZJVuIoA99dcq1JOXFMrWcGPs4YS0sx/NFHoMrM9DquNpkw/NFHYCwthSo1DQDYiZ4oCbGJHRERDQlO+cT4OKfihAaaGK6GhqKyA9VYvakS1c2ujPaCv+5GfoYe6Xp1fOyB95mBNzADH2cUSYKYkQnD5Mlo+/BDmFffA43JDMPkSRBEEQCgSk0FAFcn+tzcWC6XiMKMATwREQ0JPZvXMQNP0VZ2oBo3v1CB3juSa5o7UN0MGLRiTNblxdkdqGt67IHXMAMfT2wWC6xr13mNkqt76GGYVq7wBO8AoEpzBfASM/BESYcl9ERENCT0DNrZiZ6iSZIVrN5U2Sd4B+A59ll1CyQ5xg3HfJbQc4xcvLBZLKhavKTPHHin1YqqxUtgs1g8x8SeGXgiSioM4ImIaEjoGbT3LKcnirTyQ42esnl/Op0yyg81RmlFfvhrYscMfMwpkgTr2nWAr67y3cesa9dB6W6GqEpz74FnAE+UbBjAExHRkNAzaGcGnqKptiWwDHag50UMM/Bxy75nb5/MuxdFgbOmBvY9ewH02gNPREmFATwREQ0J7EJPsTIsXT/wSUGcFzHuQF3dI4DX6E8E9hQzzrq64M7TaACVCm0f7UTbrnJPZp6IEh8DeCIiGhIkRYJG5eo8754JTxQNxWOykZ+hh9DPOWqVgOIx2VFbk08OOyCIgNhjQoM65URzO4oZdV5ewOfZLBYcLJkNyDKaX30NRxcswNezSrz2yBNR4mIAT0REQ4JTdkIn6gAwA0/RJaoErJpXBAB9gnj37SyDBqKqvxA/Chztrv3vQo91aPQsoY8DhsmToDabvb82PQkC1GYzpKbGgBvdEVFiYgBPRERDgqRI0Ipaz9+JomnuhHw8cfVEmDO8y+TNGXpcfEY+RFUc/ErmaPfe/w50Z+DZxC7WBFGEaeWK7hu9gvju28OWL4N1/YaAG90RUWKKg58WREREkSfJDOAptuZOyMcHy76PvHQtZp82DP+38Fx8sOz7OH24EZ3OOLgmne3eM+ABZuDjiLG0FMMffQRqk8nruNpkch3Pyg6q0R0RJSZ1rBdAREQUDU6FJfQUe6JKgKIIOHNEJqaNywEAaEUVupzyAI+MAncJfU8aZuDjibG0FOmzZuHwz6+G0tkJ0/LlMEyeBEEU0bz5zYCeI9CGeEQUnxjAExHRkMAMPMWLDoeEFK3oua3TiOiMiwDe7ruEXnYCkhMQ+WtjPBBEEQIA3amnInVqsed4MI3uiChxsYSeiIiGBFmRoVO5MvA9Z8ITRZOiKLB3Ob0DeLUKTlmBU4pxEO/o8JGB7y6pZxY+rkhNTRCzsryOBdrozjB5UhRWSESRwgCeiIiGBKfiZAaeYq5LkiErQIrGO4B33xdTDjug7rUH3j0Tnvvg44rz+HGIWZlexwJpdGdauQKCKIKIEhcDeCIiGhK8Sui5B55ipKPLFaR7B/Cuv8d8H7yvLvTMwMcdxemE3NwMda8MPDBwoztjaWm0lklEEcIAnoiIhoSeY+ScCkvoKTbaHa43j/Tavhn4mO+D99XEzpOBZwAfL6TmZgDoU0LvZiwtxfi3tiNv6W0AgIJHHsb4t7YzeCdKEgzgiYhoSHDK7EJPsWfvcr15ZPBRQt/piHEA7+wnA88APm5Ix48D8B/AA65y+tRp0wAAusJCls0TJREG8ERENCT0zMBzDzzFijsD792F3p2Bj/F16auE3p2Bd3IPfLyQmpoAAGJmZr/niVnZAABnY2Okl0REUcQAnoiIhgRJlpiBp5jrcAfwPvbAx76E3scYOWbg447THcD3k4EHAHV3kzupsSnSSyKiKGIAT0REQ4KkSNCquAeeYqu9u4mdvkcAr02EPfDMwMcNqakJEASIRmO/5wkGAwSdzpOxJ6LkwACeiIiGBElhBp5iz7MH3mcTu1iX0Hf4yMCziV28kZqOQ8zIGHBfuyAIELOz4WxiCT1RMmEAT0REQ4LXGDnugacY8bkHPp5K6HvPgdcwAx9PFElCR2UloNGgbVc5FKn/72XqrCyW0BMlGQbwREQ0JLCJHcUD9x54vbpvBj6mc+AlByA7+pbQixpAEJmBjzFFklD3xz/hy+nnoWXrVkh1dTi6YAG+nlUCm8Xi93FidjYkNrEjSiohBfAOhwPffvstvvjiCzTymwIRESUAp+yERqUBwBJ6ip32Lgk6tQoqleA5Fhd74N0Beu8SevcxZuBjxmax4Mvzzkf9Y49B7p4B7+a0WlG1eInfIF7MzmIJPVGSCTiAb21txVNPPYUZM2YgIyMDo0ePRlFREfLy8lBYWIiFCxdi9+7dkVwrERFRyCRFglqlhlpQMwNPMWN3SF7l80DPOfAxvC7dAXrvDDzgKqt32KO7HgLgCt6rbl0MuXv2ex+KAgCwrl3ns5yeJfREySegAP7hhx/G6NGj8cwzz+D73/8+XnvtNXz88cf44osvsHPnTqxatQpOpxOzZ8/G3Llz8dVXX0V63UREREGRFRlqlRqiSoRTZhd6io2OLgkGjXcArxZVEFVCjDPw3QG6Rt/3Pk2Kq8EdRZUiSbCuXRfAiQqcNTWw79nb5y5VRiacViuaN78Z0J55Iop/6kBO2rFjB9555x2cccYZPu8vLi7GDTfcgCeffBJ/+ctf8N577+Gkk04K60KJiIgGwyk7IQoiREFkBp5ipt0hQa/t2z1cp1bFSQl9rwy8LAGKDNTsBw69DxROB1T9dz+n8LDv2QtnTU3A5zvr6rxu2ywWND77LOS2Nhy74w4AgNpshmnlChhLS8O6ViKKnoAC+FdeeSWgJ9PpdFi0aNGgFkRERBQJkiJBJaggqkTugaeYaXdISNH0DYC1alVsm9h5MvA99sBXbgTKlgG2Y4CtCvjKAhgLgLkbgKL5sVnnENI7IB+IOi/P83ebxYKqxUs8Jfae5+zeM49HH2EQT5SgQu5C//XXX2Pr1q1ob3e9Y6v0+gZBREQUL2RF9pTQqwU1nApL6Ck22rtknwG8KwMfwzeWHL32wFduBF6+1hW892Srdh2v3Bjd9Q1BPQPyAc81m2GYPAlAj9J7X7+bD7BnnojiX9ABfENDA0pKSnDyySfjoosuQnV1NQDgF7/4BW6//fawL5CIiGiw3CXzoiBCJaiYgaeYaXc4+zSxA1yz4GNWQi9LwHfdjYir9wPOLlfmHb6SM93Hypa7HkcRY5g8CWqzGRCE/k8UBJhWroAguq6rAUvv+9kzT0TxL+gA/rbbboNarcbRo0dhMJzYJ/XTn/4UZWVlYV0cERFROLgDdlElukrouQeeYqS9y3cJvU6tQqcjBgF85UbgkQnA9lWu2/+8Dnjo1L6Zdy+Kq6T+yI5orHDIEkQRppUr+j1HzMzE8F7l8IGW3gdbok9E8SGgPfA9WSwWbN26FSNGjPA6ftJJJ+HIkSNhWxgREVG4uAN2tdBdQs8u9BQj7Q4JxhRNn+NatQpd0S5pdpfJ98602xsCe3yrNexLIm/G0lLg0UdQs3o1pIYT89xVGRnIvvYa5N50kyfz7hZo6X0wJfpEFD+CDuDb2tq8Mu9u9fX10Ol0YVkUERFROHlK6JmBpxhrd/SzBz6aGXhZ6qdMPkBpprAth/wzlpZCcThw7PY7YL73XmgLC2GYPKlP4O7mLr13Wq2+98ELAtQmk2fPPBEllqBL6L/3ve/h+eef99wWBAGyLOP+++/HzJkzw7o4IiKicPCU0LvHyHHvLsVIe1ec7IE/smOAMvn+CIBxuGukHEWFs6YGqrQ0ZP7kx0idWuw3eAd6ld733j/ffbvnnnkiSixBZ+Dvv/9+zJgxA3v27EFXVxfuvPNOfPrpp2hsbMSHH34YiTUSERENSs8mdmqVmhl4ihlfY+QkWYHd4cThBgd2HmxA8ZhsiKoBGpcNVsjl793rmrue8+CjyFFVBU1BAYSBGtp1c5feW9eu82popzaZOAeeKMEFnYEvKirC/v37UVxcjNmzZ6OtrQ2XX3459u3bh3HjxkVijURERIPi3vMuqlwZeO6Bp1jpPUau7EA1zt/wNv77bTP2f2fDz575COdveBtlB6oju5CGg4GdZ8j1vm0sAK54nnPgo0SRJLTtKod938cQDIagRr8ZS0sx/q3tMF58EcTsbIx67jnXbQbvRAkt6Aw8AJjNZqxevTrcayEiIoqInhl4USVCVmI0rouGvA6H5CmhLztQjZtfqOizC72muQM3v1CBJ66eiLkT8sO/CFkC9v5t4POMw4FbPwbefwB4bwPws5eAk2Yz8x4lNoulTwb961klQWXQBVFEytnnoMWyDYbiKQFn8IkofgUUwO/fvz/gJzzzzDNDXkx/5s+fj48//hi1tbXIyspCSUkJNmzYgIKCAs85R48exa9+9Su8/fbbSElJwVVXXYUHHngAWq02ImsiIqLE4N7zrla5utCzhJ5iQVEU2Lv3wEuygtWbKv1OWhcArN5UidlF5vCX0x/ZAbQEkOGfuABQa4FR57puDzuNwXuU2CwWVC1e0qcJndNqdR3vNTquP2qzCYrDAampCers7PAvloiiKqAA/uyzz4YgCFB8dbLsQRAESBEagTJz5kysXLkS+fn5qKqqwh133IEf//jH2LHDNYNUkiRcfPHFyMvLwwcffICGhgYsWLAAiqLgsccei8iaiIgoMTiV7hL67gw8S+gpFrokGbICpGhElB9qRHVzh99zFQDVzR0oP9SIaeNywruQQPe/53RvjdRnuP7saA7vOsgnRZJgXbvOdwd5RQEEAda165A+a1ZAjeg0Jte0AKfVygCeKAkEFMAfOnQo0usY0G233eb5e2FhIZYvX45LL70UDocDGo0GFosFlZWV+Pbbbz1Z+QcffBDXXXcd1qxZA6PRGKulExFRjMmyq2TevQeeGXiKhY4u13WYohFR2+I/eO8p0POCEuj4N/d5+kzXnwzgo8K+Z69X2XwfigJnTQ3se/YidWrxgM+n7g7gHTU10J92WriWSUQxElAAX1hYGOl1BKWxsRF///vfMX36dGg0GgDAzp07MWHCBK+S+jlz5qCzsxN79+71O+Kus7MTnZ2dnts2mw0A4HA44HA4IvhRUDxxf635Nad4xus0dJ0O1/d5RVKgggoOid/jI4HXaP9s7a5gXKNSkK4LrA1RjkEd/s9nwRSo0wuAlmoIPor4FQiAsQDOgimAwwGoU6EB4GxtgJLgX9tEuEY7awJrYNhZUw1tAB+HkpEBiCI6j1VDH8cfN7kkwjVKkRHo1zykJnYAUFlZiaNHj6Krq8vr+Pz5ketKumzZMjz++OOw2+0499xzsXnzZs99NTU1MJm831HOysqCVqtFTT/vYq5bt85nQz6LxQKDwRC+xVNC2LZtW6yXQDQgXqfBq3JWAQB2frgTTR1NaBPasGXLlhivKnnxGu1LVoC9dQIAEW/+Zw/ONyvI1Io43gV4RrN5UZCpBeoqP8KWz8K/nvzcH2FKy2Oe/fYnXtX1/905l6O6bCsAQFAkzAfwye73cfSb5GiCFs/XaMrBgxgZwHl7Dh5EeyDfx2QZY3U6fPPaqzhurUH7mDGAKuhBVBRl8XyNUmTY7faAzhOUgTa29/LNN9/gsssuwyeffOK1L97d1TKYPfD33HPPgN3sd+/ejcmTJwMA6uvr0djYiCNHjmD16tXIyMjA5s2bIQgCfvnLX+LIkSPYunWr1+O1Wi2ef/55XHnllT6f31cGfuTIkaivr2fZ/RDicDiwbds2zJ4921PVQRRveJ2G7pP6T7DAsgAvXfQSHvv4MWhUGjz4vQdjvaykw2vUt62fWnHfls9RYzvx+4bZqMMlZ5jxlw+PAIBXHtwdIj925VmYc3qA5e4hED7fDHHTryB0tXmOKcbhkGavgXLqJV7nqu8vhPy95ZCn3hyx9URDIlyjiiTh8Jy5kGprfe+DFwSoTSYUlv17wD3wrdu3o279BkjWE30PRJMJecuXIa2kJNxLpzBIhGuUIsNmsyE3NxfNzc39xqFBZ+AXL16MMWPGYPv27Rg7dizKy8vR0NCA22+/HQ888EBQz3XLLbf4DazdRo8e7fl7bm4ucnNzcfLJJ+O0007DyJEj8dFHH2HatGkwm83YtWuX12ObmprgcDj6ZOZ70ul00Ol0fY5rNBr+oxmC+HWnRMDrNHiC6AqJ9Bo9NKIGsiLzcxhBvEZPKDtQjf998b99CtWttk785cMj+OX3xmDjf6u9GtqZM/RYNa8oMiPkejrjMuDzTUD9F8D5twFpJgiF06H21WlenwmxqwViknxd4/oa1Whgvmulq9u8IHgH8d0JM9PKFdDq9f0+jc1iQc3S2/u8CSDV1qJm6e0YHkQne4q+uL5GKSIC/XoHHcDv3LkTb7/9NvLy8qBSqaBSqXD++edj3bp1uPXWW7Fv376An8sdkIfCnfl3Z8+nTZuGNWvWoLq6Gvn5rh94FosFOp0OkyZNCuk1iIgoObi7zrub2HXJXQM8gmjwAhkVt/G/1Xjv1zPx3I7DWLPlMzz607NxyVkF4R8d50+rFcg7FTjjx/2fp89kE7soUSQJYkYmsq69FraNGyE1NXnuU5tMAc2BD3cneyKKH0EH8JIkIS0tDYArAD927BhOOeUUFBYW4osvvgj7AgGgvLwc5eXlOP/885GVlYVvvvkGd999N8aNG4dp06YBAEpLS1FUVIRrrrkG999/PxobG3HHHXdg4cKFLIUnIhri3F3n3WPkJAe70FPkBToqbu+RJkwd6xrvNW5YWvSCdwBorQEKzh74PH0GA/gosFkssK5d16cLfeqMGci5/noYJk8KKOAOdyd7IoofQXewmDBhAvbv3w8AmDp1Kn7/+9/jww8/xL333ouxY8eGfYEAkJKSgtdeew2zZs3CKaecghtuuAETJkzAe++95yl/F0URb775JvR6Pc477zxcccUVuPTSS4Mu6yciouTjGSMncIwcRU8wo+LS9a7SyZYOZySX1FeLNbCxcvoMoON4xJczlNksFlQtXuIz8G579104mxoDzpY76+rCeh4RxY+gM/C/+c1v0NbmanZy33334ZJLLsEFF1yAnJwcvPTSS2FfIACcccYZePvttwc8b9SoUV6d6YmIiADAqZwooVer1JBkBvAUecPS+9+j3PO8dL3rV7KWjiiOjupsBbpagHTzwOemZAJNhyO9oiGr35L3bseW3g4BgHHu3AGfT52XF9DrBnoeEcWPoAP4OXPmeP4+duxYVFZWorGxEVlZWZ5O9ERERPHEHbC7M/DugJ4okorHZCM/Q4+a5g6f++AFuBrWFY/JhrO7SiSqGfjW7s7kgQTwLKGPqAFL3gFAllG15DbgD6oB98AbJk+C2myG02rtt5O9YTL7RBElmqBL6Jubm9HY2Oh1LDs7G01NTbDZbGFbGBERUbi4S+bVKjVElegpqSeKJFElYNW8IgB9J727b6+aVwRRJUCnFqFVq6KbgW/pDhjTAgngM4H245FczZAWTCm7de06KAOMbRZEEaaVK7pv9Lr6enSyZwM7osQTdAB/5ZVX4sUXX+xz/OWXXx5wJBwREVEseErouQeeomzuhHw8cfVEmDO8y+nNGXo8cfVEr1FxRr06ehl4WQK+edf198ZvXLf7wwx8RAVTyu5uPjcQY2kphj/6CNS9ximrTSaOkCNKYEGX0O/atQsPPfRQn+MzZszAXXfdFZZFERERhZOnhL57DzxL6Cma5k7Ix+wiM+Y//gG0ogp3zj0VxWOy+3SbT9dr0NIZhWuzciNQtgywHXPd/r+fAsYCYO4GoGi+78foMwBHGyA5AJGzqcPNU/I+UBl9t0Az9sbSUqTPmgX7nr2wrl8PQavF6H/8nZl3ogQWdAa+s7MTTmffHy4OhwPt7e1hWRQREVE4yUqvLvRsYkdRJqoEyAowYXgGpo3L8TkqLl2vjnwJfeVG4OVrTwTvbrZq1/HKjb4fp+seybvv/wGH3h84Y09BEUQRpuXLAj4/mIy9IIpInVoMw8SJkNtaGbwTJbigA/gpU6bg6aef7nP8ySefxKRJbIRBRETxxyn3KKFXsYSeYuO4vQtZBv/Z63S9GrZIltDLkivz7rOlXvexsuV9g/PKjcDmJa6/b74NeO4S4JEJ/oN9CprNYoF1/YaBTxQEqM3mkJrPaUaMgKPqGJR+Ot0TUfwLuoR+zZo1KCkpwX//+1/MmjULAPDWW29h9+7dsFgsYV8gERHRYLkDdpWgglpQewJ6omhqbOtCVqrW7/3pOk1k98Af2dE38+5FAWxVrvPGXOA65M7Y9w763Rn7K573X3ZPAXHPf+9vhByAQTefUxfkQ2lvx/EXX4J27FgYJk9iNp4oAQWdgT/vvPOwc+dOjBw5Ei+//DI2bdqE8ePHY//+/bjgggsisUYiIqJBkWQJakENQRCYgaeYaO+S0OmUkWXoJ4CPdAm9e2xcoOeFmrGngAUy/91tMM3nbBYLrPf+DgBQs3o1ji5YgK9nlcDG5BtRwgk6Aw8AZ599Nv7+97+Hey1EREQR4VScEFWuTBP3wFMsNNq7AKD/DLw+whn4NNPA5/Q8L5SMPQUloPnvAIYtX47sa64OKWPuL8PvtFpdx9mRniihBJ2Br6iowCeffOK5/cYbb+DSSy/FypUr0dXVFdbFERERhYMkSxAF1y++apWaGXiKuqa27gB+gD3wEc3AF053dZvvM5XeTQCMw13nAcFn7CloTmtgnzt1bm5IwXu/Gf7uY4HMlSei+BF0AP8///M/+PLLLwEA33zzDX7605/CYDDglVdewZ133hn2BRIREQ2WpJwI4FWCinvgKeqO212B+cAl9BG8NlWia1ScT91B/dz1rvOA4DP2FBSbxYKadesCOjeYrvM9DZjhV5SA58oTUXwIOoD/8ssvcfbZZwMAXnnlFVx44YX4xz/+gWeffRavvvpquNdHREQ0aJIieZfQMwNPURZICb1Rr4G9S4JTkiO3kKL5rsZz+qxeL17QtyFdsBl7Cpi7rF1uaur/xEF0nQcCnxcf6HlEFHtB74FXFAWy7PrBsn37dlxyySUAgJEjR6K+vj68qyMiIgqDPiX03ANPUSDJCsoPNaK2pQMff3scahWQqvVfBp2ud/1a1trpRGY/mfpBK5oPWA8Au54CLn7QlUEvnH4i8+7mzti/fC1cQXzPMmwfGXsKSDCN64DQu84DgWfuQ83wE1H0BR3AT548Gffddx9KSkrw3nvv4YknngAAHDp0CCYTS6iIiCj+9M7AOxWW0FNklR2oxupNlahu7vAcUwnA1k9rMHdCvs/HpOtd++NbOiIcwAPA8aNA7snAGT/u/zx3xr5smXdDO2OBK3jnCLmgBdq4DgCyb7h+UA3mDJMnQW02u/ba+3rDQBCgNplCzvATUfQFXUL/yCOPoKKiArfccgvuuusujB8/HgDwz3/+E9Ons4SKiIjij1N2Qi243rMWVSJkJYIlyjTklR2oxs0vVHgF7wAgK8DNL1Sg7EC1z8e5M/C2SDayc2s6AmSNDuzcovnAkgPAaZcC6WZgwWZgyScM3kMUTLm67c0tg2owJ4giTCtXdN/otRVikHPliSg2gs7An3nmmV5d6N3uv/9+iPzHT0REcahnBl4tqCErMmRFhkoI+n1son5JsoLVmyp9Tk53W72pErOLzBBV3gGVobu8vuxADWztThSPye5zTshkyTXuraUaaKsDrJVA5kjX8UBK4FUiUHAWcPg9jowbpK4jhwM+191gLnVqccivZywtBR59BNa167wy/2qTCaaVKzhCjijBhDQH3he9Xh+upyIiIgqrnnvg3YG8pEgM4Cnsyg819sm896QAqG7uQPmhRkwbl+M5XnagGne/8SkA4LG3v8Zjb3+N/Aw9Vs0r8ltyH7DKjX1L4AFg/0vA4fdd+9wDyaan5gHtTYDkAET/4/DIP5vFgvrHHg/qMeFoMGcsLUX6rFmw79mLbxctQvqcUhT87nfMvBMlIP7mQkRESc89Rk6SJRxuPgwAKK8uZzM7CrvaFv/Be0/bKk9kQt0l97UtnV7n1DR39FtyH5DKja4mdL2DdzfbMdf9lRsHfq60Ya4/29i0OBSe5nVBCleDOUEUkTq1GPqTTwacTgbvRAmKATwRESU9SZFgd9ox59U5eOaTZwAAN22/CXNenYPtR7bHeHWUTIalB1aR+NcPD6PsQHW/JffuY6s3VUKSA+tY7kWWXJn3fgv6u5Utd53fn9Rc159tHDkWimCa1wEY9Ag5fzSjR6P9wKdo3vwm2naVD2qPPRFFHwN4IiJKapIsYX/dflS3VcNqt3rdV2uvxdJ3lzKIp7ApHpON/IyBg3gBrsD8o28aAi65D9qRHf4z771fxVblOr8/qd2ZYAbwIXFarQOf5BahBnM2iwWt27bB8c03OHbHHTi6YAG+nlUCm8USttcgoshiAE9ERElr+5HtKP1nKT5t+NTn/Up3ZnJD+QaW01NYiCoBq+YVDXieOzDfebAhoOcNtDTfS2sQAWMg5zOAD5nNYkHNusDL59UmE4Y/+khYG8zZLBZULV4CubXV67jTakXV4iUM4okSRNBN7JYuXerzuCAI0Ov1GD9+PH74wx8iOzt70IsjIiIK1fYj27H03aWeIN0fBQpq7DWoqK3AFPOUKK2OktncCfm48bzR+MuHhwM4O7DS+EBL872kmcJ7viYF0KYzgA+SrawMVUtuC+hcVUYGhj/yMFKLi8Oaeffsv/c1C15RAEGAde06pM+axb3xRHEu6AB+3759qKiogCRJOOWUU6AoCr766iuIoohTTz0Vf/rTn3D77bfjgw8+QFHRwO9AExERhZskS1hfvn7A4L2nOjuDEgqfkiJzQAH8tLG5eLWiCjXNHT6vVgGAOUOP4jEhJEYKpwPGggDK6AXXeYXTB37O1FwG8EFoLivDsaW3B3x+/up7kDZtWtjXMeD+e0UJy8g6Ioq8oEvof/jDH6KkpATHjh3D3r17UVFRgaqqKsyePRs/+9nPUFVVhe9973u47bbA3mkkIiIKt4raij773QeSZwhPp2ciwLUXPkXrP5MpAMjP0OPccTmekvveE9/dt1fNKwptHrxKBCb8OLBz564PbB582jCglQF8IGwWC44tuQ2Q5YAfI2ZFpoI10FF04RhZR0SRFXQAf//99+N3v/sdjEaj55jRaMQ999yD3//+9zAYDLj77ruxd+/esC6UiIgoUMFk0wUIMBvMmDhsYgRXREOJJCsoP9SIzBTXrPSBAvO5E/LxxNUTYe7V/M6coccTV08MfQ585UZgx2P9n2McDlzxfGBz4GUJgADU7AcOvT9w1/ohTJEkWNesDfpxkQqgAx1FF66RdUQUOUGX0Dc3N6O2trZPeXxdXR1sNhsAIDMzE11dXeFZIRERUZCCzaYvK14GMZDsI9EAyg5UY/WmSq/O8oLgvfXYnKHHqnlFXoH53An5mF1kxqqNB/Dy7m/x3A1TUTwmO7TMOxDYCDlDLnDrx4BaO/DzVW50PZ+7HP+5S1xl93M3BBb8DzH1Tz4ZXNf5bpEKoA2TJ0FtNrvW5GsfvCBAbTKFfWQdEYVfSCX0N9xwA/71r3/hu+++Q1VVFf71r3/hxhtvxKWXXgoAKC8vx8knnxzutRIREQVk4rCJMBlMEPrkPr2ZDCY8NOMhlBSWRGlllMzKDlTj5hcq+oyFc49wv/G80fi/hefig2Xf95lVF1UCJhVmoUtScM6ozNCDdyCwEXL2euDbXQM/V+VG4OVr+z6frdp1vHJj6OtMQjaLBfWPPR7cgyI0893z9KII08oVntfq/dpA+EfWEVFkBB3AP/XUU5g1axauvPJKFBYWYtSoUbjyyisxa9YsPPnkkwCAU089FX/+85/DvlgiIqJAiCoRy4uXA4DfIP6i0Rdh64+2MninsJBkBas3VfrNdwsAthyoGTCrnmVwZcMb2wZZyRjoCLmBzus3k999rGw5y+m7yV1dqFl1T0iPjXQAbSwtxfBHH4Ha5D1tIBIj64gocoIO4NPS0vDMM8+goaHB05G+oaEBTz/9NFJTUwEAZ599Ns4+++xwr5WIiChgJYUleGjGQxhmGOZ13GRw/fI6c9RMls1T2JQfauyTee/JPfe9/FBjv8+Tk6oDEIYAPtARcgOdN2AmXwFsVa7zhjibxYKvLpwBqakpqMepzeaoBdDG0lKMf2s7RjzlSrqlzpiB/HVrkT5rVsRfm4jCI+g98G5paWnIzs6GIAhIS0sL55qIiIjCoqSwBDNHzsSkFybh4rEX49Lxl+LM3DMx+e+T0e5sj/XyKInUtvgP3oM5LzvNlYFvGGwAP+AIuQBHx4Urk5/kgpn17pZ5zTUwlpTAMHlSVEvXW956yzUTHkDbu++i7d13oTabYVq5gll4ogQQdAZelmXce++9yMjI8JTQZ2Zm4ne/+x3kIMZkEBERRYNTcUJSJJybfy6mmKdAp9ZBrVKjU+qM9dIoiQxL1w98UgDnZXeX0DcNNoD//E3A4e9Nqu4S/kBGx4Urk5/EmsvKUBXErHcAyP3f/0X+XSuROrU4qsG7zWJB1eIlfWbCO61WVC1eApvFErW1EFFogs7A33XXXfjLX/6C9evX47zzzoOiKPjwww9xzz33oKOjA2vWrInEOomIiELS0tUCAEjTnKgW04t6BvAUVpMKs5CdqvVb+i7A1X2+eEz/c75TtCJSNOLgMvDupnP+duSnZAHzHg2se7wnk1/t5/kCzOQnKc+s9yCIJhNyb/qfCK3IP0WSXJl3X13oFQUQBFjXrkP6rFlsZkcUx4IO4J977jn8+c9/xvz5J77pn3XWWRg+fDgWLVrEAJ6IiOKKJ4DX9gjg1XqW0FPYuEfH9Re8Ayfmvg/E9UZAiG8wBTI+Tq0HTr04sOdTia5RcS9fC9dH0vN5g8jkJyFFklB996qgH5d1xRUxCZDte/b2ybx7URQ4a2pg37MXqVOLo7cwIgpK0CX0jY2NOPXUU/scP/XUU9HY2H9jFiIiomhr7WoFAKRr0z3HdKKOGXgKC3+j43oyZ+jxxNUTfY6O8yUnzX8mf0CHPxh4fFzLseCazhXNB654HjD2Wr+xwHV8iM6Br3/yScjHjwf9OG1hYfgXEwBnXV1YzyOi2Ag6gD/rrLPw+ON9Z1s+/vjjOOuss8KyKCIionBpcbgy8D0DeL2oR4czsKZjRP4MNDoOALIMGrz365kBB++ux4QYwFduBF65NrBzg206VzQfWHIAWLAZME0ARk4FlnwyJIN3RZLQuvMjNPz5LyE9Xp2XF+YVhfd1Y7U+IgpM0CX0v//973HxxRdj+/btmDZtGgRBwI4dO/Dtt99iy5YtkVgjERFRyNwZeK898Go9OiQG8DQ4A42OA4AmuwNPvHsQi0tOCvh5c1K1+LbJHtxiBtr33lsoTedUIjDmAmD4RKB6/5Asm7eVlaFm9b1Bj4oDAAgC1CYTDJMnhX9hATBMngS12Qyn1ep7H3yM10dEgQk6A3/hhRfiyy+/xGWXXYbjx4+jsbERl19+Ob744gtccMEFkVgjERFRyFodfQN4nahDp5Ml9DQ4gY6Oe3j7lyg7UB3w82YaNPiuqR1vfFyFnQcbIMkDBOWB7Hv3EADj8ME1nTMOB1oC/3iShfX++1G15LaQg3cAMK1cEbMGcYIowrRyhdd6TtwZ+/URUWBCmgNfUFDAZnVERJQQWrpaYFAbIPbIFjIDT+EQ6Og4AFi9qRKzi8wDNrErO1CNl/d8h9ZOJxa/+DEAID9Dj1XzivyX4R/ZMfC+954G23QuPR9orQUkByBqQn+eBNK8ZQsa//LXkB+vNpniYs66sbQUePQRWNeu82popzIakX3tNUifNSuGqyOiQAQUwO/fvz/gJzzzzDNDXgwREVG4tTpavTrQA9wDT+FRPCYbZqMeNbaBr6Xq5g6UH2rEtHE5fs9xN8TrnUevae7AzS9U+G+EF/B+dgH4ybOD37duLACguF43Y8TgnisBNJeV4djtdwT9OCE1FfmrVnnK0uMls20sLUX6rFmof/IpNP71r5Db2iA3N6P+scdx/JV/xsUbDUTkX0AB/Nlnnw1BEKD42i/TgyAIkCQpLAsjIiIKh5auFqRr0r2O6dQ6tLa3xmhFlCy2Vdagwxn47z39ldz31xBPgWtgm98sfsPBAFegAAb/byAELL37TQRbdVIG8Iokwb5nLxxWK9p27IDt9ddDep6CdWvjNhBueest1D/+eJ+98E6rFVWLlwCPPhK3ayca6gIK4A8dOhTpdRAREUVES1cLM/AUdv6y5f3pr+R+oIZ4Cvxk8Ss3Au+uDXwRwXaf98XdAO/TVwFnh2s/fZI0tLNZLH3Ky0OR+7//G7cBsCJJsK5d57uRnaIAggDr2nVInzUrbqoGiOiEgAL4whjNqyQiIhqs1q5WrxFyAPfA0+AEMj6uJwGuWfDFY7L9nhNoQzyv8zzN64IQSvf5nio3nnjNj55w/WcsAOZuSPiRcrayMlQtuW3QzyOaTMi96X/CsKLIsO/Z2/8bFIoCZ00N7Hv2InVqcfQWRkQBCagL/c6dOwN+wra2Nnz66achL4iIiCicWh2tfUro9aIenRK70FNoAhkf5+Yudl81r6jfBnaBNsTzOi+o5nVh6D7vHlXX+zVt1a7jlRtDf+4Ya96yBVW3LR38EwkCzHetjOvMtbOuLqznEVF0BRTAX3vttZg9ezZefvlltLb63jNYWVmJlStXYvz48aioqAjrIomIiELls4RerUe7sz1GK6JEF2i2HHBl3v02n+uheEw28jP08BfiC3B1o/fK4n+xJeB1ABhc9/l+R9V1Hytb7jovASiShLZd5Ti+cRMOX7sAx5be7rukPAhqsxnDE2DvuDovL6znEVF0BVRCX1lZiaeeegp33303fv7zn+Pkk09GQUEB9Ho9mpqa8Pnnn6OtrQ2XX345tm3bhgkTJkR63URERAHx1YVeJ+qYgaeQBZot/+3Fp+G688YMODoOAESVgFXzinDzCxUQ4B0m98niyxLwnweAj/4U2IINucAlDw+uxH3AbL8C2Kpc5425IPTXiTBFkpC9fTsOrV0Hubk5LM+ZefXPYZxdGled5vtjmDwJarMZTqvV95sWguDpnE9E8SegAF6j0eCWW27BLbfcgoqKCrz//vs4fPgw2tvbcdZZZ+G2227DzJkzkZ3tf28XERFRtEiyhIraCtTZ69DU0YRUdarX/Xq1Hp1OBvAUmuIx2cg0aHDc7vB5v3vPe6DBu9vcCfl44uqJWL2p0qtE39xzDnzlRuDfdwIt1YE9qSEXWPoZoNYGvA6fAm1+F44meRFiKyvDsbt+g9y2Nshhes6CRx5Gxty5YXq26BBEEaaVK1zd5gXBO4gXXNeraeWKhHgzgmgoCiiA72nixImYOHFiJNZCREQ0aNuPbMf68vWw2k8EEs9VPodxmeNQUlgCoLsLvdQBRVEgCIEHWDS0SbKC8kON2FZZ4zd4B1zZ84H2vPszd0I+ZheZcfVfdqGupQO/++EZKB6T7Xou9x70YHrfX/Lw4IN3IPDmd4NtkhcBiiSh6o5fo+Xf/w7fk6pUGP7QgzAmWPDuZiwtBR59pE/HfZXRiOxrr0H6rFkxXB0R9SegPfBERESJYPuR7Vj67lKv4B1w7YNf+u5SbD+yHYBrDjwAltFTwMoOVOP8DW/jZ898hL9+eLjfczMNGswuMof8WqJKQFG+EbICTBuXc6Js3u8edD/OXRS+zvCF013d5vvbpT/YJnlhpEgSWnd+hG9vXYzPz5kY3uAdQEECB+9uxtJSjH9rO3L/938haF1v8sjNzah/7HF8PasENoslxiskIl8SLoDv7OzE2WefDUEQ8PHHH3vdd/ToUcybNw+pqanIzc3Frbfeiq6urtgslIiIokqSJawvXw/FT4CjQMGG8g2QZAl60bWHmQE8BcI98z3QzvPH7Q6UH2oc1GsOS9ehztbj+vzPA0F0nO92ykWDWoMXlegaFQegbxDffXswTfLCRJEk1P3xT/hiSjG+vf56tFosQBh/FxQzMzH8D48mXNm8Py1vvYX6xx+H0utz5LRaUbV4CYN4ojgUdAl9rN15550oKCjAf//7X6/jkiTh4osvRl5eHj744AM0NDRgwYIFUBQFjz32WIxWS0RE0VJRW9En895bjb0GFbUV0KtdAXy7sx0ZuoxoLI8SVLAz392C6VTvyzCjDi2dTti7nDB8vQV4d21wT2DIDX82vGg+cMXzrkqAnm8mGAtcwXsM58ArkoT6J59Cw1/+AsVuD+tzp5aUQD9+PAxTi5FaXJw0e8MVSYJ17TrfjewUBRAEWNeuQ/qsWUnzMRMlg4QK4P/973/DYrHg1Vdfxb97lUJZLBZUVlbi22+/RUFBAQDgwQcfxHXXXYc1a9bAaDTGYslERBQldfbAZha/c/QdzBw1EwAz8DSwYGa+9xRop/qBHl973I7RZcuCf4Izr4hMNrxoPnDqxa5u8//4CXDGFa599lHMvCuSBPuevXBYrZAaG9H13Xdo/uc/oXQM7k0TX7JvvAGmX/867M8bD+x79nrtf+9DUeCsqYF9z16kTi2O3sKIqF+DCuA7Ojqg1w/uB1SgrFYrFi5ciNdffx0Gg6HP/Tt37sSECRM8wTsAzJkzB52dndi7dy9mzpzp83k7OzvR2XniFzibzQYAcDgccDj8N6ih5OL+WvNrTvGM12n/srRZAZ23+ZvNmDXC1aCptaMVjhR+PsMlGa/RrQeCLFsHkJ+hwzkj0gf1ecjSAeeqKiH+54PgS+cBOMeXQonk12HEuVBnjYMMAbIkA1K4+rr3r8ViQd19ayA3NUX0dYTUVAy7dzXSS0uT6nruqbMmsEkGnTXV0Cbp5yAeJeP3UQpMoF/zoAN4WZaxZs0aPPnkk7Barfjyyy8xduxY/Pa3v8Xo0aNx4403Br3YgSiKguuuuw433XQTJk+ejMOHD/c5p6amBiaTd+fTrKwsaLVa1PTz7uK6deuwevXqPsctFovPNwoouW3bti3WSyAaEK9T32RFhgEG2NF/+WxTZxPe+OANAMA777+Dr9VfR2N5Q0qyXKP/bRDw7Jcq+G/c5ouCH5js2FoWetO0/OO7cfp3f8eL2kbgQHCPVQC0a7Kx7cBx4NMtIa8hEMWdGghf78OuLRF4HVlGyqFDUDc3Q2xrg5RigHHvHhi+ORTUVyNYzpQUHD//PDR+//v4wukEIvGxxYmUgwcxMoDz9hw8iPYk/jzEq2T5PkqBswe4/SfoAP6+++7Dc889h9///vdYuHCh5/gZZ5yBhx9+OKgA/p577vEZPPe0e/du7NixAzabDStWrOj3XF+jgAYaEbRixQosXbrUc9tms2HkyJEoLS1l2f0Q4nA4sG3bNsyePRsajSbWyyHyidfpwD7f+zn+8cU/Bjxv7GljgQpg4tSJmGKaEoWVDQ3JdI1KsoJ1D/4HQHDbLBZ/fzxumTku5NcVPt8M8dXHEVS3+W5Kd2irnf8QLjr1kpDXECjV1v9AdeQDXHRRGJvlIXpZdjf9lCkwXn4Z1CYTUiZOHDL7vRVJwuE3NkKqrfW9D14QoDaZMGPRoiHzOYkHyfR9lILjrgQfSNAB/PPPP4+nn34as2bNwk033eQ5fuaZZ+Lzzz8P6rluueUWXHnllf2eM3r0aNx333346KOPoNPpvO6bPHkyfv7zn+O5556D2WzGrl27vO5vamqCw+Hok5nvSafT9XleANBoNPxHMwTx606JgNepfyWjSwIK4AvSXdutnHDycxkByXCN7jnYgBpbcMG72ajDrSWnhDT/HYBrVNyWpVA8oXhwhO5mcupoNZPLHAl8fBiaz14H0s2upnkh7IXvuaf9+D//ifby8vCv1QdVairMa+5Lmo7yQdNoYL5rJaoWLwEEoW8QrygwrVwBbZS2y5K3ZPg+SsEJ9OsddABfVVWF8ePH9zkuy3LQezVyc3ORm5s74Hl/+MMfcN9993luHzt2DHPmzMFLL72EqVOnAgCmTZuGNWvWoLq6Gvn5+QBcZfA6nQ6TJk0Kal1ERJSYJg6bCJPB5LcbvQABJoMJU8yurHuHM/xNryixSbKC8kONePK9wLdWuIPte+afHnrwDgDv/R5obww+eB8/GzhvccgBdEgqNwIfPgI4O4DXfuE6ZixwjZrr5w0EXw3obG+8AbmlJTrrBiBptchd+AuYmFmGsbQUePQRVN+9CvLx4173qTIzY7ImIupf0AH86aefjvfffx+FhYVex1955RWcc845YVtYT6NGjfK6nZaWBgAYN24cRowYAQAoLS1FUVERrrnmGtx///1obGzEHXfcgYULF7IUnohoiBBVIpYXL8dt797W5z6hOyxaVrwMqZpUAOxCT97KDlRj9abKoLvOmzP0WDWvCHMn5If+4gdeh/LehtD2d5+3GBhzQeivHazKjcDL16JPmb+t2nX8iue9gnh30G7bvh2211+ParDek2AwIPP667BrxAiccsklQz5470lubvZ5rGrxEuDRR1yBPhHFhaAD+FWrVuGaa65BVVUVZFnGa6+9hi+++ALPP/88Nm/eHIk1BkQURbz55ptYtGgRzjvvPKSkpOCqq67CAw88ELM1ERFR9JUUlsCoNUJSJLQ52jzHTQYTlhUvQ0lhCRRFgUpQod3ZHsOVUjwpO1CNm1+oCGrneZpOxFPXTMa5Y3NCy7zLkmsc2+dvArueCC14Nw4P/7z3/siSaw68z8+UAkUWYP/rMjguUOBsaET7vn2w79gBua3Nx/lRoNMhfcaFyLzySqQWF8Mpy0ndmC5YnAVPlHiCDuDnzZuHl156CWvXroUgCLj77rsxceJEbNq0CbNnz47EGvsYPXo0FB/faEaNGhXTNxGIiCj2HJIDLV0tuHva3Sg0FqLOXoc8Qx4mDpsIsbu8WBAE6EQdM/AEwFU2v/y1T4JuG3fF5JE4b/zAWwF9+vR14M3bAXt9aI93m7s+qjPYcWSH11g7RQbsdVo42kW01WjReiwFcheAzcujtyY/0i/6AYbff7934ClHZ9xdouAseKLEE9Ic+Dlz5mDOnDnhXgsREdGgWe1WKFBQkFrg2evui17UM4AnAMDjb3+F4/bgZy7PLjIH/2KyBLz6C+DT14J/bE+CCPz4r/3uNw8nRZLQVr4b9s1/h1KZBlGrwGEXYTtsgOxQRWUNgVJlZcG86u6h25wuCM66urCeR0SRF1IAT0REFI8kWcK7374LAKhrr4MkS56se296tZ4l9ARJVvDXDw8H/bj8DD2Kx2QH96DKjcDG/wU6jgf9em4yXE3zhB/9FTj90pCfpz+eYH3XLiiyDGdtLVosFiieGcXx01tI0Oth/NGPoBs5EursbKhNJhgmT2K5d4DUeXlhPY+IIi+gAD4rK6vfWeo9NTY2DmpBREREodh+ZDvWl6/3dKD/zYe/wWP7HsPy4uUoKSzpc75O1KHTyQz8UPf421+huT347PuqeUXB7Xv/9HXglQVBv05vnSlmpMy7P2yZ94GD9fikyshA9rXXIPemmxisD4Jh8iSozWY4rdZ+Z8EbJnOiE1G8CCiAf+SRRzx/b2howH333Yc5c+Zg2rRpAICdO3di69at+O1vfxuRRRIREfVn+5HtWPqua352T7X2Wix9dykemvFQnyBer9ajQ+IYuaHGPSauprkdH3xdj1crqoJ+jttKTu6/47y7OV1LNdBWBzQeAnb/OfQ1KwJeES/C2POvQPGMeUHvee8dpItZmVBnZaNt1y60bN0a98E6s+yRI4giTCtX+J4F3528M61cwc81URwJKIBfsODEO8Y/+tGPcO+99+KWW27xHLv11lvx+OOPY/v27bjttr6je4iIiCJFkiWsL1/fJ3gHAAUKBAjYUL4BM0fO9JTTS7IEh+TA101fY3fNbq8Gd5S8Qh0T15PZqMMt3x/v/4RwNafrpijACyPuwdU3Lu43499zvrqzvh5yczMURUmYjLovzLJHh3sWvHXtOq+Gdqr0dGQvuBbps2bFcHVE1FvQe+C3bt2KDRs29Dk+Z84cLF8e+46jREQ0tFTUVnjK5n1RoKDGXoOK2gpMMU/pU2p/w9YbYDKY/JbaU3LYsr8ai/5REfLj3aHzPfNP9x9IW34L7PhDyK/R90VFrE/7NZqzvw9RJXgF6VJjI8TMTDgb42BUWxgJaWnIuOwyGEtKmGWPImNpKdJnzUL9k0+h8W9/g9zaCtlmQ/1jj+P4K/+EaeUKzoInihNBB/A5OTn417/+hV//+tdex19//XXk5OSEbWFERESBqLMH1h25zl4XUqk9Jb4t+4/hlv/bN6jnMGfosWpekXfpvCwBh94HjnwAWD8Hvtg0yJWeoMiAveg30FaIKDj4Oqo/2wTbG29AbmkJ22vEmpCWBuMPf8jS+DjR8tZbqH/88T574Z1Wq6vE/tFHGMQTxYGgA/jVq1fjxhtvxLvvvuvZA//RRx+hrKwMf/5z6Pu7iIiIQpFnCKw7ck5KDu764K6gSu0p8ZUdqMaif4QevP9gghnXThuN4jHZJzLvsgT85wFgx6NAV2hZb0UG2mq1sNfqoCgKRK0CtV6G1KlClyMDtkN6yC8/g4u6zz8e8kcQR1JSYJw7B2nTpjNYjzOKJMG6dp3vRnaKAggCrGvXIX3WLH7NiGIs6AD+uuuuw2mnnYY//OEPeO2116AoCoqKivDhhx9i6tSpkVgjERGRXxOHTYTJYEKtvdZncC5AgMlggqIoQZXaU+Lp2aCusa0LxhQN7n7jQMjPZzbq8PhVE0MO3BUZsNdp4WgX4WxXQe5SQYECp11ES1UKFGd/89MTu8GiYDQi7fszoTWZAUGAYWoxUouLGfzFKfuevV773/tQFDhramDfsxepU4ujtzAi6iOkOfBTp07F3//+93CvhYiIKGiiSsTy4uVY+u7SPvcJ3TuXlxUvQ2NHYGNOAy3Jp/gSjgZ1Pakg4w/ntkI88ArQagW+LQe+sgDOE8/vL4vu7FChvV4Lu1UHud8gPbkIGgkZp4gwXvFLGH50K4P1BOKsC+z7XqDnEVHkBB3AHz16tN/7R40aFfJiiIiIQlFSWIKHZjyE33z4G7Q5TmRGTQYTlhUvQ0lhCXbX7A7ouQItyaf4UXagGje/UOGj/iI4KsiYqqrENeJ2lGg+heMVB457Zc/VELWpUOtltFm1aPluoCx68hFSU5F63nSk5Guh/vwFSJ0qqPUy1CkSDHldEFQCULkamDAubLPqKfLUeYF93wv0PCKKnKAD+NGjR0MQ/I8xkSRpUAsiIiIKhiRL2GPdg88aPkOOLge5+lzcdNZNMKWavMbDBVpqP3HYxGh/CBQkd6l8bUsHslO0uOOV/QEH7ypFxun13yCnoxmZHS0wOuwQFOBM/UGcl/Ip1J0S2uu1OGRNH1LZc188wfo550CTk3ti37oA4JEJwJh2H49SAAhA2XLg1IuDnllPsWGYPAlqsxlOq9X3PnhB8Hz9iSi2gg7g9+3zbgTjcDiwb98+PPTQQ1izZk3YFkZERDSQ7Ue2454d96C5q9nr+Prd63HPtHu8mtH1LLUXIHgF8T1L7dnALn5JsoLH3/4af/vwEI63O/rc3zM4z+hsRYvWgPQuO1q0Bhg721DUeBgT675CqrPT5/M3IS3SH0Lc8hus+yqDP/Q+YDvWz7MpgK0KOLIDGHNBxNZM4SOIIkwrV7i6zQtC3yBeUTBs+TJuiyCKA0EH8GeddVafY5MnT0ZBQQHuv/9+XH755WFZGBERUX+2H9mO2969zed9zZ3NuO3d2/DwjIe9xsK5S+17zoEHvEvtKf64A/dn3v0KY6q/wtk9sudQgBZdKoa1NaLk271IcyZ287dI6hmkq7OyIR0/Htr4tlb/zSBDOo/igrG0FHj0EVjXrvPZ0K52/QYIKhVHyRHFWEhN7Hw5+eSTsXt3YPsLiYiIBkOSJazbtW7A89aXr+8zFq6ksAQzR87EM/ufwR//+0f8YeYf8L0R32PmPYYUSYJ9z144rFY46+shNze7msJlZeLTVgFbPvgc42oP4tl+sufULSUF6XNKoTWZPZ/DATPqwUozhfc8ihvG0lJAllG1pO+bo5wHTxQfgg7gbTab121FUVBdXY177rkHJ510UtgWRkRE5M8z+59BbXvtgOdZ7VafY+FElYgzh50JADg5+2QG7xGiSBLaynfDvmsXFFmGmJUJdVY2nI2NniDdWVuL1nfegdzc7PM5hgG4LqqrTgwtaj2cs+bi5LNOdn3uojmqrXA6YCwAbNWAz+4Dguv+wumRXQeFnSJJsK7f4OdOzoMnigdBB/CZmZl9mtgpioKRI0fixRdfDNvCiIiIfNl+ZDv++N8/Bny+v7FwaRrXfueeXespMP4C8876OuTsrUDdV19Dqa9Hi8UCxW6P9XITXotaj+0jJ8Gamg2bLg31+gx8mjsWf//ldOSOy4n+glQiMHcD8PK1AAR4B/HdvyPOXc8GdgmI8+ApIcmSqzfHkQ8AWQYM2UBqLtBWB7QfByC4+nGMPj8pvi8FHcC/8847XrdVKhXy8vIwfvx4qNVhq8gnIiLqQ5IlrC9fH9Rj/I2FS9WkAmAAD/gPyKXjxyFmZvbJmPcXmOcAaO71uwL552tfupCRgYdf243vZJ0nWJeFEx3xBQDmDD2Kx2THbuFF84ErngfKlnk3tDMWuIJ3jpBLSJwHT1E3UPCtKCeO2RsAQ473fS3VwGdvQOlog71OC0e7CKlDBVEnw9nhGgEKQYFh2KMwFKZB9cNHE/77U9ARtyAImD59ep9g3el04j//+Q++973vhW1xREREPVXUVng1nxtIf2PhhkIAH2gJOzPlERbCvvTzTpqKm1+oAOAzv41V84ogqvyP9Y2KovmuUXFHdgD7/h9w4DXg1v8Cak1s10Uh4zx4Coq/4NtXoO0rMO8OvtE18M9hRYYnQHe2uwJzBQpErQKHXYTtsBmyw//oz4ZKQKWVkP/1QhhvR0IH8UEH8DNnzkR1dTWGDRvmdby5uRkzZ87kHHgiIooYf+Xw/iwvXu53f7u7hL7V0TrodUVTz4ZvUmMjxMzMkDLlFD69s+jur8Ng9qXPnZCPJ66eiNWbKlHdfKKzvjlDj1XzijB3Qn64P4zQqERXaWpXK7D/JcBeDxjjZG0UNM6DH6JkyfVGXEu1K8AOMPhWKt+AEIY3wXsG572z5woUOO0iWqtTXNn0wXyYXSpUfZiFLu2vkfvHixO2nD7oAF5RlD574AGgoaEBqampYVkUERGRL0dtRwM6L02Tht+d97t+x8IZNAYAQFsA7/xHWiCZcjErE46qY7C98QbklpZYL3no8JE9H9QItiDMnZCP2UVmWD6twc1/r8AdpSfj5hnjY5959yVjpOvPvX8DRl/gamCXoL8cD2WcB5/AegbhrdYBy89lRcYRux5S02GMPLoROin4N7OD/U6kyEBbrRb2Wp3r+6kne27oN3sePq6eHU0fycg8+D7UJ82IwmuGX8ABvHu+uyAIuO6666DT6Tz3SZKE/fv3Y/p0dhslIqLICLR5XZY2C9t/sh1atbbf81SCCga1ISIl9OHcU04RopKRPrIdWoPsKcNUp2rgzP8e5JxzAJUYva7u/RBVAuacboZGFGBM0cRn8F65Efj3na6/v7fB9Z+xwNXoLoHLVIcqzoOPA4oM4dB7wHcf+dwX7g6+65U0OG11yG7ch9HN5dDJgf8cUQEYE5ml+yx1d9pFtFSlQHFGI1DvjwBnuxoHt72HU5I9gM/IyADgysCnp6cjJSXFc59Wq8W5556LhQsXhn+FREQ05AXavE6AgLun3z1g8O6WqkkNKIDvb04595THJ0EjwVjYDl2a5KOZURdSC9QQJvwQyBiOeO9QrFIJMBn1OHa8Y+CTo61yY3c3+l6ZWlu16/gVzzOIT0CcBx9mPvaKyyk5OHz0CKzWY64JfYZsqNLzkFG9A3Ot26D+uNPv07mD70gE4MHonVEPV6l7NDQ3+xqBmRgCDuD/9re/AQBGjx6NO+64g+XyREQUNYE2r1t09qJ+y+Z7SxMN0H38FWrffdRvprx93z7Yd+yA3Bb7Unvq5it7rpchdaqg1stQp0gw5HVB6P07pDYNGDcLmHJj3Abr/hRkpKC6uT3Wy/AmS64u9D5nwSsABKBsuavRXQJ9ronz4AfUXa4u247h8OFDfYJwuaUOaG+EogA5cgPG1r8NjeT9pq4KwNju/+JZ/GfUQyOcMi3WSwhZ0HvgV61aFYl1EBER+RVo87pRhhFo21UecKb83i2HoOs6iIYIr58CI2gkpBV09AnM+2TPh/kIznvTGYEzrwSyRwOpeUB6fkLvyzZn6PBFdQve+LgKw9JdI+RiXk5/ZIf3CLk+FMBW5TpvzAVRWxYN3lCbBy85nfh811bY64/CabN6gm9/AfnoxvehczQnTBAeqN7Benu9FnarDnKCBum+KWgzpOKcebNivZCQBRTAT5w4EW+99RaysrJwzjnn+Gxi51ZRURG2xRER0dDmdHRh//YX0frVblz0hQRbioIMO5DanYhsTRFgM7iOnfQdMPrR3+KoPfAyY93Ap1AYCGoJqaZOpOQ6vAJyd5CuSeknax6IJAvWeys7UI23PqtFW5eExS9+DADIj4du9K0BjnQM9DyKG4k6D16SFZQfakRNczsa27qQmSJCf2wXcpUGyH4Cc231HpzStgenC3FW4RJhQyNY702BAgFti+6EWhN0HjtuBLTyH/7wh56mdZdeemkk10NEREnM1wi03p3W3ZnyIx9uhbB7P1K6FJwB4Azfz9jrdhzuEU5iEQ/MfUnyYL23sgPVuPmFij5Xek1zB25+oQJPXD0xdkF8mim851HciId58D2D8frWThxvd0BRgCyDFtmpWjS12pFm3Y3UzlrkCq2o6tLj4NEjMDhsgACY0IjvixXIEgLY/hSHvSEjwb1nvenrVNhrkj1Y78umTUXHrXfiwl/8NNZLGZSAAvieZfMsoSciot4CCcyDHYFmiPCaqa/eAbnU2Xceb8QC8560acDY7wOjpp4YfzQEgvXeJFnB6k2V/e0wx+pNlZhdZI5NOX3hdFe3eVs1fO+DF1z3F3JKUaKJxDz4PtlxgxaNbb4Dc9sX/0FN1SGkOY+jUUlDjmBDJtoAAahW0iCiDj9Rf4AMwUezUM0gPvAk4WuueptVi5bvEnfPejBatcB7ZwD1Gd0Ve4KATwsFVI3XY9X5ObFe3qCFXDvQ1dWF2tpayLLsdXzUqFGDXhQREcVWMJlyNnmLfzHJlAdDbQBOvwwYN2PIBen9KT/UiOpm/1UlCoDq5g6UH2rEtHEx+KVUJbpGxb18LdzzlU/ofkNh7np+LRNQIPPgTStXQBDFATPljW2d2Hu0CR98VY/WTgkqyChWfQ4TGpErHPcfmAtgMB4Ed9Buq9LDdihac9Wjz6YD9pwENKa7bru30hnbBTQbgKZ0AZ+NFKD4elPTacNt796Gh2c8HFTD23gTdAD/5Zdf4sYbb8SOHTu8jiuKAkEQIElS2BZHRETh4Ssg9zWTPJRMOcWOv1FpcROY96LojBDOvBLIGgW0H0e8j2+LtdqWwLaEBHpeRBTNd42KK1vm3dDOWOAK3jlCLmGllsxG24r7oPvDeqhbvX8eONOM2F5phaV2jycw76l3kH462nC6AJjUjZgdaFk7+eWUgf3NqbB1qNHRoYKuVo3c79RQOxJ/L0Cf7DlcQXpz6gDBeRDWl6/HzJEzISboz52gA/jrr78earUamzdvRn5+fr8N7YiIKHIUSUJb+W7Yd+3yGoHGgDxxDZQpD2hUWoy0Kjp8apyBdkWLsaOGY3jBCIjpeV7l7wIz60EZlq4P63kRUzTfNSruyA7gXzcD5tOBK//Br3WckmQFHx1swM5v6iH3ypS7s+dWWwe2f1aL0w9+id+0tni2bLipWm049cm12D/1+5g13Igc0eYpdZ8kfIULxE+QLrAniQSgQq+DVRTRIKrQrBKhAMiUJWRJMpp8HGsWVcjwdZ9DhrNWA+3nehR8KyKlC0iJ7YcXsjYNsH8M8MWIILLnYWS1W1FRW4Ep5ikRfZ1ICTqA//jjj7F3716ceuqpkVgPEdGQ1l+mvLO+Djl7K1D31ddQ6uvRYrFAsfvY/0dxI272lIdRMwyozLkIyBwJtDcCgoi0U2eiaNpFmKgo2LJlC/IvugiihrWvg1U8Jhv5GXrUNHf422EOc4ZrpFzMqURXNcXws4EOG4P3GBlon7nV1oF/H6iBvWvgilmVIuOm/a8D6NvjzfVtSsHPP7FgfGFt3H/fCpQEYI9eh3K9HjJCDLS7j32s02GnIQVtquA/OYKsoOiIjNOPAlAUqFuAM74AUhzh/XgjyVepeziz6IMV6HjaeBR0AF9UVIT6+vpIrIWIKCmFM1OeA6D5nXeit3jyppKRPrLd75zyRAzIfWlWDPindD6qlGHo0GRgqgkoGD4KhpyROHXqHExT+/71QXYk0G+XCUBUCVg1rwg3v1Dhb4c5Vs0riv08+J6yRgP/fQn45J+u7vOsuggrf9nz4/YuHG2047V9VWjpcAb9vCrImKqqxDShEoKgoElJg662E3kdzf08SoDTroa9TotUU1foH1SIema3m/oJpgcKtN3HatUqbDcYYBdjd70KsoLLd8iY/5GSUMF6z4x6PAXp/ckzRG6CQqQFHcBv2LABd955J9auXYszzjgDml7vsBuNxrAtjogoHvXMkjvr6302dnMfc9bWMlOeCNQy0kwdfjPlEBQYhnUhdVhiBuS+tCh6/Ec6A3uVk9EqZmC6WYDamIcWzTC0mYuRYzSgyOjK7sZVgDjEzJ2QjyeunojVmyq9GtqZ42EOfG+VG4GK/wd0HAdevdF1zFjganTHvfAB89cULpjseX96B+smNOIisRxpQqfXec3OFBxD1oDP5+wILOANNOAO5NgxtRqb0lLREsNgOxwEWcFp3yrIblEw4bCCaZ/Hb5bdrgY+OjV+M+rBMBlMmDhsYqyXEbKgA/iSElfHvlmzZnkdZxM7Ikp0gWTKnbW1aH3nHcjN/WUlKOYCzJQfMarw+5EZuB9WjJST6+eXO4t+TMn1dHluVxsxtnA09Nkj8bluAiCoMWtcDs4dm8MgPY7NnZCP2UVmrHh1P7YcqMYz106JvzdWKjd2d6PvVexvq3Ydv+J5BvHd+tuD3rNbezgEGqz7otYHtoY3cvVQjGK/wfdgysmTjTton/KljBmfAKkDfymizitY7x7BVjkqsYL0/iwvXp6wDeyAEAL4d1i6SUQJgpny5GFXAx+dBhwoBDLsgKkNmN7ejnFCl1dgHmymvEarReVwFdqrVMAgs1rR1DN73qikI1toRaOShmyhFQ2KEVZk4zPt6bh0ykiMyk5FdpoO5l7Z9Eti/DFQcESVgKljc/Dy3u9wzqjM+AreZcnVhb6/ifVly12N7hL4l+Zg+dqPvvNgPbaEIYvuS89gXRFkdKZUY6zmS7SpJTSrRMgA2mUJb0simsS0gTPfKTIK0mSktKrg69upDFeA90BRWtIEdpHi3tM+e5+Csw8BhujvOPAr2YP1njJ1mVg1bVVCj5ADQgjgL7zwwkisg4goYMyUJwd/o2JsBgUZ9u5jvX6Z+GVTM87t6MDEjk6EIwzQKzIAYLvBAJsqfM8bqt6BeY5g82TPm5Q0NCgZsCIb5fKpkKFCul7E5ecMx/DsVJxucO2DPcNHsE7JoSDT1XO66ng7xuWlxXg1PRzZ4T1Crg8FsFW5zhtzQdSWFS2+MurfNYWyH12GaDgEQd0MQWyDIhkgiG0QxHYAChTJAEVK9RwToGCkbEO2JGGE5ijOlQ+jXS31ynZnDOpjKy6VcftrMmTAK4iX4erD8FxJcgZ64RKve9rbNMD+sYBloiqpgvVUdSqm5U/D2cPORpY+C02dTWjuaIagEjDFNAVTzFMSOvPuFnQAv3//fp/HBUGAXq/HqFGjoNPpBr0wIhp6Ag3MmSmPYyoZxlHtSDV19SlXrzSKeFOTHtqeOUWB2enEouPNYQuwtxtSsDbHtb/zyawMPIkMmJxOLG9oQom9PUyvckKrosWb0rnYKRcNGJj3lKoVccFJuZhUmI3Tu5tV/ZRB+pA0IssVwB+LtwC+1Rre8+JY72D9xL50R9/gO6UN2rTewbfdZ2AuaJqgzdwHQQxu9Jq1+7/PAGwbZLDuS/kpKjx4OXDdNhm5Pfqrur9LLdiuQBFklJ/C0ni3ntn2SQcBXfA9BcPK/WZ5XWZ0R7X5kiKmoLSwFMMMw6AICjK1mV6B9kDHmruakaHN6HNfTkoOTKmufe3JEKAPJOgA/uyzz+539rtGo8FPf/pTPPXUU9DrYzyXlIhiKtDu6+qsbLTt2oWWrVsZmMchQSPBWNgObarUZyZ5f6XrPRsW/Ss9FeV6PdDPz4+BLGtoCmvwvnRYbp+C31pRxNJhuXiotj7gIL5V0WKLNBVWJdsTkPfMniuCgJ1yEXbJRX2CcwCeLPqELAMmdTeryjJokZvOIJ285aa5EiRbPqmGWqWKn2sjzRTe82JAkiXsse5BeXU5ZMjI1GYiQ5eJj6uq8EVdDRQIcHTq8PkxGV1Ki3fwndmEtBCC70RRfooKggwsfd1VsdTzistuAW5/TcaDl2PIB/Hxkm13bzn7ZLQQcrAe7kA72TLgsRZ0AP+vf/0Ly5Ytw69//WsUFxdDURTs3r0bDz74IFatWgWn04nly5fjN7/5DR544IFIrJmIYoyZ8sTmDsh1aZLPmeTuLuxqfeij0CyGFNyXm42mMHQIVikK7g8ioB6IBGB9TpYreO/1hoIiCBAUBRtysjDT3g4R/svaBwrMgRPZ8+8XZuPH3dnzzO5Sd1/70on8KTtQjdWbKgEA/1f+Lf6v/Fvkx0sn+sLprm7ztmr43gcvuO4vnB6Rl/cVfPsLLOrt9dhr34tvPv4G2SnZyNJnYXfNbmw9shXtzoG/x6hMwFBLTwmyggVv9Q3eAVcmXoYrQ7/7pOQpxQ5GLAP3djVQNWUkVMPyoAhA15knQz3pLKQ5m1HUfe1fNED2OplLzZNV0AH8mjVr8Oijj2LOnDmeY2eeeSZGjBiB3/72tygvL0dqaipuv/12BvBECaRnwzepsRFiZiYz5QlCUEtINXV6RqBFIiAPlARgeV4OylINg8q2AwAUVyBwf209SsNY0v6BLh1WP3PMAVcQX6NW43rxCnS1n+yzrN0tXS/imnOGY0SWwTPqidlzCreyA9W4+YWKPqFxTXMHbn6hAk9cPTG2QbxKdI2Ke/lawN/E+rnr/Tawk2QJFbUVsLZZ0dTZ5DfI8BWA1LbVYvvR7bA7g/uZ9J/K/4T0oQ5Fp32reJXP96YCkNviOq+ycGh9v5v6mYybtshIjWJTOodeg4YzRyLtih/hnDlXY6JGG70Xp7gQdAD/ySefoLCwsM/xwsJCfPLJJwBcZfbV1dWDXx0RDVoggbmj6hhsb7wBuaWfn9AUdQNlyjUpkQ/I+yMB2KPXoVyvhwygVq3CNoMB7WGay5spy1hV3xhw5j3QBnB7tZ3Q46UBn+9j/TjMPrkEN2emeALz7FRmzym6JFnB6k2V/fV3x+pNlZhdZI7ptSidejEq5t4D656n0dTVfGLGd0omlLEzkSk3IevrjX0C82Ntx7Dp4Ca0OPjzJ15ltYb3vETgrxma5w0lTQayf/88sj/8rE9VQljpdEi78HtIOeccaHJyoTaZYJg8CUKYfs5SYgo6gD/11FOxfv16PP3009BqXe/4OBwOrF+/HqeeeioAoKqqCiZT/O5zIkoGDMwTTzxlykPRc0/7rhQdLAYD7BH4JcIgy7j+uA0Lm20QcWKeeZUyDI1KWlAN4HwRnQcDWsfTP/8+zi04Z5AfDdHglB9qRHWz/73VCoDq5g6UH2rEtHE5g3qtnpnwho6GgPa6NnU24WPrx9hZsxNtjjYgXQSQ7f3ENe+5/qOE1BRgv8RAz4uENE0a5o2bh+FpwwNufObrmvbXDK3n71xtO3fCVlYGtIe/2ambMyUFeTdcD9OiRQzWqY+gA/g//vGPmD9/PkaMGIEzzzwTgiBg//79kCQJmzdvBgB88803WLRoUdgXSzQUBDK7vH3fPth37IDc1hbr5Q5pgkZCWkEHtAbZZ2O3eMmUh0M497T7pACjW4wY1TwKavtI1AptWNo9zzzQwNzN3RSud1m7O3uemXoGHv3idRzvqofiI68pQIDJYMIU86RwfoREIaltCawxWs/zQilJ9wrCiXr4bKSA+nRXw7r+5sF/NjKwXHSKmII5o+dginlKwE3R/F2/WbqsiHcft5WVoWb1vZCamiLy/G5CWhoyLrsMhpkz8K7VilMvuYTBO/kUdAA/ffp0HD58GC+88AK+/PJLKIqCH//4x7jqqquQnp4OALjmmmvCvlCiRMfAPHEMlClPhoC8P71L4z/Wa7FnkB3k/ene5o6OqivxScvZ+MTPeQaNChedkY9p43LR2NbpMzAPpqw9I2cllr67FAIEryBe6C6GXFa8jA18KC7kpmkgGg52jydr9TsT/MP6T3D8QD5L0insFJWAZ2ercPtrsmfbhue+7ttN/3MZ7rvg3KRqkKZIEqru+DVa/v3vyLxASgqMc+cgbdp0r9J4h8MBbNkSmdekpBB0AA8AaWlpuOmmm8K9FqKENVA5u7O2Fq3vvAO5uTnWSx2ShlKmPBTRKo33Z7T6B7hw8o+8AnJ3kC5AwLRxOTh3bE5Y9/eWFJbgoRkPYX35eljtJ2ZTmwwmLCtehpLCkrC9FlFPA5Wp98mKV++EoXDgN3X/XeX6j6inFKhROmwShg2bAEUQQs9yzzGh4MIG1K3bAGdNjef5xYwMZF97DU67/qakyhbbLBYc++3dUML9e5tOh/QZFyLzyiuRWlycVJ8zip6QAngAqKysxNGjR9HV5d12cf78+YNeFFG84D7z+DbUM+XB6BmkN4kq6Jwq7FSNwWeiEbUZ38EhSlFfk15lwO/Ovxdzx8wZ+OQIKCkswcyRM3HrO7fi4PGD+N15v4toGSYlp0DL1VmmTsFIkyTMa23DcKcTzSoRCoBMWUKWuzlgz2NQo7ngLGSYzkSTKKI5NQuCSgx/ttsMZJTMRv2TT6H+j38EZBlyczPqH3scx1/5J0wrV8BYWhqe14ohW1kZqpbcFtbnFAwG5Nx4A3JvSq43Oig2gg7gv/nmG1x22WX45JNPIAgClO76R6G7tFKSov9LIFGo3PPMW3fuQM5XX6Gprh7a3FxIx4+j67vvGJjHCAPz0LibvX2n5KAjxYo2dRcaRRH1atlPkN7U/V90ZWgzcPVpV2PhmQtjHiyLKhHjMsbhcPNhTDFPielaKL4Ekin/ruU7lqtTH/0F382i6kSHfh/3ZUkyTJKEiR2d8PndUW0AiuYBGcMBCMCYC4DR5/sd0RduLW+9hfrHHz+x/6mb02pF1eIlwKOPJHQQ37xlC47dfkd4nozZdoqQoAP4xYsXY8yYMdi+fTvGjh2L8vJyNDQ0RHzu++jRo3HkyBGvY8uWLcP69es9t48ePYpf/epXePvtt5GSkoKrrroKDzzwgKdbPg0d7sDcvmsXFFn22mfes6y9xWLxzDPPAdDwzrsxXXeySvTu67HUqmjxpnQudspFfbqvNyrpyBJacFDvQK1ajW9URkDTBG3mXghiYI2vokUv6vGjk3+EWaNmxV2WO1WTilZHEs0/on4F0mmdmfKhS5a0kNpOgmQv9PQY0Go7MDLbgNFZeTil8xgyq15DC5zBB9+h0qYB42YBU26MarDemyJJsK5d1yd4d92pAIIA69p1SJ81K+GC1XDvd0+/6AcYfv/9Cfd5oMQQdAC/c+dOvP3228jLy4NKpYJKpcL555+PdevW4dZbb8W+ffsisU4AwL333ouFCxd6bqelnZhXIUkSLr74YuTl5eGDDz5AQ0MDFixYAEVR8Nhjj0VsTRR9gew37xmYU+QwUx66VkWLLdJUWJVsr4DcHaQrgoCdchF2yUXdHdhliIaDEA3fwN3AStA0QZu5zxOs62L6EflmUBtw/enXx0W23Z80bRpauxjAJzpJlrDHugfl1eWQIbPT+hDkK/j21/TP+5gdipQGxWmE3jkeM04ahklnZSM3vUdjTMjAq78APn0tOh+MzgicdRVw2iVA4fSYBe092ffs9dr/3oeiwFlTA/uevUidWhy9hQ2CIkmuLQF//nNYxsKpUlNhXnMfMubODcPqiHwLOoCXJMkTOOfm5uLYsWM45ZRTUFhYiC+++CLsC+wpPT0dZrPZ530WiwWVlZX49ttvUVBQAAB48MEHcd1112HNmjUwGo0RXRsNHru0xw9BI8FY2A5tqsRMeRDcJezHlNw+mXL/gXlvMkTDoe6O001QSfugMRyE2ngAKrHLx/nxKZ7K5AeSrk1Hl9yFLqkLWpEVW/FooH3mtW212H50O+xOvnGb6GRJD+fxcyA7svwE2gYfQXg6FKcRkn0MfA8688894eK8k/J8T7GQJeA/vwc+fARwRPD6UhuA0y8Dxs0A0vPjJmjvyVlXF9bzYs1msaD67lWQjx8f9HNxjztFU9AB/IQJE7B//36MHTsWU6dOxe9//3totVo8/fTTGDt2bCTW6LFhwwb87ne/w8iRI/GTn/wEv/71rz3l8Tt37sSECRM8wTsAzJkzB52dndi7dy9mzpzp8zk7OzvR2dnpuW2z2QAADofDNcaBwsYVoO9BR3k5ZFmBOjMTqqxMSE1N6Pj4Y7Tv/AgKA/OIYmAePH+Z8myhFY1KGrKFVjSENK9chmj4yiujrtEdhy5rH5wYfBYgmgxqA841n4uzcs9CTkoOhhmG4Zy8cyCqRMiSDFmSY73EfqWoUgAATfYmZOuzY7yaxOb+uRnsz09JlrCvbh9q7bWusvZOV+fnDG0Gqu3V2HxoM7c5JIgUWcacVjumdHT43OfdJKrQeOZPgYyRyNBm4Nt6Aa/s+wodXYaQg/Bg6VQKLjwlDz+fWoipvQJ2WXJC7m4XIny2EeLmWyFEqEJHEfVQTpoNeeL1UArP8w7YJdn1XzzJzgr4vHj/Hbp1+3bU3LZ00M+jyshAxtU/R/bChRBEEU5ZBuTBfd1C/T5KiS/Qr3nQAfxvfvMbtHUHWffddx8uueQSXHDBBcjJycFLL70U7NMFbPHixZg4cSKysrJQXl6OFStW4NChQ/jzn/8MAKipqYHJZPJ6TFZWFrRaLWr6KfdZt24dVq9e3ee4xWKBwWAI7weRzGQZKYcOQd3cDLGtDVKKAaK9DaLd9Q65utmGtAMHIHYlTgYxYahkGEe1I9XUxb3lAQpPprw/J7LoWnUbRqemQKtpQ7tihwIBitiMOvWnkIS+/x6c4fkQI04DDSZpJqFIW4TR6tFQtaqA7t9xa1GLrdga2wUG4RvHNwCAzds2I1fMjfFqksO2bdv6HJMVGYedh2GTbWiVW2FXXNlMm2LDF44v0J5gb1wlMw00OF19OowqV/WiQWVAKlLRprR5vm69jwmCgLHiaPzyyz8i1dEIX0MfFQDtmmw8UfU9NB1W4c1a4GubCsCwiH9MepWC4mEKzsxWMM6oQCXU4PgXNdjqq3hUkTHp8BMYfnyXz49jsDrFVHyTV4ovzT8EBBXwWSvwWQJ8z5RljMnIgLq52e/X15mRgXet1vieY+50Yty9v4MKCOnrK2k0sBVPQevpp6N9zBhApQK2hv/r5+v7KCU3e4DbfwVF8dWJIjiNjY3IysrydKIP1D333OMzeO5p9+7dmDx5cp/jr776Kn784x+jvr4eOTk5+OUvf4kjR45ga69/QFqtFs8//zyuvPJKn8/vKwM/cuRI1NfXs+y+B2bPY0AlI31ku9/Z5RAUGIZ1IXUYg3IgkpnyvlI0KvzgdDOKxxpxoGEfDrbsh6IoyNBlwCE04L/Nb6NLSc5SXqPWiKtOuQo3nn5j3JfGB+rzxs9xVdlVeGHOCyjKKYr1chJaR1cH/vzvP2PU6aPQ4mzxlLr/t+6/2FWzC21O/pyItjRNGi4eczEKUgu8Kht69wdo7myGSlBh8rDJmGSaFPK/b+HzzRBfvd71d5z4FVPpDpV+LdyOf7ZPHORHNTD39+np47NhNuoxuTALokqAw+HAtm3bMHv2bGg0Gu8HyRJUHz4E1Y7HIIR5O4aiM0I+80oop1wEZeS0uCuPD1Tr9u2oWXq760bPEKI7BjA/9CDSSkpisLLAtG7fDutvfhvy76xZN9+E7P/5n4iWyfd7jVJSs9lsyM3NRXNzc79xaMhz4HvKzg6t5PCWW27xG1i7jR492ufxc889FwDw9ddfIycnB2azGbt27fI6p6mpCQ6Ho09mviedTgedrm/rJ41GM2T+0bApXHQJGglpBR0MzAMUfLO30Bk0KvxgghnmzBRIsoR21deA2Izatjoc72qGAAEnDxuGswtGYK/1NTz8lcV7z218NX4ftFR1KqblT8PZw85GTkoOTKmmuOsgHw6ZKZkAgA6lY8h83x8sXw3jjrUdw8aDG12l7uWxXmHySRFTUFpYipY2A979qg52u9bTlC09tQsXjM/FxBEjPIF5li4rNv9mz7gMEEWgbBlgO+Y5bNPm4c7Wq7BVjlzwnqoV8b2T83D1uYU4d2yO9172Xvr8nvfp68AbtwBdYRwJOPoCYOK1QHo+hMLpSfG9M+sHP4AoirCuXefV0E6VmQnzqrvjunmbraxsUGXzBQ89iIyLLgrjivo3lGIRcgn06x1wAH/DDTcEdN5f//rXQJ8Subm5yM0NrVzR3e0+Pz8fADBt2jSsWbMG1dXVnmMWiwU6nQ6TJk0K6TWShb+Rapx1Hhn+9pmzI7u3FkWP/0hnYK9yslemPBKBeYZejZLThsGcmQJFAbIMWmQaRHxSvxdft+4HFMUTmDd37e+3U/Unh4FXDw9qOXEtTZOGH47/YVyOe4uUNK2rMSs70fvWO1hnw7jwcAfkwwzD+syX79mkT1AJmGKaginmKdhWWYub36xA79LJpiZg43fAD66eiLnj82Py8Xgpmg+cejFguQvY9RR2Tv8zfv6WdtDfy33Ra1T42ZSRKD09v28DukBZfgvs+EP4FmXIBS56EJhwafieM44YS0sBWUb1qnsgN7uqOuSmJtSu3wBBpYrLOfDNZWU45q4cCEH2jTdENXgn6k/AAfyzzz6LwsJCnHPOOQhD1X1Qdu7ciY8++ggzZ85ERkYGdu/ejdtuuw3z58/HqFGjAAClpaUoKirCNddcg/vvvx+NjY244447sHDhwqQuhWf2PHrYAC4wvQPz3rPLG5SMsJSxA65sywUn5WJSYTayU7VobOvE8XaHJ3suqJshqu04oyAfzV37PR2rD7fVYvsX3gFIsgfmPaVp0jBv3DyMTHc1kYppti4OuAP4FsfQfiPTV6f33TW7YTliYbAeIPe/reFpw33Ol+8dkAfzb02SFazeVNkneAdce48FAKs3VWJ2kTm0IDbcVCKksSUQP3oCd7zVAhl5YX36zBQNrj9vNG75/kmD+3gPvBae4L1Htj0eO8iHk81iQdVtS/vMg3darahavAR49JG4CuJtFguOLbktpMdyLBzFo4AD+JtuugkvvvgivvnmG9xwww24+uqrQy6dD5ZOp8NLL72E1atXo7OzE4WFhVi4cCHuvPNOzzmiKOLNN9/EokWLcN555yElJQVXXXUVHnjggaisMZrcQbtt+3bYXn+d2fMw6D3PXOpUQdQxMO/J3fitShnmlSkPZ2DuK1PeMzD3HEtTo1n+HK3C51CEL6DVZkKjz4JW34RjPrLnLx8O0ychAfUM1odykN4fjUoDvagfkhl4d9D+1pG3sPHgxiH/JoYvA2XKo/UGWPmhRlQ3+9+nowCobu5A+aFGTBuXE5E1BGPL/mr88dVv8aYAjFLVokoefAA/bWwWrphS6HvcW7BkCXjnAeC99YNblDYdmP940mbbe1MkCda16/oE7647FUAQYF27DumzZsXFODW5qwvVK+8K+nGCToecXy7kWDiKSwEH8H/605/w8MMP47XXXsNf//pXrFixAhdffDFuvPFGlJaWBt3ALhgTJ07ERx99NOB5o0aNwubNmyO2jnhgs1j67Dsi/3oH5r27tA/lsvaBMuXusvZwNH7zlyl3B+S56TqYjXpMKszAvrq9KK/+ADJkr8Bc1yN7/ufPWL7rC4P10KVp05I+eO2ZYW/oaOh3q8hQ4C8wH2ymPFJqWwJrshHoeZEiyQoWv7gPm/dXQ48MyDrgEtVOKBBC/jmSnarBfT+cgIvOLBj45ADkH98N9cOLgY6m0J9Ekwqctxj43h1JnW3vzb5nb/+/gyoKnDU1sO/Zi9SpxdFbmA82iwXHVqwMumGdkJqKk3fugKp7VDVRvAmqiZ1Op8PPfvYz/OxnP8ORI0fw7LPPYtGiRXA4HKisrERaWlqk1knoLllavMT3u55DUT/j0xiYx7aEvXdgXjwmG4DsCR60+gZPQK7VZkLUZ2FL9W4s+YilugPRQovzR5yPiaaJsW9WlUTSNGlJmYF3719/6fOXsKN6x5AI1ns2XxxoT3ki/XsZlq4P63mRUHagGste3Y/mdifmqMqxSvM8VALwc/Xb+DnexjElG6sd12KrPHBgF9Zsew/CZxsx5dBjg3uS0y8HfvTnIRW4uznr6sJ6XqTYLBZU3bo4pMcWrFvL4J3iWshd6AVBgCAIUBQFsiyHc03kQ78lS8mG49P86m9UWrhGpAFAul7E5ecMx4gsg3fpeqoWx+1dyE4LPDD/sqMJL787tDN9oeqdUc/R5aBmTw0u+d4l7EwbZuna9KS4Pntm2XdV70rK/ev+9pkn86QEACgek438DD1qmjt87oMXAJgz3N+Xo2/L/mos+kcFAGCOqhxPaB7pc44ZjXhC8whudizxG8Sn6UT8/kdnhi3b7uXA6xD/9YvQZ7sPsXJ5X9R5gW2FCPS8SFAkCdY1a4N+nJiZCfO9q+Nq/z6RL0EF8J2dnZ4S+g8++ACXXHIJHn/8ccydOxcq1RCLpKJswJKlBOKvrJ2BeeRHpQUTmIsqoU9TK402A9qUZojaDAbmYeIrW+gvo+5wOLBF2BLD1SavVE0qWsI5PioGLIctuO+j+9DUOYiy4BgzaoyYOXKmz/3mQ73SRFQJWDWvCDe/UAEB8Ari3QHpqnlFUW9gJ8kK/vDWV/jDW18BAFSQsUrzvOvvvZaiEgBZAVZp/h+2dU72+nkWtqZ0/lRuBP65ILTgfYiWy/timDwJarMZTqvVd1JJEKA2mWCYHLsJUPVPPulaXxByf/Ur5C66mfvdKSEEHMAvWrQIL774IkaNGoXrr78eL774InJyYt8kZaiIdSlSwFjW7iUao9IMGhUuOiMf08bl9m32FkRgrtY34duOZhxtVvBlpWum86aDm5J+X3CkDdVsYaJJ16a75pcnkJ7/jv/11b9Qbk2M4eu+9p7z30Ng5k7IxxNXT8TqTZVeDe3MGXqsmleEuROiO0Ku7EA1lr/2CY7bHZ5jxarPUSA0+n2MSgAK0IBi1ef4SC7CddMKMWfCIEbABUKWgE2hlVMP5XJ5XwRRhGnlCteWTkHwDuK7+2GZVq6IWSBsKytD/WOPB/WYnF/9Cnn/e0uEVkQUfgEH8E8++SRGjRqFMWPG4L333sN7773n87zXXnstbIujE2JZitQTs+cu8bDPXICAaeNycO7YnD6/9PT8xV6tb2BgHmHBZNEpPqVp0mBtCy5jEyuSLOGZT57BC5UvoLmrOdbL8av3m1eJuvc83sydkI/ZRWaUHajBr/5RgWVzTsEvLxwX9cx72YFq3PRCRZ/jw3A8oMcPw3H86apzIlMq39t7vwfa/b+p4JsK+PFfgAmXR2RJicxYWgo8+kifpspqkwmmlStiVoIeyqx3VWYm8hbdHKEVEUVGwAH8tddeG9FO89Q/T8lShMvo/c06H0rZ81ZFizelc7FTLoqbBnDuX8zcDanKq8shQ8bXHZlo+Ma7SdNQ7ywdbv2NkGKQnjzivQt9z3Fvr371Kjqk2HYad+v95lVjeyOOfHYEpdNLUVxQzH8XESKqBMyd4Jr3nmHQxqRsfvlrn/i8rxaZAT3H1SVTUByN4P3A68B7G4J/3I//NqT3ug/EWFqK9FmzYN+zFw3PPQf7rl0YtnQpxIxMKJIU9Qx8qLPe8+9dzbJ5SjgBB/DPPvtsBJdBA/GULIXYURPgrPN43Wfu1rus3b3PfPc+V9l1bVstth/l+LRw6x2AJHqnagqeJEuwddpQb6/H7prdcfWGTDxl23v+W/FX8u5wOLDlmy2YbJocN5/DZCWqBAxL16G6uT3qr/3YW195lc33VC6fimNKNsxo7LMHHujeu28cjuIZ8yK6RgDAp68D/1wQ3GNSsoF5jwJF8yOypGQiiCKk5uNo37sXSlsbjt15JwBAbTZHNROvSBKq714V1GPYsI4SWchd6Cn6jCM6gPOaYK0wwtne9xejoZ4991fWHq7AHHDtN//BBDPMmSkhBebalL6BeaaWZe2RkiKmYM7oOZhinuIJzLnnlnrafmQ71pevh9XuKp+/YesNMBlMWF68HCWFJTFdm+WwBXd/eDfanNGvpuk9AYH/VuKTOUPvtRc+GrbsP4ZHuxvW+SJDhdWOa/GE5hHIincjOwWCq4nc3PWR31N+4HXg1euDe8yFy4EL7+R+9wD5G2/stFpdxx99JCoBcv2TT0I+fjzg842XXYaC+37HzDslLAbwiUKWgLJlMI5sR/rwdtjrtHC0i5A6hnb2PFzj01K1KoxLdeKi4lORZ0zBcXsXMg2B7zd3l7UfbZbxZWWmJ5PLcvbIYvacBmP7ke1Y+u5SKL2GctXaa7H03aV4aMZDMQniJVnC8veXo+xwWdRe0/1m19T8qQzWE4h7rFy0uEbF7RvwvK1yMW52LMEqzfMowIm954KxwBW8Rzq73d1xPigXLgdmrojMepJQv+ONFQUQBFjXrkP6rFkRDZQVSULjc88HfL5oMjF4p4THAD5RHNkB2I4BAAQVkGrqivGCwiPWZe3ufebnjEjH1rJ/46LzR3vma/vbb94zk8uy9sjwF5gze07hIskS1pev7xO8A4DSnSfcUL4BM0fOjNo15i6X/+snf0W7FLmy6J59HfhmV2Iblq7Hx0dr8MbHVRiW3rcCLJy27D+GW/5v4ODdbatcjG2dk/GP2RLO/fReIHMUcPWrkc9uh9JxPiXblXmngA043lhR4KypgX3PXqROLY7YOuqffBKyzRbw+ea7VjJ4p4THAD5RtCZGd+TeIlnWHsr4NLeeZe1fdjTgo/2N+Nr+NRoqG5CbmovdNbthOWJhYB4BbApH8aCitsJTNu+LAgU19hpU1FZginlKxNcT6XL5VHUqphdMxxWnXMFgPUmUHajGqxXfoaXDicUvfgzAlZGPxDi5sgOBZd57StOp8cBPzsS5E/KBpk1AS010StP/80DwHefnPcqy+SAFOt44kmOQbRZL4CPjBAHDH36Ie94pKTCATxBS6jDE24+WyJe1Bz8+zS3Usvb/fPyfkNZK/gNzlrVTPKqzB/ZLZaDnDcZDex7C3z79W9ifVy/q8aOTf4RZo2bxTbEkU3agGje/UNGnfqSmuQM3v1CBJ66eGLYgXpIVrN5UGdRj/n97dx7eVJn2D/x7cpI2SdsUuiUtS0EBseIIbSmCoiClrSCMl6PgCrjwG3QcYSrI4jsiMwqiwODyyrzDq4ijgxs6L261FEEEUYTKKFZFEEGgC5tNabpkOb8/ThOaJmlP2mZrv5/r6lXOyZPkbnNaeud5nvuOiRZR+ufxiFI3/f8f3xs4trtT4mmVww588bzy8YII3PgiC9a1g9L2xoFqgyzZ7ah8fKni8Yn33QdDQUFAYiEKNibwEWK3fTDSW6nqGgiBLgrX1rJ2X0sBnbPnHx7+HGcbzrrap3FZe2A1L2rV/HvOxJwiUbJe2R+VSse1V9Hhok5P3uOj4nH7xbdj5m9m8meyC3Im1F52HkMCIABY8m4ZxmeYOmU5/e7DZ/wulLfypsvOJ+8A0KOPvA3QYQ/sTPf2FUDdr4qGSgCE370IXHJ94OLpwlztjSsrve+DFwSojUbos7MC8vyn/v53+bkVYK936mqYwEeIqlorXvJR1bU9At3rPF6rRu7FKe2u1n6g/rRblXYWhQscZ2LeK7aX2z5zLmunriwzJRNGvRFVliqv++AFCDDq5es+EOwOO/7n6//Bmv+s6ZTHG24cjhsG3sCf1W6grYRaAlBeXY/dh89g5IWJHX6+4m9b2efcgkoAnrvFy+x/XC/AYQO+/F8gJQNIH9X5iXzZJmCbshlZCQLsN7wANfu8t5urvfHsOYAguCfxgvy3nXHRwoDsN/dr6TzY6526HibwESIlTuuzqmtrqiU93rJfiRNSUshnz1mtPfi435zIO1ElYkHOAhRuK4QAwS2Jb2p0hfk58wPys9GZ+917RvfEf13+X8jrx32d3UVVjbLZcKXjWvPB1yfw0mc/Kx7/3C3DMOE3LZL3sk3A+w/K//6wqVCcIQ0oWN55S9ebOvUo9b3xegy4mMvmO8qQlwc8vRqVS5e5FbRTG40B6wPvqn6vUNIf/8h979TlMIGPEDn9E5Aar8VH1XJV1xzV9zDiDJKEX91mzztjD3pHe50fbDiLkz9xWXsgtazSXt1YzWXtRH7KTc/FqjGr3PrAA4BRb8T8nPkBaSHXGfvdOdvevaXEaTt1nC/+FK5zzrx7Td7fmAa0XOViLpfPT3m5c5L47StcnXraIukScCD1txjQ8WclyEl83LhxsOzZi2MPPIDYq65C2hPLAjbj3Wb1+2ZEoxFJs34fkDiIQokJfIQQVQIWT8rAva+UwgEVPndktPuxnLPnfRNi2Os8TPla1s72aUSdKzc9F2P7jEVpVSn+uOWPyO+Xj0dGPhKQn6+O7neP0cTgL6P+wtn2bs75hn5Fdb3XffACAFO8/EZ7e9kdEh7dpLxw3bSR6Z7Ju2tWvJXd+kULgMETO7ac3o+l8wBgn7AK+Kn9KxDJkyCKiBmRg+iLB8N64gTMHxZBnZwMfXZWpyfyNR9vUTyWLeOoq2ICH0EKhqRize2ZWPJumc/9bx2dPf+pYTdOc/Y8IHSiDvn98jHcNNyjrzmXtROFjqgSMdw0HAm6BMRr4zv9Z6+j+931aj3uvOROFqUjAO5v6AtwT4+d/7MvnpTRoQJ2z338IyrMypfg51/ipeL9kc/amBWXAPNxeVz/0f4HCfi9dB5jFkEafB3w0wftez7yyVxcjPqvv4FksaBu714AgNpk6tSl9JLdjupN7yoay6Xz1JUxgY8wBUNSMT7DhN2Hz6Ciug6nzilrrdY8Sf/X93Ll9urGahyrOYZ3D72LGmtNCL6arqXlsnYuZyeKLLGaWJxrPNepj1lypASPfvYoqhur23X/gn4FeGL0E/zdQW58vaFv6oQ+8EX7y/G3kh8Vj0/1Ndt/TlmFcMXjvGnzTYJm4tKAq+YCdkf7n4+8MhcXy8XsWlSjt1VWyuefXt0pybRlz144zp5tc5yqZ08unacujQl8BBJVgkdl2eat1U7Xn3ab3T1Re4JJegf4Ssy5rJ2oa4mLiuvUBL7452I8+MmD7bqvAAFPXvUkCvqzbzF553xDv/D1fdj+40k8f1uWz/arSrWn57vP2f5Yo7IHUDrOmx/8mEm/drm8VJ8JfKdyFZXz1kpOkgBBQOXSZYgbN67Dy9mVLp+PnzyZS+epS2MCH4Gaz6afrj/NPegd4KzSnqhNxMGDB5GVkYWkmCQuZyfqhmI1sThn7ZwEvvjnYszbPq/d919x9Qrudac2iSoBw/r2wIf7K3D5BQkQhI71mPWn57vPlnFO6aPkavPmcnjfBy/It6ePal+wDjvw9evKxo5Z1HkV78lNm0XlJAm2igpY9uxFzIicdj+PubgYZ9e/rGhs3DXXtPt5iCIBE/gIU3KkxKNiMvmmdFm71WrFByc+wISMCdBoNKEOm4hCIDYqFr/U/NLhxyk5UtLumfce0T2weOTigFTAp64pxaBFo92B6joreuijOvRY/rSe89oyrjmVKLeKe2Ma4Gu3fsET7S9gd+QzwHK67XG6RHnpPAWE7eTJTh3njT+t49QmE/TZWe1+LqJIwAQ+gpQcKUHhtkK3fsXdWWtF4bisnYj8FauJRU1jx7Ya2R12PLH7iXbdl/vdqT1S4qIBAFU1DR1O4H8+pWwl359yB2HCb9LaHpgxWW4VVzTffa+6IU1O3jsyK650+fxlUztW5Z5apU5O7tRx3vjTOs64aCGXz1OXxwQ+Qjj/KOwOybtzWXuKPsWtSjuLwhFRIMVGdXwJfWlVqd8rpLjfnTrC2e+90lyPQca4dj+O3SFhw+6jbY4zGaJx/zV+dFHPmCy3ivvPa8D/3SfPyufM7HjruM+fVzb2ogntfx5qkz47C2qTCbbKSu/74AUBaqOxQ7PiSve+95w+nZXnqVtgAh8h2vNHYbhitXYiCkdxmo4Xsdt6dKvf9+F+d+qIFEPTDLy5oUOPs/vwGVQoeIxbctL9L5SnEoGLr5MT+JikjiXv/rSOM/Rq/x57UkQQRRgXLZSrzQuCexLfVJOhI7Pi3PtO5IkJfIQ4aWn/3qFg4rJ2IopUsVGxqLXWwiE5oBJUft/f7rDjvZ/eUzye+92pM2g1IgxaNapqOpbAl5QpW6LcL0nfvieINgCaGKCmvH33d/KndVxH9tiTYoa8PODp1ahcusxtqbvaaOxQH3jFe987YZafKJIwgY8Qyfr27x3qTJw9J6KuKjYqFhIk1FprERfl/1LktV+vxdmGtnsUA8C9l92L3//m9/x9SR1md0iI1arx+U+nMLRPj3a1kivaX44Xdv6saKxzyb7fBAEwpDZVpe8ApXvfL7+PleeDyJCXh7hx43B63TqcXLESaStWwHBtQYf2oyve+y5J3PtO3QoT+AiRmZIJo94Y8GX0sZpYTLpwEnrF9uLsORF1K7GaWABoVwJfcqQE//2f/1Y09rbBt+G+off5HR9RS0X7y7Hk3TKUV9fjxK/1+OTAKaTGa7F4Uobv9m4tKO39LgAwxWuR0z+h/QEb0oAahbPn3nDve1gTRBGG3FycXLES6sSEDifUSivXc+87dTdM4COEqBKxIGdBp1ShdybpfeL6ID4qnj3PiYhwPoGvaayBKcak+H7+Vp4flz7O79iIWiraX457Xyn1+Iugoroe975SijW3t9KjvRmlvd8lAIsnZfi//725uDTg7M/tu6/ive8d7C9PHaI2GgFBQPX7HwAqEfrsrHYn8kor13PvO3U3TOAjSG56LlaNWdVqH3hfFdyZpBMRtc456+5vJfq1X69VvDrKpDchMyXT79iImnPOmnt7O1+CPFu+5N0yjM8wtZlwK+39ftcV/RTP6vsUZwQOfQx88xYQa5STbKV/jyje+y5x73uImIuL5T3rkoTqt95C9VtvQW0ytXsfvO3smTbHsO87dUdM4CNMbnouxvYZK1elr63E6frT3INORNQJms/AK+XP0nkAmJ8zn7+jqcPamjWXAJRX12P34TMYeWFiq4+ltPf7+Azlq1K8KtsE7H0JqK8GNt4tnzOkyW3llOxVP6dwCyH3voeEubhYrkTfopWcrbJSPv/0ar+SeMluR9UTy9scZ1wwn3vfqdthAh+BRJWI4abhoQ6DiKhLcc3AK2wl5+/S+T8M/QMrzlOnUDpr3ta4ov3l+FvJj62O6ZS972WbgDemAS3XDJjL5fNTXm476Y41Knsu7n0POle1eG994CUJEARULl2GuHHjFCfbSgvYiT07cF0SRSj/++QQERF1QTq1DipBpXgJfWlVqeKl80adETMvndmR8IhclFaCb22c0uJ1QAf3vrv2rvta8A+gaIE8rjXpowBda8mawL7vIdJmsi1JsFVUwLJnr+LHVFrATuk4oq6ECTwREREAQRAQo4lRnMCftCj/w3HBiAVcOk+dJqd/AlLjtfCVUgsAUtuYNVdavG5O7qCO7X1vc++6BJiPy+Na8/37QF1re6K59z1UApFsKy1gp3QcUVfCBJ6IiKhJnCZO8RL6o+ajisZx6Tx1NlElYPGkDADwSOKdx23Nmitdht8vSd+OCJtRune9tXFKKtDrEoDBE5XHRZ0mEMm27ewZQNVKmiIILGBH3RYTeCIioiaxUbGKitjZHXa8eeDNNsel6FK4dJ4ComBIKtbcnglTvPsyeVO8VlELuc5Yhq+I0r3rrY1TUoG+7kzbs/gUEPrsLKhNJkDw8YaRn8m2ubgYJ/5UCDgcrY4zLlrIAnbULTGBJyIigpyUS5KEH87+gC8rvoS9lT25pVWlqKqravMxb7roJi6dp4ApGJKKHfOvwYaZl0OrUeGWnD7YMf8aRUvec/onoIde4/N2JcvwFUkfJVebb23Bf1t71ztjFp8CRhBFGBctbDpo8To3HStNtlstiOekUqHX31a1qzUdUVfABJ6IiLq9kiMlyN+Yjx9//RFfVX2Fuz66C/kb81FypMTreKX73/vG9e3MMIk8iCoBIy9MRGq8DnFajeJic5vLKvCrxerzdgkdLF7npBLlVnEAfC74b2vvemfM4lNAGfLy0Ovp1VAb3V8DtdGIXn60kFNUfd7hYPV56taYwBMRUbdWcqQEhdsKPSrKV1mqULit0GsSn6xXtpdT6TiijkqIicLpc42KxiqpQN9Dr+l473enjMlyqzhDi5UBhjRlLeRqTwNCa3+ysgJ9ODDk5WHAlhL0Xb8eURdeCF3OcAzYUuLXTDmrzxO1jQk8ERF1W85e7pKXFlfOc8t3L/dYTn+2/ixUrSQUAgSY9CZkpmR2bsBEPiTEROFMbYOisUoq0P9qsWL34daqvvspYzIwZz9w8SQgLg2Y/h4w55u2k/eyTcBbMwCp9f3QrEAfHgRRRMyIHOguvRRotPq9R53V54naxgSeiIi6rbZ6uUuQUGGpQGlVqetcyZESzP1kLhxtJBTzc+Zz/zsFTWJMFM7UKpuBV1qBXuk4xVQikDoUsDcA/Ue3nXC32kO+iSACN73U9hsBFFSatFRYy8v9vp8+O8tjGb4bVp8nYgJPRETdl9K97M5xrc3YO6kEFVZcvYKt4yioEmKicFphAh+0CvTexCQDljOA3db2WCXV5yU7oE/snNio06hTU2GrqoJk9V1nwZuaLVvgaPCxksTPgnhEXRUTeCIi6rb83cve1ow9ADgkB3pqe3Y4NiJ/JPgxA5/TPwGp8drW6sJ3TgV6b2JTAEiA5VTbY1l9PmJpUtMASYK1su1uHU7m4mIcnz0Hjl9/9Xq7GB/vV0E8oq6KCTwREXVbmSmZMOqNEHykMi33svs7Y08ULImxUbA02lFv9d3+0ElUCfjzxAyv60icPwmdUoHem5gU+XOtgp8RVp+PWOoU+U3P6rffRu0XuyHZW78uFbWPi45G3LhxnRkmUURiAk9ERN2WqBKxIGcBAHgk8c7j5nvZWX2ewlVCTDQAKFpGX7S/HH9933sVelO8Fmtuz1TUS75dYpLkz+cUzMx2Rg95CjpzcTF+uWcmAODU88/j6PTpODguF+biYp/3UdI+zl5ZCcuevZ0aK1EkYgJPRETdWm56LlaNWYUUfYrbeaPeiFVjVrntZXfO2PvC6vMUKokxUQCAM220kivaX457Xyn1WYX+zxMvDlzyDsh74AFlM/BuPeRbUthDnoLKuQzeVuX+Bo2tshLHZ8/xmcSzfRyRckzgiYio28tNz8VHv/sIDw1/CACw9MqlKPpdkUchuq2/bEW9zXvi423GnihYEpoS+NOttJJz9n/3tUhZAPDX97+D3dHKMuaOitIDUbHKEngAGDwRGLMQUKndzyvtIU9B0+oy+KZzlUuXeV1Oz/ZxRMpFVAL//vvvY8SIEdDpdEhKSsINN9zgdvvRo0cxadIkxMTEICkpCQ888AAaG5UVdCEiou5NVImumfMBPQZ4JOElR0pQuK0Q1Y3VXu8fHx3vMWNPFCzxOg0AoOS7Suw6dNprEt5W/3cJQHl1fef2f/cmJlnZEvqyTcDqIcC2pYCjqWq9rgcwZpGyHvIUVG0ug5ck2CoqvC6D12dnQW0yuSrNe2D7OCKXiEngN27ciDvuuAN33nkn/vOf/2Dnzp249dZbXbfb7XZMnDgRtbW12LFjB1577TVs3LgRDz74YAijJiKiSKLT6AAAdbY6t/NK2sdFq6Ixts/YgMZH5E3R/nLkrvoEAPDK50dxy9rPceXyj1G0370Pd8j6vzfnsANqLXDsS+Dwp/KxN2WbgDemebaRq6sGti0Dvn8/cDFSu3RkGbwgijAuWuj9DmwfR+QmIhJ4m82G2bNn46mnnsKsWbMwaNAgXHTRRbjxxhtdY4qLi1FWVoZXXnkFw4YNQ25uLlauXIm1a9fCbDaHMHoiIooUerUeAGCxWdzOK2kfV1lXidKq0oDFRuSNrz3tFdX1uPeVUrckPqT934HzM+onvwOO7gLWXycfl21yH+ewA0XzAa9vmDWdK1rgO/mnkOjoMnhDXh56/W0VVLGx7uONRraPI2pG3faQ0CstLcXx48ehUqkwbNgwVFRUYOjQoVixYgUuueQSAMCuXbswZMgQpKWlue6Xn5+PhoYG7N27F2PHep8VaWhoQEPD+f1izmTfarXCarUG8KuicOJ8rfmaUzjjdRp4GsjLkGvqa9y+zxU1rVdHbj7Omth9Xx9eo8Fld0h4dNO3PtNcAcCSd7/FmIGJEFUChvWOg8kQjUpzg88Wcqb4aAzrHdfpr6Hw/XsQN94JQHKrKS+Zy4E3psH+u3WQBl8njz2yA+qWM+9uJMB8HLaftkNKv9KvOHiNBo7mst9ANBphr6ryvg9eEKA2GqG57Ddev//nSkpw8onlcJw75zqn6tkTifPmQjd2bLd5zXiNdl9KX/OISOB/+uknAMCjjz6KVatWoV+/fli5ciWuvvpqHDhwAAkJCaioqIDR6F4ZuGfPnoiKikJFK/txli1bhiVLlnicLy4uhl6v79wvhMLe5s2bQx0CUZt4nQaOTZL32X5e+jka95+vofKT9SdF9z/4n4P4oOyDgMQWSXiNBseP1QIqzL6XFMt72hvw3OtFGBgvJ1QTTAJeNDsXYLql0pAAXGu04KOiDzs3UMmBvG8LIbZI3uUI5Odt3FSIzYcACCr0OrML2Qoedt+nH+H4t+1bZclrNDBi88Yj9Z+vAGh5dQGQJBwdn4uyjz7yvN/+/V7vZz97FhUPzsVXd9yOc0OGBCzucMRrtPuxWCxtD0KIE/hHH33Ua/Lc3JdffgmHwwEAePjhh/G73/0OALBu3Tr07t0bb775Jn7/+98DAAQvhS8kSfJ63mnhwoUoLCx0HZvNZvTp0wd5eXkwGAx+f00UmaxWKzZv3ozx48dDo9GEOhwir3idBsfjrz2OgRkDMWHQBNc5u8OO9za9h5OWk173wQsQkKJPwb2T7u3WFeh5jQbXu1+XA2XftDnugkuGYsJv5NZwEwBkfluJeRu/QZ3V4RqTGq/Fw9cORv4lvtsktpdwZAfU+3wXxhMA6K1nMHFID0jpV0I4YgCOrGnzcYeOzsdl7ZiB5zUaQBMm4FxmJk4+sRz2yvPbjjQmE5LmP4SBuZ5FPiW7HT+v+hu8bYgQAEAQ0HdzCdIffLBb7IHnNdp9Kd32HdIE/v7778fNN9/c6ph+/fqhpqYGAJCRkeE6Hx0djQsuuABHjx4FAJhMJnzxxRdu9z179iysVqvHzHxz0dHRiI6O9jiv0Wj4Q9MN8XWnSMDrNLB0ah0aHA1u32MNNFiYsxCF2wo9xjvbxy3IWQBtdID2DkcYXqPBkdojRvE45+thd0hIjNMhJS4acVoN7h59AUwGLXL6J0BU+Z7w6JC604qGqetOAxoNcMFVcps4czm874MXAEMa1Bdc1e4e8LxGA6fntdeiR14ezB8W4cTcuUieNxeJM2b4TL5rS79yS/Y9NFWvt/7na8SMyAlQ1OGH12j3o/T1DmkCn5SUhKSkpDbHZWVlITo6Gj/88AOuvFJ+p9VqteLnn39Geno6AGDkyJF4/PHHUV5ejtRU+V3m4uJiREdHIyuLLSeIiEgZvUbvUcQOkHvFrxqzCo99/hhO159PSIx6I+bnzGf7OAq6nP4JSI3XoqK6vpU97XJyDsgF75a8W9as4F0dniz6HosnZQQueQeAWIWz+s5xKhEoWC5XoffQFGfBE+1O3inwBFFEXO44AIAmKanVmfOOVK8n6o4iogq9wWDArFmzsHjxYhQXF+OHH37AvffeCwC46aabAAB5eXnIyMjAHXfcga+++gpbtmzB3LlzMXPmTC6FJyIixfRqvUcbOaexfcbi+gHXAwCmZ0zH2vFrUfS7IibvFBKiSsDiSfLqRM+95TJncu5PtfpOlz5KnlH3iLJZtIZe8jinjMnAlJcBXYL7UEOafJ494MOeSquFKi4OtlOnWh3X0er1RN1NRCTwAPDUU0/h5ptvxh133IHhw4fjyJEj+Pjjj9GzZ08AgCiKeP/996HVanHFFVdgypQpuP7667FixYoQR05ERJFEr9bDYvWcgS85UoL8jfl4Yf8LAID1ZevxXzv/C1t/2RrsEIlcCoakYs3tmTDFu2/fMMVrseb2TBQMSYXdIWHJu2WtNWXDknfLYHd4G9EJnDPqAHy+1dByRt1hB3Q9gX6jAbUOuGEtMP09YM43TN4jiDopCbaTrSfw+uwsqE0mV793D4IAtckEfTZX1BIBEZTAazQarFixApWVlTCbzdi8ebOrhZxT37598d5778FiseD06dN49tlnve5vJyIi8kWn0XksoS85UoLCbYUeveCrLFUo3FaIkiMlwQyRyE3BkFTsmH8N/pQ7CACw9o4s7Jh/DQqGyFsKdx8+4zHz3pxcrb4euw/7LjTXYc4ZdUOq+3lvM+rOfvHrrwO++z/AVgeULAbqznLZfIRRJyW1OQMviCKMixb6uFFO6o2LFnaLAnZESkRMAk9ERBQMerUeddbzS+jtDjue2P2E1+rzznPLdy+H3eGthjJRcIgqAcP7y6sSB5ni3Pa0V9X4Tt6bUzqu3TImA3P2A9M2yccj/+g5o162Sd773rIPfFO/eJRtCmyM1KnUyW0n8ABgyMtDr6dXAy2SdLXRiF5Pr4YhLy9AERJFnojoA09ERBQserUeZ+rPz0SWVpV6zLw3J0FChaUCpVWlGG4aHowQibwyaOUKxuY6m9v5lDhl3RGUjusQlQhccDUQHQ/EJHkumy+aD++V5yUAAlC0ABg8kTPxEUJMTEL9gQOKxsaNGwcIAnrefjt0Q4dCnZwMfXYWZ96JWuAMPBERUTMtl9CftCirfKx0HFGgxOuaEvh6q9t5Z7X6VkrIIbVZtfqg0MUD9b+6nzvymefMuxsJMB+Xx1FEUCclwd7GHnhA7gVv/uADwGaD2mSC4doCxIzIYfJO5AUTeCIiomZaFrFL1iurfKx0HFGgnJ+Bd0/g/alWHzS6nkDdr+7nzrXSC7w94yjkxIQE2KurUf3v/0PtF7sh2T23GpmLi3FwXC5OzHsIAHByxQocHJcLc3FxsMMlighM4ImIiJrRqd1n4DNTMmHUGyH4mL8UIMCkNyEzJTNYIRJ5FauVd0a2nIEHzlerT4lzL+7bvFp9UGl7eM7A+9svnsKaubgYJ1etAgCcWLAAR6dP90jMzcXFOD57DmwVFW73tVVW4vjsOUziibxgAk9ERNSMXuPeB15UiViQs8DrWGdSPz9nPkTuyaUQE1UC4qLVHnvgnQqGpOIf07IBAA+OH4QNMy93q1YfVLoeclX55trTL57CkjMxt591f42bJ+aS3Y7KpcsAyUvNg6ZzlUuXeZ21J+rOmMATERE1o1PrPPrA56bnYtWYVdCK7kW+jHojVo1Zhdz03GCGSOSTQafxOgPvVF4tvzl1y4i+GHlhYnCXzTfnbQl9e/rFU9hRmpjX7t7tMfPecqytogKWPXsDFClRZGIVeiIiomb0aj0aHY2wOWxQq87/N5mbnovXvn8NNsmGKYOmIFmfjMyUTM68U1iJ06o99sA3d+xsHbQaFRJjooIYlRfeltAD5/vFF813L2hnSJOT9+Yt5ygsWfbsVZaYf7Fb0ePZTrJAKFFzTOCJiIia0Wv0AIA6Wx3iouLcbqu0VGJ079GYcMGEUIRG1CZ5Bt77EnoAOP5rHXr10EEQQjTz7uRtCb1TxmS5VdyzWUCPdOCqufKyeb5ZFhE6O+FWJ7NAKFFzXEJPRETUjE6tAwCPZfSSJKGitgImvSkUYREpYtBqvM7A2x0Sdh06jT0/n0FMtBp2h7de60Gk6wnUmwGHw/vtKlGeob/gKqD/aCbvEURpwq0fMQJqkwnw9WaSIEBtMkGfndWJ0RFFPibwREREzejV8gx880r0AFDdUI16ez1MMUzgKXwZdGqPPfBF+8tx5fKPccvaz/HNcTO+PlaNK5d/jKL95SGKEvISekhAQ7X32xst8gy9oXcwo6JOoM/OUpSYx+QMh3HRQp9jAMC4aCF7wRO1wASeiIioGecS+uYJvN1hx5ajWwAAZ+vPwu5gVWQKTwatBjXNltAX7S/Hva+Uory63m1cRXU97n2lNHRJvK6H/LllITsn5/53Q1owoqFOJIji+cS8ZRLfIjE35OWh19OrIfbo4TZMbTSi19OrYcjLC0LERJGFCTwREVEzzhn4OqtcrbvkSAnyN+bj0V2PAgAe++Ix5G/MR8mRklCFSOSToVkRO7tDwpJ3y+Btsbzz3JJ3y0KznF7bQ/7sax+8+Zj8Ob5XUMKhzuVMzNVGo9t5b4m5IS8PiX+4DxBFpD31JPquX48BW0qYvBP5wASeiIioGdceeJsFJUdKULitEJWWSrcxVZYqFG4rZBJPYad5Ebvdh894zLw3JwEor67H7sNnghRdM7qe8mdvleiB8zPwcZyBj1SGvDwM2FKC2HHXQJ2W1mpibq86CY3JhPhJkxAzIofL5olawQSeiIioGecS+nPWc3hi9xOQvMxfOs8t372cy+kprBi0GpxrsMFmd6Cqxnfy3pzScZ2qtSX0Djvw804gKg449qV8TBFJEEVEDxoESFKribm1skLeN09EbWICT0RE1EyUSu6P/f6h9z1m3puTIKHCUoHSqtJghUbUJoNO7hB8rsGGlDitovsoHdep1HoAAnBwC3D40/NJetkmYPUQYN8rQGMNsP46+bhsU/BjpE4hGuLhqPZRrLCJraISmhbL7YnIOybwRERETUqOlGDiOxMBANuPb1d0n5OWzu15TNQRsVFyAv926XE4HBJMBi18dXwXAKTGa5HTPyFo8QGQk/FnfgNAkhN1Z5Je/GfgjWnnl887mcvl80ziI5IYHw+HxQLJ6tne0Ikz8ETKqUMdABERUThw7nf3tmS+Ncl6ZT2PiQKtaH85/uvf+wEAf3mvDADQQ6+BBDlZb35lO5P6xZMyIKp8pfgBULZJTsZb/pyZTwCfPePjTk1fQdECYPBE9oSPMGK8AQBgN5uhTkx0u02y21G7Zw+sx09AamiAZLdz/ztRGzgDT0RE3Z7dYfe5390XAQJMehMyUzIDGBmRMs52cafONbqdr7bIs57xeo3beVO8Fmtuz0TBkNSgxQiHHSiaD4/kXREJMB8HjnzW2VFRgInx8QAAe4tl9ObiYhwcl4tfps8AbDacffVVHByXC3NxcQiiJIocnIEnIqJur7SqtNX97i0JTfOX83PmQ+RsIIVYW+3iBABatQoj+vfEWYsVSyYPQU7/hODOvANy8t1yeby/zin/OaXwoDI0zcA3S+DNxcU4PnsOILlftbbKSvk8e8AT+cQZeCIi6vb83cdu1Buxaswq5KbnBigiIuWUtIurMDegpt6OQcY4jLwwMfjJO9A5yXcsC51FGjG+B4DzCbxkt6Ny6TKP5F2+UT5XuXQZJDu7DxB5wxl4IiLq9pTuYy/oV4ApF01BZkomZ94pbChtA/erpRGJMT0DHE0rOpR8C4AhDUgf1WnhUHA498A7zGYAgGXPXtgqKnzfQZJgq6iAZc9exIzICUaIRBGFM/BERNTtZaZkwqg3upbG+3LL4Fsw3DScyTuFFaVt4Gob7egZExXgaFqRPkpOwtv4OfPUNL7gCRawi0AqrRZCdDTsv8oz8LaTylY8KR1H1N0wgSciom5PVIlYkLMAADyS+ObHPbUhnL0k8iGnfwJS49tuF3eu3orEUCbwKhEoWN4squYE+WPUA4DevVI5DGnAlJeBjMlBCJICQTQYXEvo1cnKVjwpHUfU3TCBJyIiApCbnotVY1YhRZ/idt6oN+LOIXcCAOKj40MRGlGrRJWAxZMyAHhPiwFgXv5FsEsI7Qw8ICfhU14GDC2q3zuT9Ly/AuP/Kp+7fg0w/T1gzjdM3iOc2CMe9qYl9PrsLLnnu+DjLSdBgNpkgj47K4gREkUOJvBERERNctNz8dHvPsLYPmORFpOGF/NfRNHvitDf0B8AYIgyhDhCIu8KhqRize2ZMMW7L6d3tovL7CuvHkkIdQIPyMn4nP3A8JmAGO2ZpNdXAxo9MPRWoP9oLpvvAlTx8bBX/woAEEQRxkUL5RtaJvFNx8ZFC9kPnsgHJvBERETNiCoRfeP6IkqMcu13NzeaEaOJgVrF2q8UvgqGpGLH/GuQkWpAZt8e2DDzcuyYfw0KhqTijEXuDx8WCTwgJ+WmSwF7A5B+hXuSXncG0CWELjbqdKIhHo5qs+vYkJeHXk+vhtroXthQbTSiF1vIEbWKf4kQERG1oNPoYLFZXMfVDdWIj+LyeQp/okqQ98MLwMgLz+8lP3OuKYHXh0kCDwBRMfJnqwWIjj1/vu4soGO9ia5EjI9H4+HDbucMeXmIGzcOB666Cvqhw5AwbRr02VmceSdqA2fgiYiIWtCKWtTbzrfmqm6o5v53ihi6KBGWRvce2s4Z+JDvgW8uqilpb6x1P285A+iZwHcVkt0Oh8WCxhMnUPvFbrf+7oIoAnX10GdnI2ZEDpN3IgWYwBMREbWgU+tQZ6tzHZsbzTBEc/87RQa9twS+thFxWjU0Yhj96Rellz83nnM/zxn4LsNcXIyD43JR89FHsFdV4ej06Tg4Lhfm4mIAgNTYCIfFArFHj9AGShRBwui3OBERUXjQqXWwOqywOWwA5Bl4FrCjSKGPUqOuWQJvd0j49ng1otUq7Dp0GnaHFMLommm+hL457oHvEszFxTg+ew5sFRVu522VlTg+ew7MxcWw/forALlKPREpwwSeiIioBa1aruTdYG8AAFQ3cgk9RQ5dlAiLVX7zqWh/Oa5c/jHe/bocp8414pa1n+PK5R+jaH95iKOE7yX0nIGPeJLdjsqlywDJy5tFTecqly6D7fRpAIAY3yOI0RFFNibwRERELejUOgBwLaNnETuKJHqNiLpGO4r2l+PeV0pRXl3vdntFdT3ufaU09Em8cwa+5RJ6y1lAzxn4SGbZs9dj5t2NJMFWUQHL7t0AwCX0RH5gAk9ERNSCcwbemcCbG8ycgaeIoYsSUdtgw5J3y+Btsbzz3JJ3y0K7nF7j3APfbAbebgUaazgDH+FsJ08qGmc9Ib+JJPbsEcBoiLoWJvBEREQtOGfg6231sDvsqLHWcA88RQx9lBp1VofHzHtzEoDy6nrsPnwmeIG15FpC32wPfN1Z+TP3wEc0dXKysoGiChAEiAb+fiVSigk8ERFRC1rx/Ax8TWMNAHAGniKGPkp5K66qGt9JfsCpowCVxn0JvSuB5wx8JNNnZ0FtMgGC4H2AIEBtMkGMj4fKYGD7OCI/MIEnIiJqwTkDb7Fa8OnxTwEA5bXlsDvsrd2NKCzo/EjgU+K0AYxEgagY9yX0lqYVAdwDH9EEUYRx0cKmgxZJfNOxcdFCOMxmiPF8c5TIH0zgiYiIWnAm8PO2z8OiHYsAAE9++STyN+aj5EhJKEMjapNzBj45Lho+5j8hAEiN1yKnf4gT5eYJvMMO/LxT/vfJA/IxRSxDXh56Pb0aaqPR7bzaaESvp1fDkJcHe3U1C9gR+YkJPBERUQufl38OAPi14Ve381WWKhRuK2QST2HNmcDfe/WFAOCRxDuPF0/KgKjyleIHSVSM3Ae+bBOwegiw9a/y+Tdul4/LNoU2PuoQQ14eBmwpQZ+X1gEAet51JwZsKYEhLw8AYP/1V/aAJ/ITE3giIqJm7A47Vpeu9nqb1FS/e/nu5VxOT2FLp1EDADLTe2LN7ZlIiot2u90Ur8Wa2zNRMCQ1FOG5i4oBKr8F3pgGmE+432Yul88ziY9ogigi9vLLIeh0iDKZ3Pa7ywl8j9AFRxSB1KEOgIiIKJyUVpWiylLl83YJEiosFSitKsVw0/AgRkakjHMG3tJoQ8GQVPRNiMGEZz7FnNyBGNE/ETn9E0I/8+6kiQF++Rzw2fBOAIoWAIMnAioWOotkKp0ODsv5jgOS3Q7riXKotDrUfrEb+uwsFrMjUoAz8ERERM2ctCjrX6x0HFGwORP4ukZ5lUidVf587ZBUjLwwMXySdwCwNwDWulYGSID5OHDks6CFRIGh0uvhsMivtbm4GAfH5cJ24gRqd+zA0enTcXBcLszFxSGOkij8MYEnIiJqJlmvrH+x0nFEwaZzzcDLiXttgw0AEBMdwbOb5ypDHQF1kJzAW2AuLsbx2XNgq6hwu91WWYnjs+cwiSdqAxN4IiKiZjJTMmHUG33eLkCASW9CZkpmEKMiUk6ncZ+BtzTKCXxsdBjunNT2UDYu1vfPJEUGlU4Hu6UWlUuXAZKXLRNN5yqXLoNkZ40RIl+YwBMRETUjqkQsyFng9TahqX73/Jz5ELkfl8KUWlQhSlS5EvdzDXIypI8KwwQ+4QJApYZnrXwnATD0AtJHBTMqCgBVjB7WY8c8Zt7dSBJsFRWw7NkbvMCIIgwTeCIiohZy03ORbkiHVtS6nTfqjVg1ZhVy03NDFBmRMrooEXVWBwB5CX2UqEKUOgz/7IuOazYL76PhXcETLGDXBQg6PRw15xSNtZ1kjREiX8LwrVgiIqLQS41JxUU9L8JnJz7DFWlXYOrgqchMyeTMO0UEfZSIOtcMvC18979H6QHJAUx5GSia795KzpAmJ+8Zk0MXH3UalV4POByKxqqTWWOEyBcm8ERERF5o1VpYbBbUWmtxedrlbBlHEUUXJbqK2FkabYgJx/3vABAVC1gtcpI+eCLw1ECg3xVAzv+Tl83zDbMuQ6XTAWo11CYTbJWV3vfBCwLURiP02VnBD5AoQoThWipP27ZtgyAIXj++/PJL17ijR49i0qRJiImJQVJSEh544AE0NjaGMHIiIopUOlGHitoKSJDQU9sz1OEQ+UUfJcJidVaht4dnATsAiIoBbPWA3SYn6w4r0Hs40H80k/cuRqXXQ6qrg3HRQu8DBHnLhHHRQvaDJ2pFRCTwo0aNQnl5udvHPffcg379+iE7OxsAYLfbMXHiRNTW1mLHjh147bXXsHHjRjz44IMhjp6IiCKRTqPDiXPyct4EbUKIoyHyj16jdlWhP9dgc/WGDztRMfJna608I9t4DoiODW1MFBDONnKGvDz0eno1VAaD2+1qoxG9nl4NQ15eiCIkigxh+nasu6ioKJhMJtex1WrFpk2bcP/990NoereuuLgYZWVl+OWXX5CWlgYAWLlyJWbMmIHHH38chha/JIiIiFqjFeUl9AATeIo88hJ6eQ98bUMYL6HXNCXwjbWAGCXvh49iAt8VqfQ6OOrqAACGvDxYyytQtWIF0pYtgzo5GfrsLM68EykQpr/NW7dp0yacOnUKM2bMcJ3btWsXhgwZ4kreASA/Px8NDQ3Yu3cvxo4d6/WxGhoa0NDQ4Do2m80A5DcJrFZrYL4ACjvO15qvOYUzXqfBFa2Kdv07Tozj910BXqPhQ6sWUNtgg9Vqxbl6K/RRYli+LoIYDTUAq6UacAAaADZVNKQAxcprNHSk6Gg4amtd33ubpRaq2Fjo8+UZd5vDobjIXVfGa7T7UvqaR2QC/8ILLyA/Px99+vRxnauoqIDRaHQb17NnT0RFRaGilX6Ty5Ytw5IlSzzOFxcXQ6/Xd17QFBE2b94c6hCI2sTrNDiO1h8FAIgQsX3zdteKL2obr9HQO3NShVP1Aj744AMcqxSRrJXwwQcfhDosD/G1P2EMgB//bwUsUUnIBvDFvjKcOhTY5+U1GnyGH3+EyWbDB5s2AWo1Er/+BgYgLK/LcMBrtPuxWCyKxoU0gX/00Ue9Js/Nffnll6597gBw7NgxfPTRR3jjjTc8xnr740qSpFb/6Fq4cCEKCwtdx2azGX369EFeXh6X3XcjVqsVmzdvxvjx46HRaEIdDpFXvE6D6/R3p7Hlqy1I0CVg4sSJoQ4nIvAaDR9fvFuGc0erMWHCSDz/02e4qF9PTJhwcajDciN8/x7EoucBABnlb7nOX94nCo5REwLynLxGQ+ecRoOKN95E/tVjIMYbcPLrr1H3yy+YMCEwr3Wk4jXafTlXgrclpAn8/fffj5tvvrnVMf369XM7XrduHRITEzF5sntPUJPJhC+++MLt3NmzZ2G1Wj1m5puLjo5GdHS0x3mNRsMfmm6IrztFAl6nwRHbVEgrQZvA77efeI2GXqw2CvU2BzQaDWob7YjTRYXXa1K2Cdh4JwDPVmLi1r9ATB4Y0P7vvEaDTxMXBwAQbVb5e19fDzEmhq+DD7xGux+lr3dIE/ikpCQkJSUpHi9JEtatW4dp06Z5fIEjR47E448/jvLycqSmpgKQl8FHR0cjK4u9JImIyD9atRYA2EKOIpJOc76InaXRHl5F7Bx2oGg+vCXvMgEoWiD3hWcruS5D1bQ11dG0TNhhsUAVw+2qRP6KiDZyTh9//DEOHz6Mu+++2+O2vLw8ZGRk4I477sBXX32FLVu2YO7cuZg5cyaXwhMRkd+iRXl1Vr2tHl9WfAm7wx7iiIiUsTsknKypR3WdFbsOnUZNvTW8+sAf+Qwwn2hlgASYj8vjqMtQ6XQAAIelrumzBQLrTRH5LaIS+BdeeAGjRo3CxRd77uESRRHvv/8+tFotrrjiCkyZMgXXX389VqxYEYJIiYgokpUcKcFjnz8GANh3ch/u+ugu5G/MR8mRkhBHRtS6ov3luHL5x/jX7l9Qb3XglrWfw2qX8NPJc6EO7bxzlZ07jiLC+Rn4WgCAVGtxnSMi5cLo7di2/etf/2r19r59++K9994LUjRERNQVlRwpQeG2QkgtlvdWWapQuK0Qq8asQm56boiiI/KtaH857n2l1OvC9PW7jmDkhYkoGJIa9Lg8xPquTdSucRQRnLPtUt35GXiVjgk8kb8iagaeiIgokOwOO57Y/YRH8g7AdW757uVcTk9hx+6QsOTdMp+7ygFgybtlsDtaGxEk6aMAQxoAX12CBMDQSx5HXYbXPfCcgSfyGxN4IiKiJqVVpai0+F62K0FChaUCpVWlQYyKqG27D59BeXV9q2PKq+ux+/CZIEXUCpUIFCxvOvCRxBc8wQJ2XYy3PfBM4In8xwSeiIioyUnLyU4dRxQsVTWtJ+/+jgu4jMnAlJcBQ4sl/Sq1fD6ALeQoNARRhBAdfX4Gvq6OCTxROzCBJyIiapKsT+7UcUTBkhKn7dRxQZExGZizHxhyI6BPBAbkAqbLmLx3YSq9Hg6LBZIksY0cUTsxgSciImqSmZIJo94IwceyXgECTHoTMlMygxwZUety+icgNV7b2q5ypMZrkdM/IZhhtU0lAmnDAGs9EB0HaONCHREFkEqng6POAqmxEbDbOQNP1A5M4ImIiJqIKhELchYAgEcS7zyenzMfIvfmUpgRVQIWT8oA4H1XuQTgzxMvhqjyleKHUEwSYK0Fak8BUbGhjoYCSBUjz8A7l9EzgSfyHxN4IiKiZnLTc7FqzCqk6FPczhv1RraQo7BWMCQVa27PhCne+zL5v77/HYr2lwc5KgX0SfLnX48AUTGhjYUCRrLb4XA40PDDD6jd+RkAJvBE7RFRfeCJiIiCITc9F2P7jEVpVSlOWk4iWZ+MzJRMzrxT2CsYkgqHA7jvX56dEiqq63HvK6VYc3tmePSDd4pJlD9XH2cC30WZi4tRuXQZbBUVsB76CZYvdgMA6vbvR8wotgsk8gcTeCIiIi9ElYjhpuGhDoPIL3aHhL++X+b1Ngny8vol75ZhfIYpfJbTO2fgJTuX0HdB5uJiHJ89B5Akj9tOrvobovr1gyEvL/iBEUUoLqEnIiIi6iLa6gcvIYz6wTvFJJ3/NxP4LkWy21G5dJnX5N2pcukySHZ7EKMiimxM4ImIiIi6iIjrBw8AGh2gaVo6zyX0XYplz17YKipaHWOrqIBlz94gRUQU+ZjAExEREXUREdkPHji/D54JfJdiO3myU8cRERN4IiIioi4jYvvBO/fBR7MPfFeiTk7u1HFExASeiIiIqMtorR+883jxpIzwKWAHAA47IDT9SXr2Z/mYugR9dhbUJhMg+LjeBAFqkwn67KzgBkYUwZjAExEREXUhvvrBm+K14ddCrmwTsHoIcHyPfPzxX+Xjsk2hjYs6hSCKMC5a2HTgPYk3LloIQWSLTiKl2EaOiIiIqIspGJKK8Rkm7D58BlU19UiJk5fNh9XMe9km4I1pkGvjN2Mul89PeRnImByS0KjzGPLygKdXu/rAu4giev1tFVvIEfmJCTwRERFRFySqBIy8MDHUYXjnsANF8+GRvANwdawvWgAMngioODsb6Qx5eYgbNw6WPXtRvmgRrMePI3rQICbvRO3AJfREREREFFxHPgPMJ1oZIAHm4/I46hIEUUTMiBzoLvsNAECMYccBovZgAk9EREREwXWusnPHUcTQ9O0LALDXnkPtF7sh2Vm0kMgfTOCJiIiIKLhijZ07jiKCubgYZze8BgBo+O57HJ0+HQfH5cJcXBziyIgiBxN4IiIiIgqu9FGAIQ2eze6cBMDQSx5HXYK5uBjHZ8+Bo7ra7bytshLHZ89hEk+kEBN4IiIiIgoulQgULG868NGxvuAJFrDrIiS7HZVLlwGSl6KFTecqly7jcnoiBZjAExEREVHwZUyWW8UZWvSlN6SxhVwXY9mz172FXEuSBFtFBSx79gYvKKIIxTZyRERERBQaGZPlVnFHPpML1sUa5WXznHnvUmwnT3bqOKLujAk8EREREYWOSgT6jw51FBRA6uTkTh1H1J1xCT0REREREQWMPjsLapMJEHwULRQEqE0m6LOzghsYUQRiAk9ERERERAEjiCKMixY2HbRI4puOjYsWQhC5dYKoLUzgiYiIiIgooAx5eej19GqojUa382qjEb2eXg1DXl6IIiOKLNwDT0REREREAWfIy0PcuHFyVfqTJ6FOToY+O4sz70R+YAJPRERERERBIYgiYkbkhDoMoojFJfREREREREREEYAJPBEREREREVEEYAJPREREREREFAGYwBMRERERERFFACbwRERERERERBGACTwRERERERFRBGACT0RERERERBQBmMATERERERERRQAm8EREREREREQRgAk8ERERERERUQRgAk9EREREREQUAZjAExEREREREUUAJvBEREREREREEUAd6gDCjSRJAACz2RziSCiYrFYrLBYLzGYzNBpNqMMh8orXKYU7XqMU7niNUrjjNdp9OfNPZz7qCxP4FmpqagAAffr0CXEkRERERERE1J3U1NQgPj7e5+2C1FaK3804HA6cOHECcXFxEAQh1OFQkJjNZvTp0we//PILDAZDqMMh8orXKYU7XqMU7niNUrjjNdp9SZKEmpoapKWlQaXyvdOdM/AtqFQq9O7dO9RhUIgYDAb+sqSwx+uUwh2vUQp3vEYp3PEa7Z5am3l3YhE7IiIiIiIiogjABJ6IiIiIiIgoAjCBJwIQHR2NxYsXIzo6OtShEPnE65TCHa9RCne8Rinc8RqltrCIHREREREREVEE4Aw8ERERERERUQRgAk9EREREREQUAZjAExEREREREUUAJvBEREREREREEYAJPHUbzz//PPr37w+tVousrCx8+umnrY5vaGjAww8/jPT0dERHR+PCCy/Eiy++GKRoqTvy9xp99dVXcdlll0Gv1yM1NRV33nknTp8+HaRoqbvZvn07Jk2ahLS0NAiCgH//+99t3ueTTz5BVlYWtFotLrjgAvz9738PfKDUbfl7jb799tsYP348kpOTYTAYMHLkSHz00UfBCZa6pfb8HnXauXMn1Go1hg4dGrD4KDIwgadu4fXXX8ecOXPw8MMP46uvvsLo0aNx7bXX4ujRoz7vM2XKFGzZsgUvvPACfvjhB2zYsAGDBw8OYtTUnfh7je7YsQPTpk3D3XffjW+//RZvvvkmvvzyS9xzzz1Bjpy6i9raWlx22WV47rnnFI0/fPgwJkyYgNGjR+Orr77CokWL8MADD2Djxo0BjpS6K3+v0e3bt2P8+PH44IMPsHfvXowdOxaTJk3CV199FeBIqbvy9xp1qq6uxrRp0zBu3LgARUaRhG3kqFsYMWIEMjMzsWbNGte5iy++GNdffz2WLVvmMb6oqAg333wzfvrpJyQkJAQzVOqm/L1GV6xYgTVr1uDQoUOuc88++yyefPJJ/PLLL0GJmbovQRDwzjvv4Prrr/c5Zv78+di0aRO+++4717lZs2bhP//5D3bt2hWEKKk7U3KNenPJJZdg6tSpeOSRRwITGFETf67Rm2++GQMHDoQoivj3v/+Nffv2BTw+Cl+cgacur7GxEXv37kVeXp7b+by8PHz22Wde77Np0yZkZ2fjySefRK9evTBo0CDMnTsXdXV1wQiZupn2XKOjRo3CsWPH8MEHH0CSJFRWVuKtt97CxIkTgxEyUZt27drlcU3n5+djz549sFqtIYqKyDeHw4Gamhq+cU9hZd26dTh06BAWL14c6lAoTKhDHQBRoJ06dQp2ux1Go9HtvNFoREVFhdf7/PTTT9ixYwe0Wi3eeecdnDp1Cvfddx/OnDnDffDU6dpzjY4aNQqvvvoqpk6divr6ethsNkyePBnPPvtsMEImalNFRYXXa9pms+HUqVNITU0NUWRE3q1cuRK1tbWYMmVKqEMhAgD8+OOPWLBgAT799FOo1UzbSMYZeOo2BEFwO5YkyeOck8PhgCAIePXVV5GTk4MJEyZg1apVeOmllzgLTwHjzzVaVlaGBx54AI888gj27t2LoqIiHD58GLNmzQpGqESKeLumvZ0nCrUNGzbg0Ucfxeuvv46UlJRQh0MEu92OW2+9FUuWLMGgQYNCHQ6FEb6VQ11eUlISRFH0mMmsqqrymB1ySk1NRa9evRAfH+86d/HFF0OSJBw7dgwDBw4MaMzUvbTnGl22bBmuuOIKzJs3DwDwm9/8BjExMRg9ejQee+wxzm5SyJlMJq/XtFqtRmJiYoiiIvL0+uuv4+6778abb76J3NzcUIdDBACoqanBnj178NVXX+H+++8HIE8wSZIEtVqN4uJiXHPNNSGOkkKBM/DU5UVFRSErKwubN292O79582aMGjXK632uuOIKnDhxAufOnXOdO3DgAFQqFXr37h3QeKn7ac81arFYoFK5/woXRRHA+VlOolAaOXKkxzVdXFyM7OxsaDSaEEVF5G7Dhg2YMWMG/vWvf7GGCIUVg8GAb775Bvv27XN9zJo1CxdddBH27duHESNGhDpEChHOwFO3UFhYiDvuuAPZ2dkYOXIk/vGPf+Do0aOu5cYLFy7E8ePH8fLLLwMAbr31Vvz1r3/FnXfeiSVLluDUqVOYN28e7rrrLuh0ulB+KdRF+XuNTpo0CTNnzsSaNWuQn5+P8vJyzJkzBzk5OUhLSwvll0Jd1Llz53Dw4EHX8eHDh7Fv3z4kJCSgb9++HtforFmz8Nxzz6GwsBAzZ87Erl278MILL2DDhg2h+hKoi/P3Gt2wYQOmTZuGp59+GpdffrlrxYhOp3NbgUfUWfy5RlUqFYYMGeJ2/5SUFGi1Wo/z1M1IRN3Ef//3f0vp6elSVFSUlJmZKX3yySeu26ZPny5dffXVbuO/++47KTc3V9LpdFLv3r2lwsJCyWKxBDlq6k78vUafeeYZKSMjQ9LpdFJqaqp02223SceOHQty1NRdbN26VQLg8TF9+nRJkrxfo9u2bZOGDRsmRUVFSf369ZPWrFkT/MCp2/D3Gr366qtbHU/U2drze7S5xYsXS5dddllQYqXwxT7wRERERERERBGAe+CJiIiIiIiIIgATeCIiIiIiIqIIwASeiIiIiIiIKAIwgSciIiIiIiKKAEzgiYiIiIiIiCIAE3giIiIiIiKiCMAEnoiIiIiIiCgCMIEnIiIiIiIiasX27dsxadIkpKWlQRAE/Pvf//b7Md544w0MHToUer0e6enpeOqpp/x+DCbwREREFLYaGxsxYMAA7Ny5M2QxzJ07Fw888EDInp+IiEKvtrYWl112GZ577rl23f/DDz/EbbfdhlmzZmH//v14/vnnsWrVKr8fjwk8ERFRkMyYMQOCIHh8HDx4MNShha1//OMfSE9PxxVXXOF2fuvWrbjuuuuQnJwMrVaLCy+8EFOnTsX27dtdY7Zt2wZBEPDrr796PG6/fv2wevVqRTE89NBDWLduHQ4fPtyRL4WIiCLYtddei8ceeww33HCD19sbGxvx0EMPoVevXoiJicGIESOwbds21+3//Oc/cf3112PWrFm44IILMHHiRMyfPx/Lly+HJEmK42ACT0REFEQFBQUoLy93++jfv7/HuMbGxhBEF36effZZ3HPPPW7nnn/+eYwbNw6JiYl4/fXX8d133+Gf//wnRo0ahT/96U+dHkNKSgry8vLw97//vdMfm4iIuoY777wTO3fuxGuvvYavv/4aN910EwoKCvDjjz8CABoaGqDVat3uo9PpcOzYMRw5ckTx8zCBJyIiCqLo6GiYTCa3D1EUMWbMGNx///0oLCxEUlISxo8fDwAoKyvDhAkTEBsbC6PRiDvuuAOnTp1yPV5tbS2mTZuG2NhYpKamYuXKlRgzZgzmzJnjGuNtr16PHj3w0ksvuY6PHz+OqVOnomfPnkhMTMRvf/tb/Pzzz67bZ8yYgeuvvx4rVqxAamoqEhMT8Yc//AFWq9U1pqGhAQ899BD69OmD6OhoDBw4EC+88AIkScKAAQOwYsUKtxj2798PlUqFQ4cOef1elZaW4uDBg5g4caLr3NGjRzFnzhzMmTMH69evxzXXXIP+/ftj1KhRmD17Nvbs2aP0pXB56aWXvK6MePTRR11jJk+ejA0bNvj92ERE1PUdOnQIGzZswJtvvonRo0fjwgsvxNy5c3HllVdi3bp1AID8/Hy8/fbb2LJlCxwOBw4cOOBaCVZeXq74uZjAExERhYn169dDrVZj586d+J//+R+Ul5fj6quvxtChQ7Fnzx4UFRWhsrISU6ZMcd1n3rx52Lp1K9555x0UFxdj27Zt2Lt3r1/Pa7FYMHbsWMTGxmL79u3YsWMHYmNjUVBQ4LYSYOvWrTh06BC2bt2K9evX46WXXnJ7E2DatGl47bXX8Mwzz+C7777D3//+d8TGxkIQBNx1112uP2KcXnzxRdcfOt5s374dgwYNgsFgcJ3buHEjrFYrHnroIa/3EQTBr68dAKZOneq2ImLDhg1Qq9Vuy/ZzcnLwyy+/+DVLQkRE3UNpaSkkScKgQYMQGxvr+vjkk09cb1LPnDkT999/P6677jpERUXh8ssvx8033wwAEEVR8XOpA/IVEBERkVfvvfceYmNjXcfXXnst3nzzTQDAgAED8OSTT7pue+SRR5CZmYmlS5e6zr344ovo06cPDhw4gLS0NLzwwgt4+eWXXTP269evR+/evf2K6bXXXoNKpcL//u//uhLgdevWoUePHti2bRvy8vIAAD179sRzzz0HURQxePBgTJw4EVu2bMHMmTNx4MABvPHGG9i8eTNyc3MBABdccIHrOe6880488sgj2L17N3JycmC1WvHKK6+0WoH3559/Rlpamtu5AwcOwGAwwGQyuc5t3LgR06dPdx3v2rULl156qevY2/fDYrG4/q3T6aDT6QDIsyj3338/li5d6vqeAkCvXr1cMaWnp/uMmYiIuh+HwwFRFLF3716PZNz5f74gCFi+fDmWLl2KiooKJCcnY8uWLQDkuixKMYEnIiIKorFjx2LNmjWu45iYGNe/s7Oz3cbu3bsXW7dudUv4nQ4dOoS6ujo0NjZi5MiRrvMJCQm46KKL/Ipp7969OHjwIOLi4tzO19fXuy1vv+SSS9z+MElNTcU333wDANi3bx9EUcTVV1/t9TlSU1MxceJEvPjii8jJycF7772H+vp63HTTTT7jqqur89gvCHjOsufn52Pfvn04fvw4xowZA7vd7nb7p59+6vG1jRkzxuNxq6urcd111+Haa6/FvHnz3G5zJvjNE38iIiIAGDZsGOx2O6qqqjB69OhWx4qi6HpTeMOGDRg5ciRSUlIUPxcTeCIioiCKiYnBgAEDfN7WnMPhwKRJk7B8+XKPsampqa7COG0RBMGjwm3zvesOhwNZWVl49dVXPe6bnJzs+rdGo/F4XIfDAeB8gtuae+65B3fccQf+9re/Yd26dZg6dSr0er3P8UlJSa43CJwGDhyI6upqVFRUuGbhY2NjMWDAAKjV3v+s6d+/P3r06OF2ruVYu92OqVOnwmAwYO3atR6PcebMGQDu3w8iIuo+zp0759Y15vDhw9i3bx8SEhIwaNAg3HbbbZg2bRpWrlyJYcOG4dSpU/j4449x6aWXYsKECTh16hTeeustjBkzBvX19Vi3bh3efPNNfPLJJ37FwT3wREREYSozMxPffvst+vXrhwEDBrh9ON8I0Gg0+Pzzz133OXv2LA4cOOD2OMnJyW4Fcn788Ue3meTMzEz8+OOPSElJ8Xie+Ph4RbFeeumlcDgcrf4hMmHCBMTExGDNmjX48MMPcdddd7X6mMOGDcP333/v9ubDjTfeCI1G4/VNjY7405/+hG+++QbvvPOO11n//fv3Q6PR4JJLLunU5yUiosiwZ88eDBs2DMOGDQMAFBYWYtiwYXjkkUcAyFvPpk2bhgcffBAXXXQRJk+ejC+++AJ9+vRxPcb69euRnZ2NK664At9++y22bduGnJwcv+LgDDwREVGY+sMf/oC1a9filltuwbx585CUlISDBw/itddew9q1axEbG4u7774b8+bNQ2JiIoxGIx5++GGoVO7vz19zzTV47rnncPnll8PhcGD+/Plus+m33XYbnnrqKfz2t7/FX/7yF/Tu3RtHjx7F22+/jXnz5inaU9+vXz9Mnz4dd911F5555hlcdtllOHLkCKqqqlxF90RRxIwZM7Bw4UIMGDDAbem/N2PHjkVtbS2+/fZbDBkyBADQt29frFy5ErNnz8aZM2cwY8YM9O/fH2fOnMErr7zieh5/rFu3Ds8//zzeeecdqFQqVFRUAICrCBEgL8MfPXq0opUGRETU9YwZM6bVfu0ajQZLlizBkiVLvN6elJSEXbt2dTgOzsATERGFqbS0NOzcuRN2ux35+fkYMmQIZs+ejfj4eFeS/tRTT+Gqq67C5MmTkZubiyuvvBJZWVluj7Ny5Ur06dMHV111FW699VbMnTvXbem6Xq/H9u3b0bdvX9xwww24+OKLcdddd6Gurs6tAnxb1qxZgxtvvBH33XcfBg8ejJkzZ6K2ttZtzN13343GxsY2Z98BIDExETfccIPH0v4//vGPKC4uxsmTJ3HjjTdi4MCBmDBhAg4fPoyioiK3AnZKfPLJJ7Db7Zg8eTJSU1NdH83b3m3YsAEzZ87063GJiIg6myC19jYCERERRZwxY8Zg6NChrv6y4WTnzp0YM2YMjh07BqPR2Ob4b775Brm5uV6L7AXL+++/j3nz5uHrr7/2uc+eiIgoGDgDT0RERAHX0NCAgwcP4s9//jOmTJmiKHkH5L31Tz75JH7++efABtiK2tparFu3jsk7ERGFHP8nIiIiooDbsGED7r77bgwdOhT//Oc//bpv8x7voeDcw09ERBRqXEJPREREREREFAG4hJ6IiIiIiIgoAjCBJyIiIiIiIooATOCJiIiIiIiIIgATeCIiIiIiIqIIwASeiIiIiIiIKAIwgSciIiIiIiKKAEzgiYiIiIiIiCIAE3giIiIiIiKiCPD/AVjYIEox2gaGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=[12, 6])\n",
    "plt.plot(freq,y11,'-o',lw=1)\n",
    "plt.plot(freq,y11_1,'-o',lw=1)\n",
    "plt.plot(freq,y11_2,'-o',lw=1)\n",
    "plt.plot(freq,y11_3,'-o',lw=1)\n",
    "\n",
    "plt.xlabel(\"Frequency (GHz)\")\n",
    "plt.ylabel(\"Magnitude (log scale)\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_generatev2\n",
    "# code by Zhengcaizhi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import special\n",
    "import cmath\n",
    "from scipy.signal import argrelextrema\n",
    "import sawcom7 as sc\n",
    "\n",
    "def cacul(paras):\n",
    "    npiezo_1 = paras[0]\n",
    "    eta = paras[1]\n",
    "    e = paras[2]\n",
    "    alpha = paras[3]\n",
    "    c = paras[4]\n",
    "    k2 = paras[5]\n",
    "    vb = paras[6]\n",
    "    epsilon_0 = 8.8541878128e-12 #The permittivity of vacuum\n",
    "    pI = 3600E-9 #The period of IDT, normally is one wavelength\n",
    "    h = 0.08*pI #The thickness of IDT, Al or Al-Cu1%\n",
    "    W1 = 20*pI # Width of IDT (acoustic aperture), in m\n",
    "    m_ratio = 0.6 #The metallization ratio\n",
    "\n",
    "    eta_b = (eta+2*abs(e))/2\n",
    "    epsilon = npiezo_1*epsilon_0\n",
    "    x1 = np.cos(np.pi*m_ratio )\n",
    "    m1 = math.sqrt((1-x1)/2) \n",
    "    km1 = special.ellipk(m1, out=None)\n",
    "    p1 = 2*km1/np.pi\n",
    "    x2 = -np.cos(np.pi*m_ratio )\n",
    "    m2 = math.sqrt((1-x2)/2) \n",
    "    km2 = special.ellipk(m2, out=None)\n",
    "    p2 = 2*km2/np.pi\n",
    "    p_factor = p1/p2\n",
    "    freq = np.linspace(0.5E9, 1.5E9, 501)\n",
    "    # freq_mhz = freq/1e6\n",
    "    delta_v = - (eta**2)/2\n",
    "    k = abs(e)*(eta+abs(e)/2)\n",
    "    kb = -(abs(e)**2)*eta/(eta+2*abs(e))\n",
    "    delta_b = -((eta**2)-2*((abs(e)**2)))/4\n",
    "    omega = freq*2*np.pi\n",
    "    delta = omega/vb - 2*np.pi/pI - 1j*alpha\n",
    "\n",
    "    v_delta = []\n",
    "    for i in range(0,len(delta)):\n",
    "        v_delta_0 = eta_b/((cmath.sqrt(delta_b-delta[i]))+ eta_b)# wave velocity in m/s\n",
    "        v_delta.append(v_delta_0)\n",
    "    v_delta = np.array(v_delta)\n",
    "    omega = freq*2*np.pi\n",
    "    C = (W1*epsilon*p_factor)/pI ##To check\n",
    "    xi = []\n",
    "    for i in range(0,len(omega)):\n",
    "        xi_0 = c*cmath.sqrt((omega[i]*C*k2)/(pI*np.pi))\n",
    "        xi_0 = -1j*xi_0\n",
    "        xi.append(xi_0)\n",
    "    xi = np.array(xi)\n",
    "\n",
    "    lam1 = pI # Wavelength in m of SAW filters \n",
    "    c12 =  -1j*c*(k+kb*v_delta) # Reflectivity per unit length (~1.7% reflected per IDT spaced at lam/2)\n",
    "    a1 = -xi # The transduction coefficient\n",
    "    n1 = 100 # The number of IDT pairs\n",
    "    L1 = n1*lam1 # Length of total IDT the grating, in m\n",
    "    #W1 = 22*lam1 # Width of IDT (acoustic aperture), in m\n",
    "    #d = sc.delta(freq,v1,lam1) - 500j\n",
    "    Ct=n1*W1*epsilon # Static capacitance of total IDT\n",
    "    #d1 = sc.delta(freq,v1,lam1)\n",
    "    d1 = sc.thetau(c,delta,delta_v,kb,v_delta)\n",
    "    C1 = sc.C0(freq,Ct)\n",
    "    idt_ref_1 = sc.pmatrix(lam1,c12,a1,L1,d1,C1) #The P-Matrix of SAW resonator with refelection \n",
    "    # y11 = 20 * np.log10(abs(idt_ref_1.p33)/5)\n",
    "    y11 = (idt_ref_1.p33)/5\n",
    "    return y11\n",
    "def YtoZS(Y_COM, freq):\n",
    "    Y11 = 20*np.log10(abs(Y_COM))\n",
    "    Z_COM = 1/Y_COM\n",
    "    z = 50 \n",
    "    S11_COM = (1-Y_COM*z)/(1+Y_COM*z)\n",
    "\n",
    "    group_delay = -np.diff(np.unwrap(np.angle(S11_COM))) / np.diff(2 * np.pi * freq)\n",
    "    group_delay = np.concatenate(([group_delay[0]], group_delay))\n",
    "    Q_COM = 2*np.pi*(freq)*group_delay *abs(S11_COM)/(1-abs(S11_COM)**2)\n",
    "\n",
    "    DSP_COM = 10*np.log10(1-abs(S11_COM)**2)\n",
    "    return Y11, Z_COM.real, S11_COM.real, S11_COM.imag, Q_COM, DSP_COM\n",
    "results = []\n",
    "origin_paras = np.genfromtxt('D:\\\\data\\\\7p\\\\out/MP60.csv',delimiter=',')\n",
    "for i in range(0,len(origin_paras)):\n",
    "    # x = np.array([eta,e,alpha,c,k2,npiezo_1,vb,m_ratio])\n",
    "    # x = np.append(origin_paras[i], 0.6)\n",
    "    freq = np.linspace(0.5E9, 1.5E9, 501)\n",
    "    x = origin_paras[i]\n",
    "    y = cacul(x)\n",
    "    [Y0_Y11, Y0_Z, Y0_SR, Y0_SI, Y0_Q, Y0_D] = YtoZS(y, freq)\n",
    "    Inputs = np.stack((Y0_Y11, Y0_Z, Y0_SR, Y0_SI, Y0_Q, Y0_D),axis=-1)\n",
    "    results.append(Inputs)\n",
    "\n",
    "result = np.array(results)\n",
    "mu = result.reshape(-1,6).mean(axis = 0)\n",
    "sigma = result.reshape(-1,6).std(axis = 0)\n",
    "if (sigma == 0).all() != True:\n",
    "    result = (result - mu )/ sigma\n",
    "else:\n",
    "    result = result - mu\n",
    "\n",
    "file_path = 'D:\\\\data\\\\7p\\\\input2w/2w.npy'\n",
    "with open(file_path,'wb') as f:\n",
    "    np.save(f, result)\n",
    "file_path1 = file_path + 'musi.csv'\n",
    "with open(file_path1,'w') as f:\n",
    "    np.savetxt(f,mu)   \n",
    "    np.savetxt(f,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607520"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(result[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501, 6)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_o = result*sigma + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0=np.exp((result_o[0,:,0]/20)*np.log(10))\n",
    "y1=np.exp((result[0,:,0]/20)*np.log(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4940381 , -1.22807298, -0.7273927 , -0.5615703 , -0.61782843,\n",
       "       -0.92798947, -1.950366  , -1.35165828, -0.75722462, -0.5523599 ,\n",
       "       -0.57464965, -0.83729094, -1.63186739, -1.51385693, -0.79513863,\n",
       "       -0.54851926, -0.53705737, -0.75673186, -1.40400544, -1.74011796,\n",
       "       -0.84224273, -0.55015648, -0.5048445 , -0.68481992, -1.22580658,\n",
       "       -2.09667539, -0.90009817, -0.5574383 , -0.47786759, -0.62048044,\n",
       "       -1.07927356, -2.89400254, -0.97095881, -0.5706047 , -0.45603443,\n",
       "       -0.56292745, -0.95490401, -2.82425274, -1.05819414, -0.58999031,\n",
       "       -0.43929586, -0.51158093, -0.84708354, -2.02619827, -1.16709727,\n",
       "       -0.61605534, -0.4276413 , -0.46601149, -0.7522355 , -1.64124737,\n",
       "       -1.30659655, -0.64943058, -0.42109755, -0.42590208, -0.66795779,\n",
       "       -1.38238275, -1.49344518, -0.69098394, -0.41973109, -0.39102071,\n",
       "       -0.59257494, -1.18595592, -1.7650276 , -0.74192259, -0.42365398,\n",
       "       -0.36120062, -0.52488633, -1.027166  , -2.23816954, -0.80395609,\n",
       "       -0.43303388, -0.33632598, -0.46401525, -0.89375101, -4.19829487,\n",
       "       -0.87957133, -0.44810923, -0.31632173, -0.4093136 , -0.77875956,\n",
       "       -2.25785721, -0.97252706, -0.4692109 , -0.30114691, -0.36029882,\n",
       "       -0.67788417, -1.72213493, -1.08882099, -0.49679318, -0.29079107,\n",
       "       -0.31661057, -0.58828586, -1.40598932, -1.23880482, -0.53147825,\n",
       "       -0.28527374, -0.27797982, -0.5080088 , -1.17887114, -1.44260972,\n",
       "       -0.57412179, -0.28464668, -0.24420638, -0.43566151, -1.00056212,\n",
       "       -1.74807619, -0.62591309, -0.28899946, -0.21514216, -0.37023057,\n",
       "       -0.85330083, -2.3286052 , -0.6885346 , -0.29846838, -0.19067869,\n",
       "       -0.31096533, -0.72767296, -3.22372112, -0.76442999, -0.31324967,\n",
       "       -0.17073806, -0.25730304, -0.61809546, -1.97297642, -0.85728473,\n",
       "       -0.33361795, -0.15526676, -0.20881801, -0.52099619, -1.51886623,\n",
       "       -0.97296196, -0.35995194, -0.14423195, -0.16518579, -0.43396252,\n",
       "       -1.23104947, -1.12153729, -0.39277066, -0.13762023, -0.12615695,\n",
       "       -0.3552987 , -1.01794943, -1.32247288, -0.43278551, -0.13543858,\n",
       "       -0.09153746, -0.28377651, -0.8476547 , -1.621462  , -0.48097789,\n",
       "       -0.13771769, -0.06117358, -0.21848566, -0.70524143, -2.17917117,\n",
       "       -0.53871938, -0.14451749, -0.03494016, -0.15873894, -0.58252063,\n",
       "       -3.26500461, -0.60796722, -0.15593537, -0.01273166, -0.10400933,\n",
       "       -0.47451701, -1.89121447, -0.69160108, -0.17211712,  0.00554467,\n",
       "       -0.05388631, -0.3779863 , -1.42395354, -0.79404596, -0.19327167,\n",
       "        0.01997366, -0.00804446, -0.29070044, -1.13054175, -0.92253592,\n",
       "       -0.21969049,  0.03063552,  0.03378015, -0.21106735, -0.91384661,\n",
       "       -1.09002135, -0.25177392,  0.03760761,  0.07180821, -0.13791286,\n",
       "       -0.74065378, -1.32321959, -0.29006757,  0.04096518,  0.10623176,\n",
       "       -0.07034772, -0.59556116, -1.6930631 , -0.33531446,  0.04078109,\n",
       "        0.13722603, -0.00768218, -0.47015104, -2.55338072, -0.38853244,\n",
       "        0.03712508,  0.16495934,  0.05063161, -0.35931404, -2.30843809,\n",
       "       -0.45113425,  0.03006277,  0.18960134,  0.10504006, -0.25971214,\n",
       "       -1.56279799, -0.52512261,  0.01965489,  0.2113301 ,  0.1559179 ,\n",
       "       -0.16904125, -1.18756945, -0.61342634,  0.00595717,  0.23033822,\n",
       "        0.203591  , -0.08564018, -0.93123222, -0.72052167, -0.01097854,\n",
       "        0.24683843,  0.24835446, -0.00826691, -0.73436004, -0.8536912 ,\n",
       "       -0.03110109,  0.26106883,  0.29048766,  0.06403765, -0.57329077,\n",
       "       -1.0259151 , -0.05435393,  0.27329839,  0.33026707,  0.13203716,\n",
       "       -0.43616961, -1.26385631, -0.08066448,  0.28383305,  0.36797755,\n",
       "        0.19635851, -0.31620128, -1.63789829, -0.10992686,  0.29302288,\n",
       "        0.4039226 ,  0.25753305, -0.20912694, -2.49148728, -0.14197498,\n",
       "        0.30127092,  0.4384337 ,  0.31602642, -0.11210311, -2.26977331,\n",
       "       -0.17654107,  0.30904403,  0.47187935,  0.37226091, -0.02314189,\n",
       "       -1.53259327, -0.21319361,  0.31688619,  0.50467366,  0.42663238,\n",
       "        0.05919265, -1.16581257, -0.2512456 ,  0.32543445,  0.53728485,\n",
       "        0.47952295,  0.13595828, -0.91953481, -0.28962339,  0.33543733,\n",
       "        0.57024372,  0.53130987,  0.20793624, -0.73471892, -0.32668731,\n",
       "        0.34777503,  0.60415196,  0.58237007,  0.27568845, -0.5882639 ,\n",
       "       -0.36000567,  0.36348004,  0.6396901 ,  0.63307937,  0.33958318,\n",
       "       -0.46911569, -0.38611149,  0.38375569,  0.67762433,  0.68380376,\n",
       "        0.39979384, -0.37161833, -0.40032907,  0.40998913,  0.71881084,\n",
       "        0.73487845,  0.45626909, -0.29316399, -0.39684005,  0.44375452,\n",
       "        0.76419495,  0.7865666 ,  0.50866643, -0.23337483, -0.36921312,\n",
       "        0.48680207,  0.81479979,  0.83898411,  0.55623234, -0.19410156,\n",
       "       -0.31151441,  0.54103063,  0.87169398,  0.89196549,  0.59759727,\n",
       "       -0.18023905, -0.21974111,  0.60844584,  0.93591566,  0.94482724,\n",
       "        0.63042482, -0.20208074, -0.09287036,  0.69111146,  1.00830156,\n",
       "        0.99595171,  0.6507891 , -0.28202455,  0.06718767,  0.79110294,\n",
       "        1.08909902,  1.04206095,  0.65198309, -0.47896297,  0.25680016,\n",
       "        0.91044856,  1.17706331,  1.07696922,  0.62192268, -1.05225707,\n",
       "        0.47238068,  1.05092126,  1.26732998,  1.0894643 ,  0.5362058 ,\n",
       "       -1.0622456 ,  0.7120751 ,  1.21301655,  1.34659576,  1.05943987,\n",
       "        0.33247821, -0.00541093,  0.97709123,  1.39114698,  1.38398362,\n",
       "        0.94791077, -0.27498466,  0.59137889,  1.27165749,  1.55282687,\n",
       "        1.31995415,  0.64888357, -0.12728164,  1.09135644,  1.58793369,\n",
       "        1.57805018,  1.04250795, -0.92845152,  0.94095961,  1.59066491,\n",
       "        1.75192019,  1.23505107, -0.23651184,  0.95802307,  1.68543168,\n",
       "        1.84301403,  1.16088626, -0.68029524,  1.28911065,  1.98166833,\n",
       "        1.53900462, -0.13594019,  1.1582382 ,  2.07844937,  1.08286008,\n",
       "        0.56750009,  2.06854834, -0.70037057,  0.91201806,  1.19682979,\n",
       "        1.34958601,  1.44963377,  1.52013261,  1.57116994,  1.60824143,\n",
       "        1.63481086,  1.65328254,  1.66543094,  1.67261464,  1.67589557,\n",
       "        1.67611292,  1.67393353,  1.66988856,  1.66440158,  1.65781051,\n",
       "        1.65038504,  1.6423405 ,  1.63384891,  1.62504785,  1.6160475 ,\n",
       "        1.60693627,  1.59778535,  1.58865235,  1.57958423,  1.5706198 ,\n",
       "        1.56179174,  1.55312841,  1.54465559,  1.53639818,  1.52838224,\n",
       "        1.52063759,  1.51320183,  1.50612712,  1.4994937 ,  1.49344073,\n",
       "        1.48825615,  1.48477607,  1.54776405,  1.66392883,  1.67639982,\n",
       "        1.6673768 ,  1.65261303,  1.63548631,  1.62377981,  1.6143401 ,\n",
       "        1.60123419,  1.5852422 ,  1.57193674,  1.56303558,  1.55391322,\n",
       "        1.54096049,  1.52548474,  1.51222105,  1.50334468,  1.49535104,\n",
       "        1.48418619,  1.46957279,  1.45519339,  1.44495345,  1.43782336,\n",
       "        1.42936233,  1.41701077,  1.40227543,  1.38956333,  1.38141869,\n",
       "        1.37514008,  1.36634256,  1.35350013,  1.339161  ,  1.32792194,\n",
       "        1.32127625,  1.31563813,  1.30676932,  1.29379974,  1.27994264,\n",
       "        1.26986885,  1.26439133,  1.25938587,  1.25066367,  1.23774265,\n",
       "        1.22429258,  1.21509641,  1.21058966,  1.20626627,  1.1978597 ,\n",
       "        1.18507326,  1.17190333,  1.16333058,  1.1596565 ,  1.15607627,\n",
       "        1.14812416,  1.13552271,  1.12249849,  1.11432014,  1.11136192,\n",
       "        1.10858174])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0,:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obs.PackClass as pk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16, 12])\n",
    "plt.subplot(3,2,1)\n",
    "plt.plot(freq,result[0][:,0])\n",
    "plt.plot(freq,result_o[0][:,0])\n",
    "# plt.grid('on')\n",
    "plt.subplot(3,2,2)\n",
    "plt.plot(freq,result[0][:,1])\n",
    "plt.plot(freq,result_o[0][:,1])\n",
    "plt.subplot(3,2,3)\n",
    "plt.plot(freq,result[0][:,2])\n",
    "plt.plot(freq,result_o[0][:,2])\n",
    "plt.subplot(3,2,4)\n",
    "plt.plot(freq,result[0][:,3])\n",
    "plt.plot(freq,result_o[0][:,3])\n",
    "plt.subplot(3,2,5)\n",
    "plt.plot(freq,result[0][:,4])   \n",
    "plt.plot(freq,result_o[0][:,4])\n",
    "plt.subplot(3,2,6)\n",
    "plt.plot(freq,result[0][:,5])\n",
    "plt.plot(freq,result_o[0][:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = pk.Plot_all(y1,y1,freq,name1='0',name2='1')\n",
    "plot.plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 501, 6)]     0           []                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 501)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1003)         503506      ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1003)         503506      ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1003)         503506      ['lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1003)         503506      ['lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1003)         503506      ['lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1003)         503506      ['lambda_5[0][0]']               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1003)         1007012     ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1003)         1007012     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1003)         1007012     ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1003)         1007012     ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1003)         1007012     ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1003)         1007012     ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1003)         1007012     ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1003)         1007012     ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1003)         1007012     ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 1003)         1007012     ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 1003)         1007012     ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 1003)         1007012     ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 16)           16064       ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16)           16064       ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16)           16064       ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 16)           16064       ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 16)           16064       ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 16)           16064       ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 96)           0           ['dense_18[0][0]',               \n",
      "                                                                  'dense_19[0][0]',               \n",
      "                                                                  'dense_20[0][0]',               \n",
      "                                                                  'dense_21[0][0]',               \n",
      "                                                                  'dense_22[0][0]',               \n",
      "                                                                  'dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 100)          9700        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 100)          10100       ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 100)          10100       ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 29)           2929        ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 7)            210         ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,234,603\n",
      "Trainable params: 15,234,603\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.2664\n",
      "Epoch 1: val_loss improved from inf to 0.06159, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 12s 122ms/step - loss: 0.0768 - accuracy: 0.2664 - val_loss: 0.0616 - val_accuracy: 0.2969\n",
      "Epoch 2/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.4328\n",
      "Epoch 2: val_loss improved from 0.06159 to 0.04382, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 13s 161ms/step - loss: 0.0474 - accuracy: 0.4328 - val_loss: 0.0438 - val_accuracy: 0.4531\n",
      "Epoch 3/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.4823\n",
      "Epoch 3: val_loss improved from 0.04382 to 0.03914, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 12s 146ms/step - loss: 0.0396 - accuracy: 0.4823 - val_loss: 0.0391 - val_accuracy: 0.4375\n",
      "Epoch 4/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.5074\n",
      "Epoch 4: val_loss improved from 0.03914 to 0.03706, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 11s 134ms/step - loss: 0.0375 - accuracy: 0.5074 - val_loss: 0.0371 - val_accuracy: 0.4688\n",
      "Epoch 5/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.5225\n",
      "Epoch 5: val_loss improved from 0.03706 to 0.03405, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 0.0338 - accuracy: 0.5225 - val_loss: 0.0340 - val_accuracy: 0.5000\n",
      "Epoch 6/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.5331\n",
      "Epoch 6: val_loss improved from 0.03405 to 0.03038, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 0.0315 - accuracy: 0.5331 - val_loss: 0.0304 - val_accuracy: 0.5156\n",
      "Epoch 7/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.5454\n",
      "Epoch 7: val_loss improved from 0.03038 to 0.02884, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 0.0291 - accuracy: 0.5454 - val_loss: 0.0288 - val_accuracy: 0.5312\n",
      "Epoch 8/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.5515\n",
      "Epoch 8: val_loss improved from 0.02884 to 0.02883, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 0.0288 - accuracy: 0.5515 - val_loss: 0.0288 - val_accuracy: 0.5078\n",
      "Epoch 9/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.5680\n",
      "Epoch 9: val_loss improved from 0.02883 to 0.02839, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 11s 141ms/step - loss: 0.0273 - accuracy: 0.5680 - val_loss: 0.0284 - val_accuracy: 0.5469\n",
      "Epoch 10/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.5688\n",
      "Epoch 10: val_loss improved from 0.02839 to 0.02792, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 0.0269 - accuracy: 0.5688 - val_loss: 0.0279 - val_accuracy: 0.5469\n",
      "Epoch 11/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.5753\n",
      "Epoch 11: val_loss did not improve from 0.02792\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 0.0267 - accuracy: 0.5753 - val_loss: 0.0280 - val_accuracy: 0.5156\n",
      "Epoch 12/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.5767\n",
      "Epoch 12: val_loss improved from 0.02792 to 0.02700, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.0266 - accuracy: 0.5767 - val_loss: 0.0270 - val_accuracy: 0.5156\n",
      "Epoch 13/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0262 - accuracy: 0.5744\n",
      "Epoch 13: val_loss did not improve from 0.02700\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 0.0262 - accuracy: 0.5736 - val_loss: 0.0271 - val_accuracy: 0.5078\n",
      "Epoch 14/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.5817\n",
      "Epoch 14: val_loss did not improve from 0.02700\n",
      "80/80 [==============================] - 5s 64ms/step - loss: 0.0263 - accuracy: 0.5813 - val_loss: 0.0273 - val_accuracy: 0.4844\n",
      "Epoch 15/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.5815\n",
      "Epoch 15: val_loss did not improve from 0.02700\n",
      "80/80 [==============================] - 5s 58ms/step - loss: 0.0256 - accuracy: 0.5815 - val_loss: 0.0273 - val_accuracy: 0.5156\n",
      "Epoch 16/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.5815\n",
      "Epoch 16: val_loss did not improve from 0.02700\n",
      "80/80 [==============================] - 5s 58ms/step - loss: 0.0257 - accuracy: 0.5815 - val_loss: 0.0274 - val_accuracy: 0.5312\n",
      "Epoch 17/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.5822\n",
      "Epoch 17: val_loss improved from 0.02700 to 0.02673, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 7s 87ms/step - loss: 0.0258 - accuracy: 0.5822 - val_loss: 0.0267 - val_accuracy: 0.5156\n",
      "Epoch 18/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.5903\n",
      "Epoch 18: val_loss improved from 0.02673 to 0.02631, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 8s 106ms/step - loss: 0.0255 - accuracy: 0.5903 - val_loss: 0.0263 - val_accuracy: 0.5078\n",
      "Epoch 19/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.5805\n",
      "Epoch 19: val_loss did not improve from 0.02631\n",
      "80/80 [==============================] - 6s 77ms/step - loss: 0.0254 - accuracy: 0.5805 - val_loss: 0.0269 - val_accuracy: 0.5000\n",
      "Epoch 20/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.5938\n",
      "Epoch 20: val_loss did not improve from 0.02631\n",
      "80/80 [==============================] - 5s 63ms/step - loss: 0.0252 - accuracy: 0.5939 - val_loss: 0.0268 - val_accuracy: 0.5391\n",
      "Epoch 21/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.5910\n",
      "Epoch 21: val_loss did not improve from 0.02631\n",
      "80/80 [==============================] - 5s 57ms/step - loss: 0.0252 - accuracy: 0.5914 - val_loss: 0.0267 - val_accuracy: 0.5234\n",
      "Epoch 22/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.5844\n",
      "Epoch 22: val_loss did not improve from 0.02631\n",
      "80/80 [==============================] - 5s 58ms/step - loss: 0.0248 - accuracy: 0.5844 - val_loss: 0.0265 - val_accuracy: 0.5312\n",
      "Epoch 23/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0249 - accuracy: 0.5940\n",
      "Epoch 23: val_loss improved from 0.02631 to 0.02609, saving model to c:\\Users\\caizhi.zheng\\code\\For AI\\obs\\weights\n",
      "80/80 [==============================] - 7s 87ms/step - loss: 0.0249 - accuracy: 0.5947 - val_loss: 0.0261 - val_accuracy: 0.5312\n",
      "Epoch 24/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.5869\n",
      "Epoch 24: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 78ms/step - loss: 0.0248 - accuracy: 0.5869 - val_loss: 0.0268 - val_accuracy: 0.5312\n",
      "Epoch 25/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.5938\n",
      "Epoch 25: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 62ms/step - loss: 0.0247 - accuracy: 0.5938 - val_loss: 0.0267 - val_accuracy: 0.5391\n",
      "Epoch 26/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.5852\n",
      "Epoch 26: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 58ms/step - loss: 0.0249 - accuracy: 0.5852 - val_loss: 0.0270 - val_accuracy: 0.5391\n",
      "Epoch 27/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0242 - accuracy: 0.5969\n",
      "Epoch 27: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 60ms/step - loss: 0.0242 - accuracy: 0.5966 - val_loss: 0.0264 - val_accuracy: 0.5312\n",
      "Epoch 28/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0246 - accuracy: 0.5900\n",
      "Epoch 28: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 0.0245 - accuracy: 0.5899 - val_loss: 0.0271 - val_accuracy: 0.5391\n",
      "Epoch 29/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0244 - accuracy: 0.5903\n",
      "Epoch 29: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 58ms/step - loss: 0.0244 - accuracy: 0.5899 - val_loss: 0.0274 - val_accuracy: 0.5078\n",
      "Epoch 30/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.5949\n",
      "Epoch 30: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 74ms/step - loss: 0.0240 - accuracy: 0.5949 - val_loss: 0.0272 - val_accuracy: 0.5156\n",
      "Epoch 31/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0244 - accuracy: 0.5831\n",
      "Epoch 31: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 78ms/step - loss: 0.0244 - accuracy: 0.5830 - val_loss: 0.0269 - val_accuracy: 0.5469\n",
      "Epoch 32/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.5846\n",
      "Epoch 32: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0239 - accuracy: 0.5846 - val_loss: 0.0268 - val_accuracy: 0.5469\n",
      "Epoch 33/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.5890\n",
      "Epoch 33: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 63ms/step - loss: 0.0238 - accuracy: 0.5890 - val_loss: 0.0272 - val_accuracy: 0.5781\n",
      "Epoch 34/500\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0236 - accuracy: 0.5911\n",
      "Epoch 34: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 66ms/step - loss: 0.0236 - accuracy: 0.5910 - val_loss: 0.0275 - val_accuracy: 0.5469\n",
      "Epoch 35/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.5845\n",
      "Epoch 35: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 63ms/step - loss: 0.0238 - accuracy: 0.5845 - val_loss: 0.0266 - val_accuracy: 0.5391\n",
      "Epoch 36/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.5846\n",
      "Epoch 36: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 74ms/step - loss: 0.0234 - accuracy: 0.5846 - val_loss: 0.0273 - val_accuracy: 0.5391\n",
      "Epoch 37/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.5937\n",
      "Epoch 37: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0232 - accuracy: 0.5937 - val_loss: 0.0270 - val_accuracy: 0.5547\n",
      "Epoch 38/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.5865\n",
      "Epoch 38: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0230 - accuracy: 0.5865 - val_loss: 0.0278 - val_accuracy: 0.5078\n",
      "Epoch 39/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.5892\n",
      "Epoch 39: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0230 - accuracy: 0.5892 - val_loss: 0.0282 - val_accuracy: 0.5000\n",
      "Epoch 40/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.5849\n",
      "Epoch 40: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0228 - accuracy: 0.5849 - val_loss: 0.0271 - val_accuracy: 0.5859\n",
      "Epoch 41/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.5960\n",
      "Epoch 41: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0226 - accuracy: 0.5960 - val_loss: 0.0276 - val_accuracy: 0.5391\n",
      "Epoch 42/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.5917\n",
      "Epoch 42: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0221 - accuracy: 0.5917 - val_loss: 0.0266 - val_accuracy: 0.5391\n",
      "Epoch 43/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.5885\n",
      "Epoch 43: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0222 - accuracy: 0.5885 - val_loss: 0.0276 - val_accuracy: 0.5625\n",
      "Epoch 44/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.5858\n",
      "Epoch 44: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0219 - accuracy: 0.5858 - val_loss: 0.0276 - val_accuracy: 0.5234\n",
      "Epoch 45/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.6011\n",
      "Epoch 45: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0215 - accuracy: 0.6011 - val_loss: 0.0285 - val_accuracy: 0.5703\n",
      "Epoch 46/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.5966\n",
      "Epoch 46: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0213 - accuracy: 0.5966 - val_loss: 0.0277 - val_accuracy: 0.5469\n",
      "Epoch 47/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.6013\n",
      "Epoch 47: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0208 - accuracy: 0.6013 - val_loss: 0.0283 - val_accuracy: 0.5625\n",
      "Epoch 48/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.6037\n",
      "Epoch 48: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0205 - accuracy: 0.6037 - val_loss: 0.0288 - val_accuracy: 0.5391\n",
      "Epoch 49/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.5983\n",
      "Epoch 49: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0200 - accuracy: 0.5983 - val_loss: 0.0284 - val_accuracy: 0.5391\n",
      "Epoch 50/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.6122\n",
      "Epoch 50: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0198 - accuracy: 0.6122 - val_loss: 0.0279 - val_accuracy: 0.5547\n",
      "Epoch 51/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.6145\n",
      "Epoch 51: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0193 - accuracy: 0.6145 - val_loss: 0.0303 - val_accuracy: 0.5469\n",
      "Epoch 52/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.6127\n",
      "Epoch 52: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 73ms/step - loss: 0.0189 - accuracy: 0.6127 - val_loss: 0.0289 - val_accuracy: 0.5547\n",
      "Epoch 53/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.6197\n",
      "Epoch 53: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0185 - accuracy: 0.6197 - val_loss: 0.0299 - val_accuracy: 0.5469\n",
      "Epoch 54/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.6247\n",
      "Epoch 54: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0182 - accuracy: 0.6247 - val_loss: 0.0303 - val_accuracy: 0.5547\n",
      "Epoch 55/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.6217\n",
      "Epoch 55: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0178 - accuracy: 0.6217 - val_loss: 0.0297 - val_accuracy: 0.5234\n",
      "Epoch 56/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.6322\n",
      "Epoch 56: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0175 - accuracy: 0.6322 - val_loss: 0.0292 - val_accuracy: 0.5312\n",
      "Epoch 57/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.6306\n",
      "Epoch 57: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0165 - accuracy: 0.6306 - val_loss: 0.0317 - val_accuracy: 0.5703\n",
      "Epoch 58/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.6325\n",
      "Epoch 58: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 74ms/step - loss: 0.0167 - accuracy: 0.6325 - val_loss: 0.0311 - val_accuracy: 0.5234\n",
      "Epoch 59/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.6475\n",
      "Epoch 59: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 0.0159 - accuracy: 0.6475 - val_loss: 0.0309 - val_accuracy: 0.5469\n",
      "Epoch 60/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.6464\n",
      "Epoch 60: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 7s 86ms/step - loss: 0.0158 - accuracy: 0.6464 - val_loss: 0.0305 - val_accuracy: 0.5391\n",
      "Epoch 61/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.6493\n",
      "Epoch 61: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 69ms/step - loss: 0.0151 - accuracy: 0.6493 - val_loss: 0.0328 - val_accuracy: 0.5391\n",
      "Epoch 62/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.6528\n",
      "Epoch 62: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 68ms/step - loss: 0.0150 - accuracy: 0.6528 - val_loss: 0.0305 - val_accuracy: 0.5625\n",
      "Epoch 63/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.6658\n",
      "Epoch 63: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0140 - accuracy: 0.6658 - val_loss: 0.0314 - val_accuracy: 0.5703\n",
      "Epoch 64/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.6597\n",
      "Epoch 64: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 69ms/step - loss: 0.0145 - accuracy: 0.6597 - val_loss: 0.0314 - val_accuracy: 0.5391\n",
      "Epoch 65/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.6633\n",
      "Epoch 65: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0136 - accuracy: 0.6633 - val_loss: 0.0317 - val_accuracy: 0.5625\n",
      "Epoch 66/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.6666\n",
      "Epoch 66: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 69ms/step - loss: 0.0138 - accuracy: 0.6666 - val_loss: 0.0317 - val_accuracy: 0.5625\n",
      "Epoch 67/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.6686\n",
      "Epoch 67: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 7s 85ms/step - loss: 0.0129 - accuracy: 0.6686 - val_loss: 0.0328 - val_accuracy: 0.5234\n",
      "Epoch 68/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.6761\n",
      "Epoch 68: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0134 - accuracy: 0.6761 - val_loss: 0.0317 - val_accuracy: 0.5703\n",
      "Epoch 69/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.6853\n",
      "Epoch 69: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 69ms/step - loss: 0.0125 - accuracy: 0.6853 - val_loss: 0.0335 - val_accuracy: 0.5625\n",
      "Epoch 70/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.6808\n",
      "Epoch 70: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0127 - accuracy: 0.6808 - val_loss: 0.0327 - val_accuracy: 0.5859\n",
      "Epoch 71/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.6900\n",
      "Epoch 71: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0121 - accuracy: 0.6900 - val_loss: 0.0343 - val_accuracy: 0.5547\n",
      "Epoch 72/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.6882\n",
      "Epoch 72: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0120 - accuracy: 0.6882 - val_loss: 0.0335 - val_accuracy: 0.5469\n",
      "Epoch 73/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.6867\n",
      "Epoch 73: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0117 - accuracy: 0.6867 - val_loss: 0.0341 - val_accuracy: 0.5391\n",
      "Epoch 74/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.6908\n",
      "Epoch 74: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0117 - accuracy: 0.6908 - val_loss: 0.0339 - val_accuracy: 0.5312\n",
      "Epoch 75/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.6982\n",
      "Epoch 75: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0114 - accuracy: 0.6982 - val_loss: 0.0341 - val_accuracy: 0.5859\n",
      "Epoch 76/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.6951\n",
      "Epoch 76: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0110 - accuracy: 0.6951 - val_loss: 0.0329 - val_accuracy: 0.5703\n",
      "Epoch 77/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.7037\n",
      "Epoch 77: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0109 - accuracy: 0.7037 - val_loss: 0.0335 - val_accuracy: 0.5234\n",
      "Epoch 78/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.6989\n",
      "Epoch 78: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0108 - accuracy: 0.6989 - val_loss: 0.0347 - val_accuracy: 0.5781\n",
      "Epoch 79/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.7023\n",
      "Epoch 79: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0106 - accuracy: 0.7023 - val_loss: 0.0352 - val_accuracy: 0.5391\n",
      "Epoch 80/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.6985\n",
      "Epoch 80: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0104 - accuracy: 0.6985 - val_loss: 0.0345 - val_accuracy: 0.5625\n",
      "Epoch 81/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.7072\n",
      "Epoch 81: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0102 - accuracy: 0.7072 - val_loss: 0.0347 - val_accuracy: 0.5469\n",
      "Epoch 82/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.6992\n",
      "Epoch 82: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0101 - accuracy: 0.6992 - val_loss: 0.0340 - val_accuracy: 0.5547\n",
      "Epoch 83/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.7165\n",
      "Epoch 83: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0097 - accuracy: 0.7165 - val_loss: 0.0357 - val_accuracy: 0.5625\n",
      "Epoch 84/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.7033\n",
      "Epoch 84: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0097 - accuracy: 0.7033 - val_loss: 0.0350 - val_accuracy: 0.5859\n",
      "Epoch 85/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.7156\n",
      "Epoch 85: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 74ms/step - loss: 0.0095 - accuracy: 0.7156 - val_loss: 0.0348 - val_accuracy: 0.5781\n",
      "Epoch 86/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.7155\n",
      "Epoch 86: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 78ms/step - loss: 0.0095 - accuracy: 0.7155 - val_loss: 0.0340 - val_accuracy: 0.5703\n",
      "Epoch 87/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.7181\n",
      "Epoch 87: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 0.0091 - accuracy: 0.7181 - val_loss: 0.0347 - val_accuracy: 0.5547\n",
      "Epoch 88/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.7251\n",
      "Epoch 88: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 0.0089 - accuracy: 0.7251 - val_loss: 0.0354 - val_accuracy: 0.5000\n",
      "Epoch 89/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.7220\n",
      "Epoch 89: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 74ms/step - loss: 0.0090 - accuracy: 0.7220 - val_loss: 0.0356 - val_accuracy: 0.5391\n",
      "Epoch 90/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.7248\n",
      "Epoch 90: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 73ms/step - loss: 0.0085 - accuracy: 0.7248 - val_loss: 0.0359 - val_accuracy: 0.5547\n",
      "Epoch 91/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.7212\n",
      "Epoch 91: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0086 - accuracy: 0.7212 - val_loss: 0.0357 - val_accuracy: 0.5547\n",
      "Epoch 92/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.7235\n",
      "Epoch 92: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0081 - accuracy: 0.7235 - val_loss: 0.0355 - val_accuracy: 0.5312\n",
      "Epoch 93/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.7324\n",
      "Epoch 93: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0081 - accuracy: 0.7324 - val_loss: 0.0360 - val_accuracy: 0.5078\n",
      "Epoch 94/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.7282\n",
      "Epoch 94: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 69ms/step - loss: 0.0077 - accuracy: 0.7282 - val_loss: 0.0368 - val_accuracy: 0.5391\n",
      "Epoch 95/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.7307\n",
      "Epoch 95: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0079 - accuracy: 0.7307 - val_loss: 0.0368 - val_accuracy: 0.5312\n",
      "Epoch 96/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.7366\n",
      "Epoch 96: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 69ms/step - loss: 0.0073 - accuracy: 0.7366 - val_loss: 0.0373 - val_accuracy: 0.5234\n",
      "Epoch 97/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.7327\n",
      "Epoch 97: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0075 - accuracy: 0.7327 - val_loss: 0.0354 - val_accuracy: 0.5547\n",
      "Epoch 98/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.7350\n",
      "Epoch 98: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0070 - accuracy: 0.7350 - val_loss: 0.0368 - val_accuracy: 0.5391\n",
      "Epoch 99/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.7370\n",
      "Epoch 99: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0070 - accuracy: 0.7370 - val_loss: 0.0357 - val_accuracy: 0.5234\n",
      "Epoch 100/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.7411\n",
      "Epoch 100: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 69ms/step - loss: 0.0065 - accuracy: 0.7411 - val_loss: 0.0365 - val_accuracy: 0.5234\n",
      "Epoch 101/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.7365\n",
      "Epoch 101: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0067 - accuracy: 0.7365 - val_loss: 0.0369 - val_accuracy: 0.5312\n",
      "Epoch 102/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.7442\n",
      "Epoch 102: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 68ms/step - loss: 0.0062 - accuracy: 0.7442 - val_loss: 0.0358 - val_accuracy: 0.5391\n",
      "Epoch 103/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.7430\n",
      "Epoch 103: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0062 - accuracy: 0.7430 - val_loss: 0.0364 - val_accuracy: 0.5312\n",
      "Epoch 104/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.7526\n",
      "Epoch 104: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0059 - accuracy: 0.7526 - val_loss: 0.0366 - val_accuracy: 0.5703\n",
      "Epoch 105/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.7435\n",
      "Epoch 105: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 69ms/step - loss: 0.0059 - accuracy: 0.7435 - val_loss: 0.0349 - val_accuracy: 0.5625\n",
      "Epoch 106/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.7500\n",
      "Epoch 106: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0054 - accuracy: 0.7500 - val_loss: 0.0364 - val_accuracy: 0.5312\n",
      "Epoch 107/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.7497\n",
      "Epoch 107: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0054 - accuracy: 0.7497 - val_loss: 0.0357 - val_accuracy: 0.5391\n",
      "Epoch 108/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.7572\n",
      "Epoch 108: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0053 - accuracy: 0.7572 - val_loss: 0.0365 - val_accuracy: 0.5312\n",
      "Epoch 109/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.7548\n",
      "Epoch 109: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0052 - accuracy: 0.7548 - val_loss: 0.0370 - val_accuracy: 0.5469\n",
      "Epoch 110/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.7643\n",
      "Epoch 110: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0048 - accuracy: 0.7643 - val_loss: 0.0365 - val_accuracy: 0.5547\n",
      "Epoch 111/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.7569\n",
      "Epoch 111: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0048 - accuracy: 0.7569 - val_loss: 0.0369 - val_accuracy: 0.5156\n",
      "Epoch 112/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.7600\n",
      "Epoch 112: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0047 - accuracy: 0.7600 - val_loss: 0.0371 - val_accuracy: 0.5547\n",
      "Epoch 113/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.7650\n",
      "Epoch 113: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0045 - accuracy: 0.7650 - val_loss: 0.0369 - val_accuracy: 0.5469\n",
      "Epoch 114/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.7729\n",
      "Epoch 114: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0043 - accuracy: 0.7729 - val_loss: 0.0360 - val_accuracy: 0.5391\n",
      "Epoch 115/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.7694\n",
      "Epoch 115: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0042 - accuracy: 0.7694 - val_loss: 0.0370 - val_accuracy: 0.5469\n",
      "Epoch 116/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.7729\n",
      "Epoch 116: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0042 - accuracy: 0.7729 - val_loss: 0.0367 - val_accuracy: 0.5469\n",
      "Epoch 117/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.7781\n",
      "Epoch 117: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0038 - accuracy: 0.7781 - val_loss: 0.0371 - val_accuracy: 0.6016\n",
      "Epoch 118/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.7796\n",
      "Epoch 118: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0038 - accuracy: 0.7796 - val_loss: 0.0372 - val_accuracy: 0.5781\n",
      "Epoch 119/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.7784\n",
      "Epoch 119: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0037 - accuracy: 0.7784 - val_loss: 0.0374 - val_accuracy: 0.5547\n",
      "Epoch 120/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.7836\n",
      "Epoch 120: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0038 - accuracy: 0.7836 - val_loss: 0.0367 - val_accuracy: 0.5547\n",
      "Epoch 121/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.7853\n",
      "Epoch 121: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0037 - accuracy: 0.7853 - val_loss: 0.0380 - val_accuracy: 0.5625\n",
      "Epoch 122/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.7828\n",
      "Epoch 122: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0036 - accuracy: 0.7828 - val_loss: 0.0367 - val_accuracy: 0.5547\n",
      "Epoch 123/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.7885\n",
      "Epoch 123: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0034 - accuracy: 0.7885 - val_loss: 0.0373 - val_accuracy: 0.5469\n",
      "Epoch 124/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.7865\n",
      "Epoch 124: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0034 - accuracy: 0.7865 - val_loss: 0.0369 - val_accuracy: 0.5547\n",
      "Epoch 125/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.7930\n",
      "Epoch 125: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 5s 69ms/step - loss: 0.0033 - accuracy: 0.7930 - val_loss: 0.0377 - val_accuracy: 0.5781\n",
      "Epoch 126/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.7881\n",
      "Epoch 126: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0033 - accuracy: 0.7881 - val_loss: 0.0360 - val_accuracy: 0.5625\n",
      "Epoch 127/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.8014\n",
      "Epoch 127: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0030 - accuracy: 0.8014 - val_loss: 0.0377 - val_accuracy: 0.5703\n",
      "Epoch 128/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.7891\n",
      "Epoch 128: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0033 - accuracy: 0.7891 - val_loss: 0.0365 - val_accuracy: 0.5469\n",
      "Epoch 129/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.7993\n",
      "Epoch 129: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0029 - accuracy: 0.7993 - val_loss: 0.0374 - val_accuracy: 0.5469\n",
      "Epoch 130/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.7977\n",
      "Epoch 130: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0030 - accuracy: 0.7977 - val_loss: 0.0375 - val_accuracy: 0.5625\n",
      "Epoch 131/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.8002\n",
      "Epoch 131: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0030 - accuracy: 0.8002 - val_loss: 0.0370 - val_accuracy: 0.5703\n",
      "Epoch 132/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.7991\n",
      "Epoch 132: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0029 - accuracy: 0.7991 - val_loss: 0.0373 - val_accuracy: 0.5703\n",
      "Epoch 133/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.8095\n",
      "Epoch 133: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0027 - accuracy: 0.8095 - val_loss: 0.0366 - val_accuracy: 0.5625\n",
      "Epoch 134/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.8013\n",
      "Epoch 134: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0030 - accuracy: 0.8013 - val_loss: 0.0374 - val_accuracy: 0.5703\n",
      "Epoch 135/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.8164\n",
      "Epoch 135: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0027 - accuracy: 0.8164 - val_loss: 0.0366 - val_accuracy: 0.5781\n",
      "Epoch 136/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.8035\n",
      "Epoch 136: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0028 - accuracy: 0.8035 - val_loss: 0.0378 - val_accuracy: 0.5391\n",
      "Epoch 137/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.8150\n",
      "Epoch 137: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0026 - accuracy: 0.8150 - val_loss: 0.0369 - val_accuracy: 0.5938\n",
      "Epoch 138/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.8042\n",
      "Epoch 138: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0027 - accuracy: 0.8042 - val_loss: 0.0372 - val_accuracy: 0.5703\n",
      "Epoch 139/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.8138\n",
      "Epoch 139: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0024 - accuracy: 0.8138 - val_loss: 0.0371 - val_accuracy: 0.5781\n",
      "Epoch 140/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.8158\n",
      "Epoch 140: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0026 - accuracy: 0.8158 - val_loss: 0.0369 - val_accuracy: 0.5312\n",
      "Epoch 141/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.8100\n",
      "Epoch 141: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0025 - accuracy: 0.8100 - val_loss: 0.0379 - val_accuracy: 0.5703\n",
      "Epoch 142/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.8240\n",
      "Epoch 142: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0024 - accuracy: 0.8240 - val_loss: 0.0368 - val_accuracy: 0.5781\n",
      "Epoch 143/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.8213\n",
      "Epoch 143: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0024 - accuracy: 0.8213 - val_loss: 0.0380 - val_accuracy: 0.5625\n",
      "Epoch 144/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.8144\n",
      "Epoch 144: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0025 - accuracy: 0.8144 - val_loss: 0.0379 - val_accuracy: 0.5859\n",
      "Epoch 145/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.8224\n",
      "Epoch 145: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0024 - accuracy: 0.8224 - val_loss: 0.0371 - val_accuracy: 0.5625\n",
      "Epoch 146/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.8190\n",
      "Epoch 146: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0023 - accuracy: 0.8190 - val_loss: 0.0380 - val_accuracy: 0.5781\n",
      "Epoch 147/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.8247\n",
      "Epoch 147: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0023 - accuracy: 0.8247 - val_loss: 0.0370 - val_accuracy: 0.5781\n",
      "Epoch 148/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.8172\n",
      "Epoch 148: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0023 - accuracy: 0.8172 - val_loss: 0.0370 - val_accuracy: 0.5469\n",
      "Epoch 149/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.8230\n",
      "Epoch 149: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0024 - accuracy: 0.8230 - val_loss: 0.0375 - val_accuracy: 0.5391\n",
      "Epoch 150/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.8306\n",
      "Epoch 150: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0021 - accuracy: 0.8306 - val_loss: 0.0376 - val_accuracy: 0.5781\n",
      "Epoch 151/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.8217\n",
      "Epoch 151: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0022 - accuracy: 0.8217 - val_loss: 0.0370 - val_accuracy: 0.5781\n",
      "Epoch 152/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.8266\n",
      "Epoch 152: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0022 - accuracy: 0.8266 - val_loss: 0.0374 - val_accuracy: 0.5703\n",
      "Epoch 153/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.8287\n",
      "Epoch 153: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0021 - accuracy: 0.8287 - val_loss: 0.0370 - val_accuracy: 0.5703\n",
      "Epoch 154/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.8335\n",
      "Epoch 154: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0022 - accuracy: 0.8335 - val_loss: 0.0379 - val_accuracy: 0.5391\n",
      "Epoch 155/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.8216\n",
      "Epoch 155: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0021 - accuracy: 0.8216 - val_loss: 0.0366 - val_accuracy: 0.5391\n",
      "Epoch 156/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.8350\n",
      "Epoch 156: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0021 - accuracy: 0.8350 - val_loss: 0.0385 - val_accuracy: 0.5547\n",
      "Epoch 157/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.8318\n",
      "Epoch 157: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0021 - accuracy: 0.8318 - val_loss: 0.0367 - val_accuracy: 0.5703\n",
      "Epoch 158/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.8307\n",
      "Epoch 158: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0021 - accuracy: 0.8307 - val_loss: 0.0371 - val_accuracy: 0.5703\n",
      "Epoch 159/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.8351\n",
      "Epoch 159: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0020 - accuracy: 0.8351 - val_loss: 0.0371 - val_accuracy: 0.5625\n",
      "Epoch 160/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.8354\n",
      "Epoch 160: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0021 - accuracy: 0.8354 - val_loss: 0.0370 - val_accuracy: 0.5625\n",
      "Epoch 161/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.8294\n",
      "Epoch 161: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0020 - accuracy: 0.8294 - val_loss: 0.0368 - val_accuracy: 0.5703\n",
      "Epoch 162/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.8373\n",
      "Epoch 162: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0019 - accuracy: 0.8373 - val_loss: 0.0366 - val_accuracy: 0.5547\n",
      "Epoch 163/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.8362\n",
      "Epoch 163: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0020 - accuracy: 0.8362 - val_loss: 0.0376 - val_accuracy: 0.5703\n",
      "Epoch 164/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.8369\n",
      "Epoch 164: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0019 - accuracy: 0.8369 - val_loss: 0.0378 - val_accuracy: 0.5469\n",
      "Epoch 165/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.8378\n",
      "Epoch 165: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0019 - accuracy: 0.8378 - val_loss: 0.0375 - val_accuracy: 0.5625\n",
      "Epoch 166/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.8364\n",
      "Epoch 166: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0019 - accuracy: 0.8364 - val_loss: 0.0372 - val_accuracy: 0.5547\n",
      "Epoch 167/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.8397\n",
      "Epoch 167: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0019 - accuracy: 0.8397 - val_loss: 0.0368 - val_accuracy: 0.5703\n",
      "Epoch 168/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.8424\n",
      "Epoch 168: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0019 - accuracy: 0.8424 - val_loss: 0.0373 - val_accuracy: 0.5625\n",
      "Epoch 169/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.8387\n",
      "Epoch 169: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0019 - accuracy: 0.8387 - val_loss: 0.0370 - val_accuracy: 0.5781\n",
      "Epoch 170/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.8460\n",
      "Epoch 170: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0019 - accuracy: 0.8460 - val_loss: 0.0369 - val_accuracy: 0.5625\n",
      "Epoch 171/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.8371\n",
      "Epoch 171: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0019 - accuracy: 0.8371 - val_loss: 0.0378 - val_accuracy: 0.5703\n",
      "Epoch 172/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.8407\n",
      "Epoch 172: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0018 - accuracy: 0.8407 - val_loss: 0.0381 - val_accuracy: 0.5703\n",
      "Epoch 173/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.8461\n",
      "Epoch 173: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0018 - accuracy: 0.8461 - val_loss: 0.0371 - val_accuracy: 0.5547\n",
      "Epoch 174/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.8478\n",
      "Epoch 174: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0018 - accuracy: 0.8478 - val_loss: 0.0375 - val_accuracy: 0.5547\n",
      "Epoch 175/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.8452\n",
      "Epoch 175: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 78ms/step - loss: 0.0017 - accuracy: 0.8452 - val_loss: 0.0372 - val_accuracy: 0.5859\n",
      "Epoch 176/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.8415\n",
      "Epoch 176: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0017 - accuracy: 0.8415 - val_loss: 0.0378 - val_accuracy: 0.5703\n",
      "Epoch 177/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.8464\n",
      "Epoch 177: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0017 - accuracy: 0.8464 - val_loss: 0.0371 - val_accuracy: 0.5625\n",
      "Epoch 178/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.8471\n",
      "Epoch 178: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0017 - accuracy: 0.8471 - val_loss: 0.0373 - val_accuracy: 0.5781\n",
      "Epoch 179/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.8492\n",
      "Epoch 179: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0016 - accuracy: 0.8492 - val_loss: 0.0377 - val_accuracy: 0.5547\n",
      "Epoch 180/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.8511\n",
      "Epoch 180: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0017 - accuracy: 0.8511 - val_loss: 0.0368 - val_accuracy: 0.5781\n",
      "Epoch 181/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.8516\n",
      "Epoch 181: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0017 - accuracy: 0.8516 - val_loss: 0.0378 - val_accuracy: 0.5547\n",
      "Epoch 182/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.8436\n",
      "Epoch 182: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0016 - accuracy: 0.8436 - val_loss: 0.0371 - val_accuracy: 0.5391\n",
      "Epoch 183/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.8480\n",
      "Epoch 183: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0016 - accuracy: 0.8480 - val_loss: 0.0363 - val_accuracy: 0.5391\n",
      "Epoch 184/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.8509\n",
      "Epoch 184: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.0017 - accuracy: 0.8509 - val_loss: 0.0376 - val_accuracy: 0.5703\n",
      "Epoch 185/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.8538\n",
      "Epoch 185: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0016 - accuracy: 0.8538 - val_loss: 0.0380 - val_accuracy: 0.5625\n",
      "Epoch 186/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.8496\n",
      "Epoch 186: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 7s 82ms/step - loss: 0.0017 - accuracy: 0.8496 - val_loss: 0.0367 - val_accuracy: 0.5703\n",
      "Epoch 187/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.8519\n",
      "Epoch 187: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 8s 105ms/step - loss: 0.0016 - accuracy: 0.8519 - val_loss: 0.0374 - val_accuracy: 0.5625\n",
      "Epoch 188/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.8545\n",
      "Epoch 188: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0016 - accuracy: 0.8545 - val_loss: 0.0377 - val_accuracy: 0.5703\n",
      "Epoch 189/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8489\n",
      "Epoch 189: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 7s 82ms/step - loss: 0.0015 - accuracy: 0.8489 - val_loss: 0.0369 - val_accuracy: 0.5781\n",
      "Epoch 190/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8539\n",
      "Epoch 190: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 7s 88ms/step - loss: 0.0015 - accuracy: 0.8539 - val_loss: 0.0377 - val_accuracy: 0.5547\n",
      "Epoch 191/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8597\n",
      "Epoch 191: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 0.0015 - accuracy: 0.8597 - val_loss: 0.0370 - val_accuracy: 0.5547\n",
      "Epoch 192/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.8563\n",
      "Epoch 192: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 73ms/step - loss: 0.0016 - accuracy: 0.8563 - val_loss: 0.0368 - val_accuracy: 0.5547\n",
      "Epoch 193/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.8523\n",
      "Epoch 193: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0016 - accuracy: 0.8523 - val_loss: 0.0374 - val_accuracy: 0.5703\n",
      "Epoch 194/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8598\n",
      "Epoch 194: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 77ms/step - loss: 0.0015 - accuracy: 0.8598 - val_loss: 0.0372 - val_accuracy: 0.5469\n",
      "Epoch 195/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.8552\n",
      "Epoch 195: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 74ms/step - loss: 0.0016 - accuracy: 0.8552 - val_loss: 0.0376 - val_accuracy: 0.5625\n",
      "Epoch 196/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8571\n",
      "Epoch 196: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 75ms/step - loss: 0.0015 - accuracy: 0.8571 - val_loss: 0.0373 - val_accuracy: 0.5391\n",
      "Epoch 197/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8595\n",
      "Epoch 197: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 73ms/step - loss: 0.0015 - accuracy: 0.8595 - val_loss: 0.0369 - val_accuracy: 0.5859\n",
      "Epoch 198/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.8554\n",
      "Epoch 198: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 73ms/step - loss: 0.0016 - accuracy: 0.8554 - val_loss: 0.0371 - val_accuracy: 0.5625\n",
      "Epoch 199/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8627\n",
      "Epoch 199: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0015 - accuracy: 0.8627 - val_loss: 0.0372 - val_accuracy: 0.5781\n",
      "Epoch 200/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8554\n",
      "Epoch 200: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0015 - accuracy: 0.8554 - val_loss: 0.0378 - val_accuracy: 0.5547\n",
      "Epoch 201/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.8542\n",
      "Epoch 201: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0014 - accuracy: 0.8542 - val_loss: 0.0374 - val_accuracy: 0.5703\n",
      "Epoch 202/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.8665\n",
      "Epoch 202: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0014 - accuracy: 0.8665 - val_loss: 0.0372 - val_accuracy: 0.5781\n",
      "Epoch 203/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8614\n",
      "Epoch 203: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 0.0015 - accuracy: 0.8614 - val_loss: 0.0370 - val_accuracy: 0.5547\n",
      "Epoch 204/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.8602\n",
      "Epoch 204: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0014 - accuracy: 0.8602 - val_loss: 0.0372 - val_accuracy: 0.5859\n",
      "Epoch 205/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8617\n",
      "Epoch 205: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 75ms/step - loss: 0.0013 - accuracy: 0.8617 - val_loss: 0.0366 - val_accuracy: 0.5625\n",
      "Epoch 206/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.8629\n",
      "Epoch 206: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 7s 83ms/step - loss: 0.0014 - accuracy: 0.8629 - val_loss: 0.0377 - val_accuracy: 0.5625\n",
      "Epoch 207/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.8638\n",
      "Epoch 207: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 9s 117ms/step - loss: 0.0014 - accuracy: 0.8638 - val_loss: 0.0372 - val_accuracy: 0.5625\n",
      "Epoch 208/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8594\n",
      "Epoch 208: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 0.0015 - accuracy: 0.8594 - val_loss: 0.0374 - val_accuracy: 0.5781\n",
      "Epoch 209/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.8593\n",
      "Epoch 209: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 0.0014 - accuracy: 0.8593 - val_loss: 0.0373 - val_accuracy: 0.5859\n",
      "Epoch 210/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8584\n",
      "Epoch 210: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 75ms/step - loss: 0.0015 - accuracy: 0.8584 - val_loss: 0.0378 - val_accuracy: 0.5625\n",
      "Epoch 211/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.8647\n",
      "Epoch 211: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 0.0014 - accuracy: 0.8647 - val_loss: 0.0373 - val_accuracy: 0.5625\n",
      "Epoch 212/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.8620\n",
      "Epoch 212: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 75ms/step - loss: 0.0014 - accuracy: 0.8620 - val_loss: 0.0368 - val_accuracy: 0.5469\n",
      "Epoch 213/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8674\n",
      "Epoch 213: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 75ms/step - loss: 0.0013 - accuracy: 0.8674 - val_loss: 0.0373 - val_accuracy: 0.5625\n",
      "Epoch 214/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8671\n",
      "Epoch 214: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 73ms/step - loss: 0.0013 - accuracy: 0.8671 - val_loss: 0.0376 - val_accuracy: 0.5547\n",
      "Epoch 215/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8647\n",
      "Epoch 215: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0013 - accuracy: 0.8647 - val_loss: 0.0374 - val_accuracy: 0.5547\n",
      "Epoch 216/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.8698\n",
      "Epoch 216: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 7s 85ms/step - loss: 0.0014 - accuracy: 0.8698 - val_loss: 0.0374 - val_accuracy: 0.5625\n",
      "Epoch 217/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8640\n",
      "Epoch 217: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 7s 84ms/step - loss: 0.0013 - accuracy: 0.8640 - val_loss: 0.0375 - val_accuracy: 0.5703\n",
      "Epoch 218/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8717\n",
      "Epoch 218: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0013 - accuracy: 0.8717 - val_loss: 0.0375 - val_accuracy: 0.5781\n",
      "Epoch 219/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8663\n",
      "Epoch 219: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 73ms/step - loss: 0.0013 - accuracy: 0.8663 - val_loss: 0.0373 - val_accuracy: 0.5781\n",
      "Epoch 220/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.8648\n",
      "Epoch 220: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 71ms/step - loss: 0.0012 - accuracy: 0.8648 - val_loss: 0.0373 - val_accuracy: 0.5625\n",
      "Epoch 221/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8721\n",
      "Epoch 221: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 72ms/step - loss: 0.0013 - accuracy: 0.8721 - val_loss: 0.0374 - val_accuracy: 0.5625\n",
      "Epoch 222/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8678\n",
      "Epoch 222: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 74ms/step - loss: 0.0013 - accuracy: 0.8678 - val_loss: 0.0373 - val_accuracy: 0.5625\n",
      "Epoch 223/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8719\n",
      "Epoch 223: val_loss did not improve from 0.02609\n",
      "80/80 [==============================] - 6s 74ms/step - loss: 0.0013 - accuracy: 0.8719 - val_loss: 0.0375 - val_accuracy: 0.5703\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0375 - accuracy: 0.5703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03747507557272911, 0.5703125]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load net_2paras\n",
    "# code by Zhengcaizhi\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import keras.optimizers as optimizer\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import models, layers, Input, Model\n",
    "import skrf as rf\n",
    "from data_gene_2paras import cacul\n",
    "import keras.backend as K \n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Lambda, Concatenate, Dense, Conv1D, Flatten, Dropout\n",
    "\n",
    "# model2v3-v5\n",
    "def IANN():\n",
    "    input = Input(shape=(501,6))\n",
    "    branch1 = Lambda(lambda x:x[:,:,0])(input)  \n",
    "    branch2 = Lambda(lambda x:x[:,:,1])(input)\n",
    "    branch3 = Lambda(lambda x:x[:,:,2])(input)\n",
    "    branch4 = Lambda(lambda x:x[:,:,3])(input)\n",
    "    branch5 = Lambda(lambda x:x[:,:,4])(input)\n",
    "    branch6 = Lambda(lambda x:x[:,:,5])(input)\n",
    "    h1 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch1)\n",
    "    h2 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch2)\n",
    "    h3 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch3)\n",
    "    h4 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch4)\n",
    "    h5 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch5)\n",
    "    h6 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(branch6)\n",
    "    h1 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h1)\n",
    "    h2 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h2)\n",
    "    h3 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h3)\n",
    "    h4 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h4)\n",
    "    h5 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h5)\n",
    "    h6 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h6)\n",
    "    h1 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h1)\n",
    "    h2 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h2)\n",
    "    h3 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h3)\n",
    "    h4 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h4)\n",
    "    h5 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h5)\n",
    "    h6 = Dense(1003,activation='relu', kernel_initializer='he_uniform')(h6)\n",
    "    # h1 = Dense(3207,activation='relu', kernel_initializer='he_uniform')(h1)\n",
    "    # h2 = Dense(3207,activation='relu', kernel_initializer='he_uniform')(h2)\n",
    "    # h3 = Dense(3207,activation='relu', kernel_initializer='he_uniform')(h3)\n",
    "    # h4 = Dense(3207,activation='relu', kernel_initializer='he_uniform')(h4)\n",
    "    # h5 = Dense(3207,activation='relu', kernel_initializer='he_uniform')(h5)\n",
    "    # hn drop?\n",
    "    o1 = Dense(16,activation='relu', kernel_initializer='he_uniform')(h1)\n",
    "    o2 = Dense(16,activation='relu', kernel_initializer='he_uniform')(h2)\n",
    "    o3 = Dense(16,activation='relu', kernel_initializer='he_uniform')(h3)\n",
    "    o4 = Dense(16,activation='relu', kernel_initializer='he_uniform')(h4)\n",
    "    o5 = Dense(16,activation='relu', kernel_initializer='he_uniform')(h5)\n",
    "    o6 = Dense(16,activation='relu', kernel_initializer='he_uniform')(h6)\n",
    "    out1 = Concatenate(axis = -1)((o1,o2,o3,o4,o5,o6))\n",
    "    out1 = Dense(100, activation='relu', kernel_initializer='he_uniform')(out1) #model2v6\n",
    "    out1 = Dense(100, activation='relu', kernel_initializer='he_uniform')(out1) #model2v6\n",
    "    out1 = Dense(100, activation='relu', kernel_initializer='he_uniform')(out1) #model2v6\n",
    "    h2 = Dense(29, activation='relu', kernel_initializer='he_uniform')(out1)\n",
    "    out2 = Dense(7,activation='sigmoid', kernel_initializer='he_uniform')(h2)\n",
    "    model = Model(inputs=input, outputs=out2)\n",
    "    model.summary() \n",
    "    return model\n",
    "\n",
    "def conv(): #model3v1\n",
    "    input = Input(shape=(801,5))\n",
    "    branchs = Lambda(lambda x: tf.split(x, num_or_size_splits=5, axis=-1))(input)\n",
    "    o1 = Conv1D(4,256,activation='relu', kernel_initializer='he_uniform')(branchs[0])\n",
    "    o2 = Conv1D(4,256,activation='relu', kernel_initializer='he_uniform')(branchs[1])\n",
    "    o3 = Conv1D(4,256,activation='relu', kernel_initializer='he_uniform')(branchs[2])\n",
    "    o4 = Conv1D(4,256,activation='relu', kernel_initializer='he_uniform')(branchs[3])\n",
    "    o5 = Conv1D(4,256,activation='relu', kernel_initializer='he_uniform')(branchs[4])\n",
    "    o1 = Conv1D(16,256,activation='relu', kernel_initializer='he_uniform')(o1)\n",
    "    o2 = Conv1D(16,256,activation='relu', kernel_initializer='he_uniform')(o2)\n",
    "    o3 = Conv1D(16,256,activation='relu', kernel_initializer='he_uniform')(o3)\n",
    "    o4 = Conv1D(16,256,activation='relu', kernel_initializer='he_uniform')(o4)\n",
    "    o5 = Conv1D(16,256,activation='relu', kernel_initializer='he_uniform')(o5)\n",
    "    out1 = Concatenate(axis = -1)((o1,o2,o3,o4,o5))\n",
    "    c1 = Conv1D(64,256,activation='relu', kernel_initializer='he_uniform')(out1)\n",
    "    out1 = Flatten()(c1)\n",
    "    out2 = Dense(2,activation='sigmoid', kernel_initializer='he_uniform')(h2)\n",
    "    model = Model(inputs=input, outputs=out2)\n",
    "    model.summary() \n",
    "    return model\n",
    "# names = [[150,45,80],[150,50,80],[150,55,80]\n",
    "#          ,[60,45,80],[60,50,80],[60,55,80]\n",
    "#          ,[90,45,80],[90,50,80],[90,55,80]]\n",
    "\n",
    "# MP_num = [45,60,65,55,50]\n",
    "# MP_num = [45]\n",
    "# Pitch_num = [180,200,225,260]\n",
    "# PF = [[180,8.5],[200,7.5],[225,6.5],[260,5.5]]\n",
    "# PF = [[200,7.5]]\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "0.001,\n",
    "decay_steps=1000,\n",
    "#   10的decay训练有些过于慢了\n",
    "decay_rate=1,\n",
    "staircase=False)\n",
    "\n",
    "\n",
    "\n",
    "model = IANN()\n",
    "freq = np.linspace(5*1e8,1.5*1e9,501)\n",
    "data_path = 'D:\\\\data\\\\7p/input2w/2w.npy'\n",
    "data = np.load(data_path)\n",
    "label_path = 'D:\\\\data\\\\7p/out/MP60.csv'\n",
    "label = np.genfromtxt(label_path,delimiter=',')\n",
    "label_mm_path = label_path + 'maxmin.csv'\n",
    "label_mm = np.genfromtxt(label_mm_path, delimiter=',')\n",
    "label_max = label_mm[0]\n",
    "label_min = label_mm[1]\n",
    "label = (label-label_min) / (label_max - label_min)\n",
    "data_set = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(data), tf.data.Dataset.from_tensor_slices(label)))\n",
    "BATCHSIZE=128\n",
    "dataset = data_set\n",
    "valiset = dataset.take(128).batch(BATCHSIZE).cache().prefetch(tf.data.AUTOTUNE) \n",
    "trainset = dataset.skip(128)\n",
    "trainset = trainset.cache().shuffle(trainset.cardinality(),seed=1919810,reshuffle_each_iteration=True).repeat().batch(BATCHSIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "checkpoint_filepath = os.path.join(os.getcwd(),'obs','weights')\n",
    "# shutil.rmtree(checkpoint_filepath)\n",
    "# checkpoint_filepath = checkpoint_filepath + '\\\\' +str(Pitch)\n",
    "# log_dir=\"c:\\\\Users\\\\caizhi.zheng\\\\code\\\\For AI\\\\logs2v5\\\\\"+str(MP)+'-'+str(Pitch)\n",
    "log_dir=\"c:\\\\Users\\\\caizhi.zheng\\\\code\\\\For AI\\\\logs\"\n",
    "# shutil.rmtree(log_dir,ignore_errors=True)  \n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', verbose=1, save_weights_only=True, \n",
    "                                    save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=200)\n",
    "Board = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
    "opt = optimizer.Adam(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=opt, loss='mse', metrics='accuracy')\n",
    "# reduce_lr = LearningRateScheduler(scheduler)\n",
    "# model.fit(dataset, epochs=25)\n",
    "history = model.fit(trainset, validation_data=valiset, steps_per_epoch=80, epochs=500, callbacks=[model_checkpoint,early_stop,Board])#validation_split=0.02,)\n",
    "model.evaluate(valiset)\n",
    "# test\n",
    "# if Pitch != 225:\n",
    "#     pitch = int(0.1*Pitch)\n",
    "# test_path = 'C:\\\\Users\\\\caizhi.zheng\\\\code\\\\For AI\\\\SNP Selection/'+'LT'+str(NT)+ 'MP'+str(MP) +'P'+str(pitch) +'/'\n",
    "# saw_set = rf.read_all_networks(test_path)\n",
    "# for name in saw_set:\n",
    "#     saw = saw_set[name]\n",
    "# y0 = saw['900-1100MHZ'].y[:,0,0]\n",
    "# test_set = np.ones((801,5))*0.01\n",
    "# # test_set = np.zeros((801,5))\n",
    "# # data_mm_path = data_path + 'musi.csv'\n",
    "# # data_mm = np.genfromtxt(data_mm_path, delimiter=',')\n",
    "# # data_mu = data_mm[:5]\n",
    "# # data_sigma = data_mm[5:10]\n",
    "# # test_set = (test_set * data_sigma) + data_mu\n",
    "\n",
    "# test_set = tf.data.Dataset.from_tensors(test_set).batch(1) \n",
    "# # model = models.load_model(checkpoint_filepath)\n",
    "# # model.evaluate(dataset.batch(BATCHSIZE))\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "# test_result = model.predict(test_set)\n",
    "# print(test_result) \n",
    "# # print(test_result.shape) \n",
    "# # 反归一化\n",
    "# test_result = (test_result[0]*(label_max - label_min)) + label_min\n",
    "# print(test_result) \n",
    "# result_path = 'c:\\\\Users\\\\caizhi.zheng\\\\code\\\\For AI\\\\loss_picture/result' + suffix + '.csv'\n",
    "# with open(result_path, 'w') as f:\n",
    "#     np.savetxt(f, test_result, delimiter=',')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.loadtxt('./h-0.01-0.01-1.txt',skiprows=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = tt[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.vectorize(complex)(ttt[:,0],ttt[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(YtoZS(test,freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 501)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_su = (test.T - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 239ms/step\n"
     ]
    }
   ],
   "source": [
    "test_set = tf.data.Dataset.from_tensors(test_su).batch(1) \n",
    "# model = models.load_model(checkpoint_filepath)\n",
    "# model.evaluate(dataset.batch(BATCHSIZE))\n",
    "model.load_weights(checkpoint_filepath)\n",
    "test_result = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.8061975e-16, 4.2130578e-02, 2.1157525e-02, 9.9575937e-01,\n",
       "        9.9561900e-01, 1.5915664e-02, 9.1835809e-01]], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.02850237e+01 2.56665254e+02 1.47058702e+02 5.99151874e-02\n",
      " 2.13752354e+00 7.23715451e-03 4.93382288e+03]\n"
     ]
    }
   ],
   "source": [
    "test_result = (test_result[0]*(label_max - label_min)) + label_min\n",
    "print(test_result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
