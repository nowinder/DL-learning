## fwd
In a tandem ANN a forward solver network is trained in a first step.The training of the actual inverse design network (the generator) subsequently uses the fixed pre-trained forward model as a physics predictor to evaluate the inverse design output. In consequence, the loss function does not compare ambiguous design layouts but operates in the physics domain (comparing, e.g., the extinction efficiency rather than the design parameters). In this way, different design parameters which lead to a similar physical response no longer confuse the ANN, and all correct solutions to a given design problem yield a positive training feedback
在串联 ANN 中，第一步是训练正向求解网络，然后使用固定的预训练正向模型作为物理预测器来评估反向设计输出。因此，损失函数并不比较模棱两可的设计布局，而是在物理领域进行操作（例如，比较消光效率而不是设计参数）。这样，导致类似物理响应的不同设计参数就不会再使 ANN 感到困惑，而且给定设计问题的所有正确解决方案都会产生正向的训练反馈。

The “tandem neural network” can stabilize the generator (G) via a physics loss, based on a pre-trained forward model (fwd). This approach, however, limits the inverse design to one solution per design target, rendering inaccessible possible multiple solutions to a given problem.
“串联神经网络”可以根据预先训练好的前向模型（fwd），通过物理损失函数建立模型（G）。然而，这种方法将逆向设计限制为每个设计目标只有一个解决方案，从而使特定问题无法获得多种可能的解决方案。

Another model that circumvents the one-to-many problem is the cGAN [68,73–76]. A cGAN takes as input not only the design target but also an additional “latent vector,” which is a normally distributed sequence of random values. The network then learns to use different values of the latent vector to address the distinct non-unique solutions. In addition to the introduction of a latent vector, a further peculiarity of cGANs is their loss function, which is a discriminator network that tries to distinguish generated solutions from real ones, and which is also subject to training.
另一种规避一对多问题的模型是 cGAN [68,73-76]。cGAN 的输入不仅包括设计目标，还包括额外的 "潜在向量"，即正态分布的随机值序列。然后，网络会学习使用潜向量的不同值来处理不同的非唯一解决方案。除了引入潜向量外，cGANs 的另一个特点是其损失函数，它是一个判别网络，试图将生成的解决方案与真实解决方案区分开来，并且也需要经过训练。

## 52
We introduce a bidirectional deep neural network archi-tecture that is composed of two networks (Fig. 2a), where the ﬁrst is a Geometry-predicting-network (GPN) that predicts a geometry based on the spectra (the inverse path) and the second is a Spectrum-predicting-network (SPN) that predicts the spectra based on the nanoparticle geometry (the direct path). The geometry predicted by the GPN is fed into the SPN which, in turn, predicts the spectrum. We thus solve the harder inverse problem ﬁrst, i.e., predicting the geometry based on two spectra for both polarizations, and then, using the predicted geometry, we match the recovered spectrum with the original one. It is worthwhile to note that the training of such a bidirec-tional network requires a dedicated learning procedure, since the input to the SPN is a predicted geometry rather than the actual geometry. Furthermore, we also observe a signiﬁcant gain from training one network on all the training sets rather than the alternative of training mul-tiple separate networks. It is crucial to stress that the learning phase in the DNN is a nonrecurring effort, which means that once the data set is learned, the query phase is quasi instantaneous. This approach is a clear departure from evolutionary methods in which for every query, the whole parameter space is searched for optimization.

我们引入了一种双向深度神经网络架构，它由两个网络组成（图 2a），第一个是几何预测网络（GPN），根据光谱预测几何形状（反向路径），第二个是光谱预测网络（SPN），根据纳米粒子的几何形状预测光谱（直接路径）。GPN 预测的几何形状被输入 SPN，而 SPN 则反过来预测光谱。因此，我们首先要解决难度较大的逆问题，即根据两个极化的两个光谱预测几何形状，然后利用预测的几何形状将恢复的光谱与原始光谱进行匹配。值得注意的是，由于 SPN 的输入是预测的几何图形而非实际几何图形，<mark>因此这种双向网络的训练需要专门的学习程序</mark>。此外，我们还观察到，在所有训练集上训练一个网络比训练多个独立网络收益更大。需要强调的是，DNN 的学习阶段是非重复性的，这意味着一旦学习了数据集，查询阶段几乎是瞬时的。这种方法明显有别于进化方法，在进化方法中，每次查询都要搜索整个参数空间进行优化。

Instead, we propose to train one network that contains both SPN and GPN in order to optimize them together so they will co-adapt to each other. We call this process co-adaptation of networks. We solve the (harder) inverse problem first using the GPN, and then, using the predicted geometry, make sure that the recovered spectrum matches the original one using SPN. Training this type of a network requires a dedicated procedure since the input to the second half of the network is a predicted geometry and not the true geometry that is known to produce the spectrum. During the training procedure, we perform one forward pass on the GPN, then we clone the output to create two queries for each experiment - one for each polarization. These two queries are different in the polarization flag, indicating the relevant polarization. Since we use batch learning, we perform it on the entire batch and create a double sized batch for the SPN. We then perform forward on the SPN, by feeding it with the GPN output. At the end, we backward the two networks, calculate a loss for each one, sum the loss and send it to the optimizer with the gradients of both networks. We have also noticed a few training tricks that led to better results/time to converge. The first is doing this backward on the SPN only after a predefined amount of epochs, when the GPN becomes stable with moderate results. We also noticed that with each epoch, performing a few extra epochs without doing backward on the GPN leads to better results - i.e. we give the SPN more than one epoch to adjust to each GPN hypothesis during the training. 

相反，我们建议训练一个同时包含 SPN 和 GPN 的网络，以便对它们进行优化，使它们能够相互共同适应。我们将这一过程称为网络的共同适应。我们首先使用 GPN 解决（较难的）逆问题，然后使用预测的几何图形，使用 SPN 确保恢复的频谱与原始频谱相匹配。训练这种网络需要专门的程序，因为网络后半部分的输入是预测的几何图形，而不是已知产生频谱的真实几何图形。在训练过程中，我们对 GPN 进行一次前向传递，然后克隆输出，为每次实验创建两个查询--每个极化一个。这两个查询的极化标志不同，表示相关的极化。由于我们使用批量学习，因此我们对整个批量进行学习，并为 SPN 创建一个双倍大小的批量。然后，我们向 SPN 输入 GPN 输出，对其进行前向学习。最后，我们将两个网络后退，计算每个网络的损失，将损失相加，并将其与两个网络的梯度一起发送给优化器。我们还注意到一些训练技巧能带来更好的结果/收敛时间。首先，只有在 GPN 变得稳定且结果适中的情况下，才会在 SPN 上进行预先确定数量的历时训练。我们还注意到，在不对 GPN 进行后向训练的情况下，每个历元多进行几个历元的训练会带来更好的结果--也就是说，在训练过程中，我们给 SPN 多于一个历元的时间来适应每个 GPN 假设。

The loss function that we use is: 	
Loss = GPN_MSE(predictedGeometry,groundTruthGeometry) + SPNx_MSE(predictedSpetrumX,groundTruthSpectrumX) + SPNy_MSE(predictedSpetrumY,groundTruthSpectrumY)
Where predictedGeometry is the ouput of the GPN and groundTruthSpectrumX and PredictedSpetrumY are both the output of the SPN
